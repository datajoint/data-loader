{"config": {"indexing": "full", "lang": ["en"], "min_search_length": 3, "prebuild_index": false, "separator": "[\\s\\-]+"}, "docs": [{"location": "", "text": "Elements Interface for external analysis packages \u00b6 DataJoint Element for interoperability with other software. DataJoint Elements collectively standardize and automate data collection and analysis for neuroscience experiments. Typical Elements are modular pipelines for data storage and processing with corresponding database tables that can be combined with other Elements to assemble a fully functional pipeline. Element Interface is home to a number of utilities that make this possible. Element Interface provides a number of loaders and utility functions used across a number of other Elements. Calcium imaging loaders: Suite2p, CaImAn, PrairieView File management, see find_full_path API Data ingestion, see ingest_csv_to_table API Visit the Concepts page for more information on these tools.", "title": "Element Interface"}, {"location": "#elements-interface-for-external-analysis-packages", "text": "DataJoint Element for interoperability with other software. DataJoint Elements collectively standardize and automate data collection and analysis for neuroscience experiments. Typical Elements are modular pipelines for data storage and processing with corresponding database tables that can be combined with other Elements to assemble a fully functional pipeline. Element Interface is home to a number of utilities that make this possible. Element Interface provides a number of loaders and utility functions used across a number of other Elements. Calcium imaging loaders: Suite2p, CaImAn, PrairieView File management, see find_full_path API Data ingestion, see ingest_csv_to_table API Visit the Concepts page for more information on these tools.", "title": "Elements Interface for external analysis packages"}, {"location": "changelog/", "text": "Changelog \u00b6 Observes Semantic Versioning standard and Keep a Changelog convention. 0.4.0 - 2022-12-14 \u00b6 Add - mkdocs documentation Add - improved docstrings for mkdocs Add - EXTRACT trigger and loader tools 0.3.0 - 2022-10-7 \u00b6 Add - Function prairieviewreader to parse metadata from Bruker PrarieView acquisition system Update - Changelog with tag links 0.2.1 - 2022-07-13 \u00b6 Add - Adopt black formatting Add - Code of Conduct 0.2.0 - 2022-07-06 \u00b6 First release of element-interface . Bugfix - Fix for tifffile import. Add - Function run_caiman to trigger CNMF algorithm. Add - Function ingest_csv_to_table to insert data from CSV files into tables. Add - Function recursive_search to search through nested dictionary for a key. Add - Function upload_to_dandi to upload Neurodata Without Borders file to the DANDI platform. Update - Remove extras_require feature to allow this package to be published to PyPI. 0.1.0a1 - 2022-01-12 \u00b6 Change - Rename the package element-data-loader to element-interface . 0.1.0a0 - 2021-06-21 \u00b6 Add - Readers for: ScanImage , Suite2p , CaImAn .", "title": "Changelog"}, {"location": "changelog/#changelog", "text": "Observes Semantic Versioning standard and Keep a Changelog convention.", "title": "Changelog"}, {"location": "changelog/#040-2022-12-14", "text": "Add - mkdocs documentation Add - improved docstrings for mkdocs Add - EXTRACT trigger and loader tools", "title": "0.4.0 - 2022-12-14"}, {"location": "changelog/#030-2022-10-7", "text": "Add - Function prairieviewreader to parse metadata from Bruker PrarieView acquisition system Update - Changelog with tag links", "title": "0.3.0 - 2022-10-7"}, {"location": "changelog/#021-2022-07-13", "text": "Add - Adopt black formatting Add - Code of Conduct", "title": "0.2.1 - 2022-07-13"}, {"location": "changelog/#020-2022-07-06", "text": "First release of element-interface . Bugfix - Fix for tifffile import. Add - Function run_caiman to trigger CNMF algorithm. Add - Function ingest_csv_to_table to insert data from CSV files into tables. Add - Function recursive_search to search through nested dictionary for a key. Add - Function upload_to_dandi to upload Neurodata Without Borders file to the DANDI platform. Update - Remove extras_require feature to allow this package to be published to PyPI.", "title": "0.2.0 - 2022-07-06"}, {"location": "changelog/#010a1-2022-01-12", "text": "Change - Rename the package element-data-loader to element-interface .", "title": "0.1.0a1 - 2022-01-12"}, {"location": "changelog/#010a0-2021-06-21", "text": "Add - Readers for: ScanImage , Suite2p , CaImAn .", "title": "0.1.0a0 - 2021-06-21"}, {"location": "citation/", "text": "Citation \u00b6 If your work uses this Element, please cite the following manuscript and Research Resource Identifier (RRID). Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358 DataJoint Elements ( RRID:SCR_021894 ) - Element Interface (version 0.4.0)", "title": "Citation"}, {"location": "citation/#citation", "text": "If your work uses this Element, please cite the following manuscript and Research Resource Identifier (RRID). Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D, Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358 DataJoint Elements ( RRID:SCR_021894 ) - Element Interface (version 0.4.0)", "title": "Citation"}, {"location": "concepts/", "text": "Concepts \u00b6 Software interoperability \u00b6 While software versioning helps researchers keep track of changes over time, it often makes sense to separate the interface from the pipeline itself. By collecting utilities here in Element Interface, maintainers can keep up to date with the latest developments across other packages, without causing issues in the respective Element. Element Features \u00b6 General utilities \u00b6 utils.find_full_path and utils.find_root_directory are used across many Elements and Workflows to allow for the flexibility of providing one or more root directories in the user's config, and extrapolating from a relative path at runtime. utils.ingest_csv_to_table is used across workflow examples to ingest from sample data from local CSV files into sets of manual tables. While researchers may wish to manually insert for day-to-day operations, it helps to have a more complete dataset when learning how to use various Elements. Suite2p \u00b6 This Element provides functions to independently run Suite2p's motion correction, segmentation, and deconvolution steps. These functions currently work for single plane tiff files. If one is running all Suite2p pre-processing steps concurrently, these functions are not required and one can run suite2p.run_s2p() . The wrapper functions here were developed primarily because run_s2p cannot individually run deconvolution using the spikedetect flag ( Suite2p Issue #718 ). Requirements ops dictionary db dictionary Note: The ops dictionary returned from the motion_correction_suite2p and segmentation_suite2p functions is only a subset of the keys generated with the suite2p.default_ops() function. Element Architecture \u00b6 The functions for each acquisition and analysis package are stored within a separate module. Acquisition packages: ScanImage Analysis packages: - Suite2p loader and trigger - CaImAn loader and trigger Data upload: DANDI Roadmap \u00b6 Further development of this Element is community driven. Upon user requests and based on guidance from the Scientific Steering Group we will additional features to this Element.", "title": "Concepts"}, {"location": "concepts/#concepts", "text": "", "title": "Concepts"}, {"location": "concepts/#software-interoperability", "text": "While software versioning helps researchers keep track of changes over time, it often makes sense to separate the interface from the pipeline itself. By collecting utilities here in Element Interface, maintainers can keep up to date with the latest developments across other packages, without causing issues in the respective Element.", "title": "Software interoperability"}, {"location": "concepts/#element-features", "text": "", "title": "Element Features"}, {"location": "concepts/#general-utilities", "text": "utils.find_full_path and utils.find_root_directory are used across many Elements and Workflows to allow for the flexibility of providing one or more root directories in the user's config, and extrapolating from a relative path at runtime. utils.ingest_csv_to_table is used across workflow examples to ingest from sample data from local CSV files into sets of manual tables. While researchers may wish to manually insert for day-to-day operations, it helps to have a more complete dataset when learning how to use various Elements.", "title": "General utilities"}, {"location": "concepts/#suite2p", "text": "This Element provides functions to independently run Suite2p's motion correction, segmentation, and deconvolution steps. These functions currently work for single plane tiff files. If one is running all Suite2p pre-processing steps concurrently, these functions are not required and one can run suite2p.run_s2p() . The wrapper functions here were developed primarily because run_s2p cannot individually run deconvolution using the spikedetect flag ( Suite2p Issue #718 ). Requirements ops dictionary db dictionary Note: The ops dictionary returned from the motion_correction_suite2p and segmentation_suite2p functions is only a subset of the keys generated with the suite2p.default_ops() function.", "title": "Suite2p"}, {"location": "concepts/#element-architecture", "text": "The functions for each acquisition and analysis package are stored within a separate module. Acquisition packages: ScanImage Analysis packages: - Suite2p loader and trigger - CaImAn loader and trigger Data upload: DANDI", "title": "Element Architecture"}, {"location": "concepts/#roadmap", "text": "Further development of this Element is community driven. Upon user requests and based on guidance from the Scientific Steering Group we will additional features to this Element.", "title": "Roadmap"}, {"location": "api/element_interface/caiman_loader/", "text": "CaImAn \u00b6 Parse the CaImAn output file CaImAn results doc Expecting the following objects dims: dview: estimates: Segmentations and traces mmap_file: params: Input parameters remove_very_bad_comps: skip_refinement: motion_correction: Motion correction shifts and summary images Example output_dir = ' /subject1/session0/caiman' loaded_dataset = caiman_loader.CaImAn(output_dir) Attributes: Name Type Description alignment_channel hard-coded to 0 caiman_fp file path with all required files: \"/motion_correction/reference_image\", \"/motion_correction/correlation_image\", \"/motion_correction/average_image\", \"/motion_correction/max_image\", \"/estimates/A\", cnmf loaded caiman object; cm.source_extraction.cnmf.cnmf.load_CNMF(caiman_fp) creation_time file creation time curation_time file creation time extract_masks dict function to extract masks h5f caiman_fp read as h5py file masks dict result of extract_masks motion_correction h5f \"motion_correction\" property params cnmf.params segmentation_channel hard-coded to 0 Source code in element_interface/caiman_loader.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 class CaImAn : \"\"\"Parse the CaImAn output file [CaImAn results doc](https://caiman.readthedocs.io/en/master/Getting_Started.html#result-variables-for-2p-batch-analysis) Expecting the following objects: - dims: - dview: - estimates: Segmentations and traces - mmap_file: - params: Input parameters - remove_very_bad_comps: - skip_refinement: - motion_correction: Motion correction shifts and summary images Example: > output_dir = '<imaging_root_data_dir>/subject1/session0/caiman' > loaded_dataset = caiman_loader.CaImAn(output_dir) Attributes: alignment_channel: hard-coded to 0 caiman_fp: file path with all required files: \"/motion_correction/reference_image\", \"/motion_correction/correlation_image\", \"/motion_correction/average_image\", \"/motion_correction/max_image\", \"/estimates/A\", cnmf: loaded caiman object; cm.source_extraction.cnmf.cnmf.load_CNMF(caiman_fp) creation_time: file creation time curation_time: file creation time extract_masks: function to extract masks h5f: caiman_fp read as h5py file masks: dict result of extract_masks motion_correction: h5f \"motion_correction\" property params: cnmf.params segmentation_channel: hard-coded to 0 \"\"\" def __init__ ( self , caiman_dir : str ): \"\"\"Initialize CaImAn loader class Args: caiman_dir (str): string, absolute file path to CaIman directory Raises: FileNotFoundError: No CaImAn analysis output file found FileNotFoundError: No CaImAn analysis output found, missing required fields \"\"\" # ---- Search and verify CaImAn output file exists ---- caiman_dir = pathlib . Path ( caiman_dir ) if not caiman_dir . exists (): raise FileNotFoundError ( \"CaImAn directory not found: {} \" . format ( caiman_dir )) for fp in caiman_dir . glob ( \"*.hdf5\" ): with h5py . File ( fp , \"r\" ) as h5f : if all ( s in h5f for s in _required_hdf5_fields ): self . caiman_fp = fp break else : raise FileNotFoundError ( \"No CaImAn analysis output file found at {} \" \" containg all required fields ( {} )\" . format ( caiman_dir , _required_hdf5_fields ) ) # ---- Initialize CaImAn's results ---- self . cnmf = cm . source_extraction . cnmf . cnmf . load_CNMF ( self . caiman_fp ) self . params = self . cnmf . params self . h5f = h5py . File ( self . caiman_fp , \"r\" ) self . motion_correction = self . h5f [ \"motion_correction\" ] self . _masks = None # ---- Metainfo ---- self . creation_time = datetime . fromtimestamp ( os . stat ( self . caiman_fp ) . st_ctime ) self . curation_time = datetime . fromtimestamp ( os . stat ( self . caiman_fp ) . st_ctime ) @property def masks ( self ): if self . _masks is None : self . _masks = self . extract_masks () return self . _masks @property def alignment_channel ( self ): return 0 # hard-code to channel index 0 @property def segmentation_channel ( self ): return 0 # hard-code to channel index 0 def extract_masks ( self ) -> dict : \"\"\"Extract masks from CaImAn object Raises: NotImplemented: Not yet implemented for 3D datasets Returns: dict: Mask attributes - mask_id, mask_npix, mask_weights, mask_center_x, mask_center_y, mask_center_z, mask_xpix, mask_ypix, mask_zpix, inferred_trace, dff, spikes \"\"\" if self . params . motion [ \"is3D\" ]: raise NotImplemented ( \"CaImAn mask extraction for volumetric data not yet implemented\" ) comp_contours = cm . utils . visualization . get_contours ( self . cnmf . estimates . A , self . cnmf . dims ) masks = [] for comp_idx , comp_contour in enumerate ( comp_contours ): ind , _ , weights = scipy . sparse . find ( self . cnmf . estimates . A [:, comp_idx ]) if self . cnmf . params . motion [ \"is3D\" ]: xpix , ypix , zpix = np . unravel_index ( ind , self . cnmf . dims , order = \"F\" ) center_x , center_y , center_z = comp_contour [ \"CoM\" ] . astype ( int ) else : xpix , ypix = np . unravel_index ( ind , self . cnmf . dims , order = \"F\" ) center_x , center_y = comp_contour [ \"CoM\" ] . astype ( int ) center_z = 0 zpix = np . full ( len ( weights ), center_z ) masks . append ( { \"mask_id\" : comp_contour [ \"neuron_id\" ], \"mask_npix\" : len ( weights ), \"mask_weights\" : weights , \"mask_center_x\" : center_x , \"mask_center_y\" : center_y , \"mask_center_z\" : center_z , \"mask_xpix\" : xpix , \"mask_ypix\" : ypix , \"mask_zpix\" : zpix , \"inferred_trace\" : self . cnmf . estimates . C [ comp_idx , :], \"dff\" : self . cnmf . estimates . F_dff [ comp_idx , :], \"spikes\" : self . cnmf . estimates . S [ comp_idx , :], } ) return masks __init__ ( caiman_dir ) \u00b6 Initialize CaImAn loader class Parameters: Name Type Description Default caiman_dir str string, absolute file path to CaIman directory required Raises: Type Description FileNotFoundError No CaImAn analysis output file found FileNotFoundError No CaImAn analysis output found, missing required fields Source code in element_interface/caiman_loader.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __init__ ( self , caiman_dir : str ): \"\"\"Initialize CaImAn loader class Args: caiman_dir (str): string, absolute file path to CaIman directory Raises: FileNotFoundError: No CaImAn analysis output file found FileNotFoundError: No CaImAn analysis output found, missing required fields \"\"\" # ---- Search and verify CaImAn output file exists ---- caiman_dir = pathlib . Path ( caiman_dir ) if not caiman_dir . exists (): raise FileNotFoundError ( \"CaImAn directory not found: {} \" . format ( caiman_dir )) for fp in caiman_dir . glob ( \"*.hdf5\" ): with h5py . File ( fp , \"r\" ) as h5f : if all ( s in h5f for s in _required_hdf5_fields ): self . caiman_fp = fp break else : raise FileNotFoundError ( \"No CaImAn analysis output file found at {} \" \" containg all required fields ( {} )\" . format ( caiman_dir , _required_hdf5_fields ) ) # ---- Initialize CaImAn's results ---- self . cnmf = cm . source_extraction . cnmf . cnmf . load_CNMF ( self . caiman_fp ) self . params = self . cnmf . params self . h5f = h5py . File ( self . caiman_fp , \"r\" ) self . motion_correction = self . h5f [ \"motion_correction\" ] self . _masks = None # ---- Metainfo ---- self . creation_time = datetime . fromtimestamp ( os . stat ( self . caiman_fp ) . st_ctime ) self . curation_time = datetime . fromtimestamp ( os . stat ( self . caiman_fp ) . st_ctime ) extract_masks () \u00b6 Extract masks from CaImAn object Raises: Type Description NotImplemented Not yet implemented for 3D datasets Returns: Name Type Description dict dict Mask attributes - mask_id, mask_npix, mask_weights, mask_center_x, mask_center_y, mask_center_z, mask_xpix, mask_ypix, mask_zpix, inferred_trace, dff, spikes Source code in element_interface/caiman_loader.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def extract_masks ( self ) -> dict : \"\"\"Extract masks from CaImAn object Raises: NotImplemented: Not yet implemented for 3D datasets Returns: dict: Mask attributes - mask_id, mask_npix, mask_weights, mask_center_x, mask_center_y, mask_center_z, mask_xpix, mask_ypix, mask_zpix, inferred_trace, dff, spikes \"\"\" if self . params . motion [ \"is3D\" ]: raise NotImplemented ( \"CaImAn mask extraction for volumetric data not yet implemented\" ) comp_contours = cm . utils . visualization . get_contours ( self . cnmf . estimates . A , self . cnmf . dims ) masks = [] for comp_idx , comp_contour in enumerate ( comp_contours ): ind , _ , weights = scipy . sparse . find ( self . cnmf . estimates . A [:, comp_idx ]) if self . cnmf . params . motion [ \"is3D\" ]: xpix , ypix , zpix = np . unravel_index ( ind , self . cnmf . dims , order = \"F\" ) center_x , center_y , center_z = comp_contour [ \"CoM\" ] . astype ( int ) else : xpix , ypix = np . unravel_index ( ind , self . cnmf . dims , order = \"F\" ) center_x , center_y = comp_contour [ \"CoM\" ] . astype ( int ) center_z = 0 zpix = np . full ( len ( weights ), center_z ) masks . append ( { \"mask_id\" : comp_contour [ \"neuron_id\" ], \"mask_npix\" : len ( weights ), \"mask_weights\" : weights , \"mask_center_x\" : center_x , \"mask_center_y\" : center_y , \"mask_center_z\" : center_z , \"mask_xpix\" : xpix , \"mask_ypix\" : ypix , \"mask_zpix\" : zpix , \"inferred_trace\" : self . cnmf . estimates . C [ comp_idx , :], \"dff\" : self . cnmf . estimates . F_dff [ comp_idx , :], \"spikes\" : self . cnmf . estimates . S [ comp_idx , :], } ) return masks", "title": "caiman_loader.py"}, {"location": "api/element_interface/caiman_loader/#element_interface.caiman_loader.CaImAn", "text": "Parse the CaImAn output file CaImAn results doc Expecting the following objects dims: dview: estimates: Segmentations and traces mmap_file: params: Input parameters remove_very_bad_comps: skip_refinement: motion_correction: Motion correction shifts and summary images Example output_dir = ' /subject1/session0/caiman' loaded_dataset = caiman_loader.CaImAn(output_dir) Attributes: Name Type Description alignment_channel hard-coded to 0 caiman_fp file path with all required files: \"/motion_correction/reference_image\", \"/motion_correction/correlation_image\", \"/motion_correction/average_image\", \"/motion_correction/max_image\", \"/estimates/A\", cnmf loaded caiman object; cm.source_extraction.cnmf.cnmf.load_CNMF(caiman_fp) creation_time file creation time curation_time file creation time extract_masks dict function to extract masks h5f caiman_fp read as h5py file masks dict result of extract_masks motion_correction h5f \"motion_correction\" property params cnmf.params segmentation_channel hard-coded to 0 Source code in element_interface/caiman_loader.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 class CaImAn : \"\"\"Parse the CaImAn output file [CaImAn results doc](https://caiman.readthedocs.io/en/master/Getting_Started.html#result-variables-for-2p-batch-analysis) Expecting the following objects: - dims: - dview: - estimates: Segmentations and traces - mmap_file: - params: Input parameters - remove_very_bad_comps: - skip_refinement: - motion_correction: Motion correction shifts and summary images Example: > output_dir = '<imaging_root_data_dir>/subject1/session0/caiman' > loaded_dataset = caiman_loader.CaImAn(output_dir) Attributes: alignment_channel: hard-coded to 0 caiman_fp: file path with all required files: \"/motion_correction/reference_image\", \"/motion_correction/correlation_image\", \"/motion_correction/average_image\", \"/motion_correction/max_image\", \"/estimates/A\", cnmf: loaded caiman object; cm.source_extraction.cnmf.cnmf.load_CNMF(caiman_fp) creation_time: file creation time curation_time: file creation time extract_masks: function to extract masks h5f: caiman_fp read as h5py file masks: dict result of extract_masks motion_correction: h5f \"motion_correction\" property params: cnmf.params segmentation_channel: hard-coded to 0 \"\"\" def __init__ ( self , caiman_dir : str ): \"\"\"Initialize CaImAn loader class Args: caiman_dir (str): string, absolute file path to CaIman directory Raises: FileNotFoundError: No CaImAn analysis output file found FileNotFoundError: No CaImAn analysis output found, missing required fields \"\"\" # ---- Search and verify CaImAn output file exists ---- caiman_dir = pathlib . Path ( caiman_dir ) if not caiman_dir . exists (): raise FileNotFoundError ( \"CaImAn directory not found: {} \" . format ( caiman_dir )) for fp in caiman_dir . glob ( \"*.hdf5\" ): with h5py . File ( fp , \"r\" ) as h5f : if all ( s in h5f for s in _required_hdf5_fields ): self . caiman_fp = fp break else : raise FileNotFoundError ( \"No CaImAn analysis output file found at {} \" \" containg all required fields ( {} )\" . format ( caiman_dir , _required_hdf5_fields ) ) # ---- Initialize CaImAn's results ---- self . cnmf = cm . source_extraction . cnmf . cnmf . load_CNMF ( self . caiman_fp ) self . params = self . cnmf . params self . h5f = h5py . File ( self . caiman_fp , \"r\" ) self . motion_correction = self . h5f [ \"motion_correction\" ] self . _masks = None # ---- Metainfo ---- self . creation_time = datetime . fromtimestamp ( os . stat ( self . caiman_fp ) . st_ctime ) self . curation_time = datetime . fromtimestamp ( os . stat ( self . caiman_fp ) . st_ctime ) @property def masks ( self ): if self . _masks is None : self . _masks = self . extract_masks () return self . _masks @property def alignment_channel ( self ): return 0 # hard-code to channel index 0 @property def segmentation_channel ( self ): return 0 # hard-code to channel index 0 def extract_masks ( self ) -> dict : \"\"\"Extract masks from CaImAn object Raises: NotImplemented: Not yet implemented for 3D datasets Returns: dict: Mask attributes - mask_id, mask_npix, mask_weights, mask_center_x, mask_center_y, mask_center_z, mask_xpix, mask_ypix, mask_zpix, inferred_trace, dff, spikes \"\"\" if self . params . motion [ \"is3D\" ]: raise NotImplemented ( \"CaImAn mask extraction for volumetric data not yet implemented\" ) comp_contours = cm . utils . visualization . get_contours ( self . cnmf . estimates . A , self . cnmf . dims ) masks = [] for comp_idx , comp_contour in enumerate ( comp_contours ): ind , _ , weights = scipy . sparse . find ( self . cnmf . estimates . A [:, comp_idx ]) if self . cnmf . params . motion [ \"is3D\" ]: xpix , ypix , zpix = np . unravel_index ( ind , self . cnmf . dims , order = \"F\" ) center_x , center_y , center_z = comp_contour [ \"CoM\" ] . astype ( int ) else : xpix , ypix = np . unravel_index ( ind , self . cnmf . dims , order = \"F\" ) center_x , center_y = comp_contour [ \"CoM\" ] . astype ( int ) center_z = 0 zpix = np . full ( len ( weights ), center_z ) masks . append ( { \"mask_id\" : comp_contour [ \"neuron_id\" ], \"mask_npix\" : len ( weights ), \"mask_weights\" : weights , \"mask_center_x\" : center_x , \"mask_center_y\" : center_y , \"mask_center_z\" : center_z , \"mask_xpix\" : xpix , \"mask_ypix\" : ypix , \"mask_zpix\" : zpix , \"inferred_trace\" : self . cnmf . estimates . C [ comp_idx , :], \"dff\" : self . cnmf . estimates . F_dff [ comp_idx , :], \"spikes\" : self . cnmf . estimates . S [ comp_idx , :], } ) return masks", "title": "CaImAn"}, {"location": "api/element_interface/caiman_loader/#element_interface.caiman_loader.CaImAn.__init__", "text": "Initialize CaImAn loader class Parameters: Name Type Description Default caiman_dir str string, absolute file path to CaIman directory required Raises: Type Description FileNotFoundError No CaImAn analysis output file found FileNotFoundError No CaImAn analysis output found, missing required fields Source code in element_interface/caiman_loader.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __init__ ( self , caiman_dir : str ): \"\"\"Initialize CaImAn loader class Args: caiman_dir (str): string, absolute file path to CaIman directory Raises: FileNotFoundError: No CaImAn analysis output file found FileNotFoundError: No CaImAn analysis output found, missing required fields \"\"\" # ---- Search and verify CaImAn output file exists ---- caiman_dir = pathlib . Path ( caiman_dir ) if not caiman_dir . exists (): raise FileNotFoundError ( \"CaImAn directory not found: {} \" . format ( caiman_dir )) for fp in caiman_dir . glob ( \"*.hdf5\" ): with h5py . File ( fp , \"r\" ) as h5f : if all ( s in h5f for s in _required_hdf5_fields ): self . caiman_fp = fp break else : raise FileNotFoundError ( \"No CaImAn analysis output file found at {} \" \" containg all required fields ( {} )\" . format ( caiman_dir , _required_hdf5_fields ) ) # ---- Initialize CaImAn's results ---- self . cnmf = cm . source_extraction . cnmf . cnmf . load_CNMF ( self . caiman_fp ) self . params = self . cnmf . params self . h5f = h5py . File ( self . caiman_fp , \"r\" ) self . motion_correction = self . h5f [ \"motion_correction\" ] self . _masks = None # ---- Metainfo ---- self . creation_time = datetime . fromtimestamp ( os . stat ( self . caiman_fp ) . st_ctime ) self . curation_time = datetime . fromtimestamp ( os . stat ( self . caiman_fp ) . st_ctime )", "title": "__init__()"}, {"location": "api/element_interface/caiman_loader/#element_interface.caiman_loader.CaImAn.extract_masks", "text": "Extract masks from CaImAn object Raises: Type Description NotImplemented Not yet implemented for 3D datasets Returns: Name Type Description dict dict Mask attributes - mask_id, mask_npix, mask_weights, mask_center_x, mask_center_y, mask_center_z, mask_xpix, mask_ypix, mask_zpix, inferred_trace, dff, spikes Source code in element_interface/caiman_loader.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def extract_masks ( self ) -> dict : \"\"\"Extract masks from CaImAn object Raises: NotImplemented: Not yet implemented for 3D datasets Returns: dict: Mask attributes - mask_id, mask_npix, mask_weights, mask_center_x, mask_center_y, mask_center_z, mask_xpix, mask_ypix, mask_zpix, inferred_trace, dff, spikes \"\"\" if self . params . motion [ \"is3D\" ]: raise NotImplemented ( \"CaImAn mask extraction for volumetric data not yet implemented\" ) comp_contours = cm . utils . visualization . get_contours ( self . cnmf . estimates . A , self . cnmf . dims ) masks = [] for comp_idx , comp_contour in enumerate ( comp_contours ): ind , _ , weights = scipy . sparse . find ( self . cnmf . estimates . A [:, comp_idx ]) if self . cnmf . params . motion [ \"is3D\" ]: xpix , ypix , zpix = np . unravel_index ( ind , self . cnmf . dims , order = \"F\" ) center_x , center_y , center_z = comp_contour [ \"CoM\" ] . astype ( int ) else : xpix , ypix = np . unravel_index ( ind , self . cnmf . dims , order = \"F\" ) center_x , center_y = comp_contour [ \"CoM\" ] . astype ( int ) center_z = 0 zpix = np . full ( len ( weights ), center_z ) masks . append ( { \"mask_id\" : comp_contour [ \"neuron_id\" ], \"mask_npix\" : len ( weights ), \"mask_weights\" : weights , \"mask_center_x\" : center_x , \"mask_center_y\" : center_y , \"mask_center_z\" : center_z , \"mask_xpix\" : xpix , \"mask_ypix\" : ypix , \"mask_zpix\" : zpix , \"inferred_trace\" : self . cnmf . estimates . C [ comp_idx , :], \"dff\" : self . cnmf . estimates . F_dff [ comp_idx , :], \"spikes\" : self . cnmf . estimates . S [ comp_idx , :], } ) return masks", "title": "extract_masks()"}, {"location": "api/element_interface/dandi/", "text": "upload_to_dandi ( data_directory , dandiset_id , staging = True , working_directory = None , api_key = None , sync = False ) \u00b6 Upload NWB files to DANDI Archive Parameters: Name Type Description Default data_directory str directory that contains source data required dandiset_id str 6-digit zero-padded string required staging bool If true, use staging server. If false, use production server. True working_directory str Dir in which to create symlinked dandiset. Must have write permissions to this directory. Default is current directory. None api_key str Provide the DANDI API key if not already in an environmental variable DANDI_API_KEY None sync str If True, delete all files in archive that are not present in the local directory. False Source code in element_interface/dandi.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def upload_to_dandi ( data_directory : str , dandiset_id : str , staging : bool = True , working_directory : str = None , api_key : str = None , sync : bool = False , ): \"\"\"Upload NWB files to DANDI Archive Args: data_directory (str): directory that contains source data dandiset_id (str): 6-digit zero-padded string staging (bool): If true, use staging server. If false, use production server. working_directory (str, optional): Dir in which to create symlinked dandiset. Must have write permissions to this directory. Default is current directory. api_key (str, optional): Provide the DANDI API key if not already in an environmental variable DANDI_API_KEY sync (str, optional): If True, delete all files in archive that are not present in the local directory. \"\"\" working_directory = working_directory or os . path . curdir if api_key is not None : os . environ [ \"DANDI_API_KEY\" ] = api_key dandiset_directory = os . path . join ( working_directory , str ( dandiset_id ) ) # enforce str download ( f \"https://gui-staging.dandiarchive.org/#/dandiset/ { dandiset_id } \" if staging else dandiset_id , output_dir = working_directory , ) subprocess . run ( [ \"dandi\" , \"organize\" , \"-d\" , dandiset_directory , data_directory , \"-f\" , \"dry\" ], shell = True , # without this param, subprocess interprets first arg as file/dir ) subprocess . run ( [ \"dandi\" , \"organize\" , \"-d\" , dandiset_directory , data_directory ], shell = True ) print ( f \"work_dir: { working_directory } \\n data_dir: { data_directory } \\n \" + f \"dand_dir: { dandiset_directory } \" ) upload ( [ dandiset_directory ], # dandiset_path=dandiset_directory, # dandi.upload has no such arg dandi_instance = \"dandi-staging\" if staging else \"dandi\" , sync = sync , )", "title": "dandi.py"}, {"location": "api/element_interface/dandi/#element_interface.dandi.upload_to_dandi", "text": "Upload NWB files to DANDI Archive Parameters: Name Type Description Default data_directory str directory that contains source data required dandiset_id str 6-digit zero-padded string required staging bool If true, use staging server. If false, use production server. True working_directory str Dir in which to create symlinked dandiset. Must have write permissions to this directory. Default is current directory. None api_key str Provide the DANDI API key if not already in an environmental variable DANDI_API_KEY None sync str If True, delete all files in archive that are not present in the local directory. False Source code in element_interface/dandi.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def upload_to_dandi ( data_directory : str , dandiset_id : str , staging : bool = True , working_directory : str = None , api_key : str = None , sync : bool = False , ): \"\"\"Upload NWB files to DANDI Archive Args: data_directory (str): directory that contains source data dandiset_id (str): 6-digit zero-padded string staging (bool): If true, use staging server. If false, use production server. working_directory (str, optional): Dir in which to create symlinked dandiset. Must have write permissions to this directory. Default is current directory. api_key (str, optional): Provide the DANDI API key if not already in an environmental variable DANDI_API_KEY sync (str, optional): If True, delete all files in archive that are not present in the local directory. \"\"\" working_directory = working_directory or os . path . curdir if api_key is not None : os . environ [ \"DANDI_API_KEY\" ] = api_key dandiset_directory = os . path . join ( working_directory , str ( dandiset_id ) ) # enforce str download ( f \"https://gui-staging.dandiarchive.org/#/dandiset/ { dandiset_id } \" if staging else dandiset_id , output_dir = working_directory , ) subprocess . run ( [ \"dandi\" , \"organize\" , \"-d\" , dandiset_directory , data_directory , \"-f\" , \"dry\" ], shell = True , # without this param, subprocess interprets first arg as file/dir ) subprocess . run ( [ \"dandi\" , \"organize\" , \"-d\" , dandiset_directory , data_directory ], shell = True ) print ( f \"work_dir: { working_directory } \\n data_dir: { data_directory } \\n \" + f \"dand_dir: { dandiset_directory } \" ) upload ( [ dandiset_directory ], # dandiset_path=dandiset_directory, # dandi.upload has no such arg dandi_instance = \"dandi-staging\" if staging else \"dandi\" , sync = sync , )", "title": "upload_to_dandi()"}, {"location": "api/element_interface/extract_loader/", "text": "EXTRACT_loader \u00b6 Source code in element_interface/extract_loader.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class EXTRACT_loader : def __init__ ( self , extract_dir : str ): \"\"\"Initialize EXTRACT loader class Args: extract_dir (str): string, absolute file path to EXTRACT directory Raises: FileNotFoundError: Could not find EXTRACT results \"\"\" from scipy.io import loadmat try : extract_file = next ( Path ( extract_dir ) . glob ( \"*_extract_output.mat\" )) except StopInteration : raise FileNotFoundError ( f \"EXTRACT output result file is not found at { extract_dir } .\" ) results = loadmat ( extract_file ) self . creation_time = datetime . fromtimestamp ( os . stat ( extract_file ) . st_ctime ) self . S = results [ \"output\" ][ 0 ][ \"spatial_weights\" ][ 0 ] # (Height, Width, MaskId) self . T = results [ \"output\" ][ 0 ][ \"temporal_weights\" ][ 0 ] # (Time, MaskId) def load_results ( self ): \"\"\"Load the EXTRACT results Returns: masks (dict): Details of the masks identified with the EXTRACT segmentation package. \"\"\" from scipy.sparse import find S_transposed = self . S . transpose ([ 2 , 0 , 1 ]) # MaskId, Height, Width masks = [] for mask_id , s in enumerate ( S_transposed ): ypixels , xpixels , weights = find ( s ) masks . append ( dict ( mask_id = mask_id , mask_npix = len ( weights ), mask_weights = weights , mask_center_x = int ( np . average ( xpixels , weights = weights ) + 0.5 ), mask_center_y = int ( np . average ( ypixels , weights = weights ) + 0.5 ), mask_center_z = None , mask_xpix = xpixels , mask_ypix = ypixels , mask_zpix = None , ) ) return masks __init__ ( extract_dir ) \u00b6 Initialize EXTRACT loader class Parameters: Name Type Description Default extract_dir str string, absolute file path to EXTRACT directory required Raises: Type Description FileNotFoundError Could not find EXTRACT results Source code in element_interface/extract_loader.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , extract_dir : str ): \"\"\"Initialize EXTRACT loader class Args: extract_dir (str): string, absolute file path to EXTRACT directory Raises: FileNotFoundError: Could not find EXTRACT results \"\"\" from scipy.io import loadmat try : extract_file = next ( Path ( extract_dir ) . glob ( \"*_extract_output.mat\" )) except StopInteration : raise FileNotFoundError ( f \"EXTRACT output result file is not found at { extract_dir } .\" ) results = loadmat ( extract_file ) self . creation_time = datetime . fromtimestamp ( os . stat ( extract_file ) . st_ctime ) self . S = results [ \"output\" ][ 0 ][ \"spatial_weights\" ][ 0 ] # (Height, Width, MaskId) self . T = results [ \"output\" ][ 0 ][ \"temporal_weights\" ][ 0 ] # (Time, MaskId) load_results () \u00b6 Load the EXTRACT results Returns: Name Type Description masks dict Details of the masks identified with the EXTRACT segmentation package. Source code in element_interface/extract_loader.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def load_results ( self ): \"\"\"Load the EXTRACT results Returns: masks (dict): Details of the masks identified with the EXTRACT segmentation package. \"\"\" from scipy.sparse import find S_transposed = self . S . transpose ([ 2 , 0 , 1 ]) # MaskId, Height, Width masks = [] for mask_id , s in enumerate ( S_transposed ): ypixels , xpixels , weights = find ( s ) masks . append ( dict ( mask_id = mask_id , mask_npix = len ( weights ), mask_weights = weights , mask_center_x = int ( np . average ( xpixels , weights = weights ) + 0.5 ), mask_center_y = int ( np . average ( ypixels , weights = weights ) + 0.5 ), mask_center_z = None , mask_xpix = xpixels , mask_ypix = ypixels , mask_zpix = None , ) ) return masks", "title": "extract_loader.py"}, {"location": "api/element_interface/extract_loader/#element_interface.extract_loader.EXTRACT_loader", "text": "Source code in element_interface/extract_loader.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 class EXTRACT_loader : def __init__ ( self , extract_dir : str ): \"\"\"Initialize EXTRACT loader class Args: extract_dir (str): string, absolute file path to EXTRACT directory Raises: FileNotFoundError: Could not find EXTRACT results \"\"\" from scipy.io import loadmat try : extract_file = next ( Path ( extract_dir ) . glob ( \"*_extract_output.mat\" )) except StopInteration : raise FileNotFoundError ( f \"EXTRACT output result file is not found at { extract_dir } .\" ) results = loadmat ( extract_file ) self . creation_time = datetime . fromtimestamp ( os . stat ( extract_file ) . st_ctime ) self . S = results [ \"output\" ][ 0 ][ \"spatial_weights\" ][ 0 ] # (Height, Width, MaskId) self . T = results [ \"output\" ][ 0 ][ \"temporal_weights\" ][ 0 ] # (Time, MaskId) def load_results ( self ): \"\"\"Load the EXTRACT results Returns: masks (dict): Details of the masks identified with the EXTRACT segmentation package. \"\"\" from scipy.sparse import find S_transposed = self . S . transpose ([ 2 , 0 , 1 ]) # MaskId, Height, Width masks = [] for mask_id , s in enumerate ( S_transposed ): ypixels , xpixels , weights = find ( s ) masks . append ( dict ( mask_id = mask_id , mask_npix = len ( weights ), mask_weights = weights , mask_center_x = int ( np . average ( xpixels , weights = weights ) + 0.5 ), mask_center_y = int ( np . average ( ypixels , weights = weights ) + 0.5 ), mask_center_z = None , mask_xpix = xpixels , mask_ypix = ypixels , mask_zpix = None , ) ) return masks", "title": "EXTRACT_loader"}, {"location": "api/element_interface/extract_loader/#element_interface.extract_loader.EXTRACT_loader.__init__", "text": "Initialize EXTRACT loader class Parameters: Name Type Description Default extract_dir str string, absolute file path to EXTRACT directory required Raises: Type Description FileNotFoundError Could not find EXTRACT results Source code in element_interface/extract_loader.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , extract_dir : str ): \"\"\"Initialize EXTRACT loader class Args: extract_dir (str): string, absolute file path to EXTRACT directory Raises: FileNotFoundError: Could not find EXTRACT results \"\"\" from scipy.io import loadmat try : extract_file = next ( Path ( extract_dir ) . glob ( \"*_extract_output.mat\" )) except StopInteration : raise FileNotFoundError ( f \"EXTRACT output result file is not found at { extract_dir } .\" ) results = loadmat ( extract_file ) self . creation_time = datetime . fromtimestamp ( os . stat ( extract_file ) . st_ctime ) self . S = results [ \"output\" ][ 0 ][ \"spatial_weights\" ][ 0 ] # (Height, Width, MaskId) self . T = results [ \"output\" ][ 0 ][ \"temporal_weights\" ][ 0 ] # (Time, MaskId)", "title": "__init__()"}, {"location": "api/element_interface/extract_loader/#element_interface.extract_loader.EXTRACT_loader.load_results", "text": "Load the EXTRACT results Returns: Name Type Description masks dict Details of the masks identified with the EXTRACT segmentation package. Source code in element_interface/extract_loader.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def load_results ( self ): \"\"\"Load the EXTRACT results Returns: masks (dict): Details of the masks identified with the EXTRACT segmentation package. \"\"\" from scipy.sparse import find S_transposed = self . S . transpose ([ 2 , 0 , 1 ]) # MaskId, Height, Width masks = [] for mask_id , s in enumerate ( S_transposed ): ypixels , xpixels , weights = find ( s ) masks . append ( dict ( mask_id = mask_id , mask_npix = len ( weights ), mask_weights = weights , mask_center_x = int ( np . average ( xpixels , weights = weights ) + 0.5 ), mask_center_y = int ( np . average ( ypixels , weights = weights ) + 0.5 ), mask_center_z = None , mask_xpix = xpixels , mask_ypix = ypixels , mask_zpix = None , ) ) return masks", "title": "load_results()"}, {"location": "api/element_interface/extract_trigger/", "text": "EXTRACT_trigger \u00b6 Source code in element_interface/extract_trigger.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class EXTRACT_trigger : m_template = dedent ( \"\"\" % Load Data data = load('{scanfile}'); M = data.M; % Input Paramaters config = struct(); {parameters_list_string} % Run EXTRACT output = extractor(M, config); save('{output_fullpath}', 'output'); \"\"\" ) def __init__ ( self , scanfile : Union [ str , Path ], parameters : dict , output_dir : Union [ str , Path ], ) -> None : \"\"\"A helper class to trigger EXTRACT analysis in element-calcium-imaging. Args: scanfile (Union[str, Path]): Full path of the scan parameters (dict): EXTRACT input paramaters. output_dir (Union[str, Path]): Directory to store the outputs of EXTRACT analysis. \"\"\" assert isinstance ( parameters , dict ) self . scanfile = Path ( scanfile ) self . output_dir = Path ( output_dir ) self . parameters = parameters def write_matlab_run_script ( self ): \"\"\"Compose a matlab script and save it with the name run_extract.m. The composed script is basically the formatted version of the m_template attribute.\"\"\" self . output_fullpath = ( self . output_dir / f \" { self . scanfile . stem } _extract_output.mat\" ) m_file_content = self . m_template . format ( ** dict ( parameters_list_string = \" \\n \" . join ( [ f \"config. { k } = ' { v } ';\" if isinstance ( v , str ) else f \"config. { k } = { str ( v ) . lower () } ;\" if isinstance ( v , bool ) else f \"config. { k } = { v } ;\" for k , v in self . parameters . items () ] ), scanfile = self . scanfile . as_posix (), output_fullpath = self . output_fullpath . as_posix (), ) ) . lstrip () self . m_file_fp = self . output_dir / \"run_extract.m\" with open ( self . m_file_fp , \"w\" ) as f : f . write ( m_file_content ) def run ( self ): \"\"\"Run the matlab run_extract.m script.\"\"\" self . write_matlab_run_script () current_dir = Path . cwd () os . chdir ( self . output_dir ) try : import matlab.engine eng = matlab . engine . start_matlab () eng . run_extract () except Exception as e : raise e finally : os . chdir ( current_dir ) __init__ ( scanfile , parameters , output_dir ) \u00b6 A helper class to trigger EXTRACT analysis in element-calcium-imaging. Parameters: Name Type Description Default scanfile Union [ str , Path ] Full path of the scan required parameters dict EXTRACT input paramaters. required output_dir Union [ str , Path ] Directory to store the outputs of EXTRACT analysis. required Source code in element_interface/extract_trigger.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , scanfile : Union [ str , Path ], parameters : dict , output_dir : Union [ str , Path ], ) -> None : \"\"\"A helper class to trigger EXTRACT analysis in element-calcium-imaging. Args: scanfile (Union[str, Path]): Full path of the scan parameters (dict): EXTRACT input paramaters. output_dir (Union[str, Path]): Directory to store the outputs of EXTRACT analysis. \"\"\" assert isinstance ( parameters , dict ) self . scanfile = Path ( scanfile ) self . output_dir = Path ( output_dir ) self . parameters = parameters run () \u00b6 Run the matlab run_extract.m script. Source code in element_interface/extract_trigger.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def run ( self ): \"\"\"Run the matlab run_extract.m script.\"\"\" self . write_matlab_run_script () current_dir = Path . cwd () os . chdir ( self . output_dir ) try : import matlab.engine eng = matlab . engine . start_matlab () eng . run_extract () except Exception as e : raise e finally : os . chdir ( current_dir ) write_matlab_run_script () \u00b6 Compose a matlab script and save it with the name run_extract.m. The composed script is basically the formatted version of the m_template attribute. Source code in element_interface/extract_trigger.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def write_matlab_run_script ( self ): \"\"\"Compose a matlab script and save it with the name run_extract.m. The composed script is basically the formatted version of the m_template attribute.\"\"\" self . output_fullpath = ( self . output_dir / f \" { self . scanfile . stem } _extract_output.mat\" ) m_file_content = self . m_template . format ( ** dict ( parameters_list_string = \" \\n \" . join ( [ f \"config. { k } = ' { v } ';\" if isinstance ( v , str ) else f \"config. { k } = { str ( v ) . lower () } ;\" if isinstance ( v , bool ) else f \"config. { k } = { v } ;\" for k , v in self . parameters . items () ] ), scanfile = self . scanfile . as_posix (), output_fullpath = self . output_fullpath . as_posix (), ) ) . lstrip () self . m_file_fp = self . output_dir / \"run_extract.m\" with open ( self . m_file_fp , \"w\" ) as f : f . write ( m_file_content )", "title": "extract_trigger.py"}, {"location": "api/element_interface/extract_trigger/#element_interface.extract_trigger.EXTRACT_trigger", "text": "Source code in element_interface/extract_trigger.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 class EXTRACT_trigger : m_template = dedent ( \"\"\" % Load Data data = load('{scanfile}'); M = data.M; % Input Paramaters config = struct(); {parameters_list_string} % Run EXTRACT output = extractor(M, config); save('{output_fullpath}', 'output'); \"\"\" ) def __init__ ( self , scanfile : Union [ str , Path ], parameters : dict , output_dir : Union [ str , Path ], ) -> None : \"\"\"A helper class to trigger EXTRACT analysis in element-calcium-imaging. Args: scanfile (Union[str, Path]): Full path of the scan parameters (dict): EXTRACT input paramaters. output_dir (Union[str, Path]): Directory to store the outputs of EXTRACT analysis. \"\"\" assert isinstance ( parameters , dict ) self . scanfile = Path ( scanfile ) self . output_dir = Path ( output_dir ) self . parameters = parameters def write_matlab_run_script ( self ): \"\"\"Compose a matlab script and save it with the name run_extract.m. The composed script is basically the formatted version of the m_template attribute.\"\"\" self . output_fullpath = ( self . output_dir / f \" { self . scanfile . stem } _extract_output.mat\" ) m_file_content = self . m_template . format ( ** dict ( parameters_list_string = \" \\n \" . join ( [ f \"config. { k } = ' { v } ';\" if isinstance ( v , str ) else f \"config. { k } = { str ( v ) . lower () } ;\" if isinstance ( v , bool ) else f \"config. { k } = { v } ;\" for k , v in self . parameters . items () ] ), scanfile = self . scanfile . as_posix (), output_fullpath = self . output_fullpath . as_posix (), ) ) . lstrip () self . m_file_fp = self . output_dir / \"run_extract.m\" with open ( self . m_file_fp , \"w\" ) as f : f . write ( m_file_content ) def run ( self ): \"\"\"Run the matlab run_extract.m script.\"\"\" self . write_matlab_run_script () current_dir = Path . cwd () os . chdir ( self . output_dir ) try : import matlab.engine eng = matlab . engine . start_matlab () eng . run_extract () except Exception as e : raise e finally : os . chdir ( current_dir )", "title": "EXTRACT_trigger"}, {"location": "api/element_interface/extract_trigger/#element_interface.extract_trigger.EXTRACT_trigger.__init__", "text": "A helper class to trigger EXTRACT analysis in element-calcium-imaging. Parameters: Name Type Description Default scanfile Union [ str , Path ] Full path of the scan required parameters dict EXTRACT input paramaters. required output_dir Union [ str , Path ] Directory to store the outputs of EXTRACT analysis. required Source code in element_interface/extract_trigger.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , scanfile : Union [ str , Path ], parameters : dict , output_dir : Union [ str , Path ], ) -> None : \"\"\"A helper class to trigger EXTRACT analysis in element-calcium-imaging. Args: scanfile (Union[str, Path]): Full path of the scan parameters (dict): EXTRACT input paramaters. output_dir (Union[str, Path]): Directory to store the outputs of EXTRACT analysis. \"\"\" assert isinstance ( parameters , dict ) self . scanfile = Path ( scanfile ) self . output_dir = Path ( output_dir ) self . parameters = parameters", "title": "__init__()"}, {"location": "api/element_interface/extract_trigger/#element_interface.extract_trigger.EXTRACT_trigger.run", "text": "Run the matlab run_extract.m script. Source code in element_interface/extract_trigger.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def run ( self ): \"\"\"Run the matlab run_extract.m script.\"\"\" self . write_matlab_run_script () current_dir = Path . cwd () os . chdir ( self . output_dir ) try : import matlab.engine eng = matlab . engine . start_matlab () eng . run_extract () except Exception as e : raise e finally : os . chdir ( current_dir )", "title": "run()"}, {"location": "api/element_interface/extract_trigger/#element_interface.extract_trigger.EXTRACT_trigger.write_matlab_run_script", "text": "Compose a matlab script and save it with the name run_extract.m. The composed script is basically the formatted version of the m_template attribute. Source code in element_interface/extract_trigger.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def write_matlab_run_script ( self ): \"\"\"Compose a matlab script and save it with the name run_extract.m. The composed script is basically the formatted version of the m_template attribute.\"\"\" self . output_fullpath = ( self . output_dir / f \" { self . scanfile . stem } _extract_output.mat\" ) m_file_content = self . m_template . format ( ** dict ( parameters_list_string = \" \\n \" . join ( [ f \"config. { k } = ' { v } ';\" if isinstance ( v , str ) else f \"config. { k } = { str ( v ) . lower () } ;\" if isinstance ( v , bool ) else f \"config. { k } = { v } ;\" for k , v in self . parameters . items () ] ), scanfile = self . scanfile . as_posix (), output_fullpath = self . output_fullpath . as_posix (), ) ) . lstrip () self . m_file_fp = self . output_dir / \"run_extract.m\" with open ( self . m_file_fp , \"w\" ) as f : f . write ( m_file_content )", "title": "write_matlab_run_script()"}, {"location": "api/element_interface/prairieviewreader/", "text": "get_pv_metadata ( pvtiffile ) \u00b6 Extract metadata for scans generated by PrairieView acquisition software. The PrairieView software generates one .ome.tif imaging file per frame acquired. The metadata for all frames is contained one .xml file. This function locates the .xml file and generates a dictionary necessary to populate the DataJoint ScanInfo and Field tables. PrairieView works with resonance scanners with a single field. PrairieView does not support bidirectional x and y scanning. ROI information is not contained in the .xml file. All images generated using PrairieView have square dimensions(e.g. 512x512). Parameters: Name Type Description Default pvtiffile str An absolute path to the .ome.tif image file. required Raises: Type Description FileNotFoundError No .xml file containing information about the acquired scan was found at path in parent directory at pvtiffile . Returns: Name Type Description metainfo dict A dict mapping keys to corresponding metadata values fetched from the .xml file. Source code in element_interface/prairieviewreader.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def get_pv_metadata ( pvtiffile : str ) -> dict : \"\"\"Extract metadata for scans generated by PrairieView acquisition software. The PrairieView software generates one .ome.tif imaging file per frame acquired. The metadata for all frames is contained one .xml file. This function locates the .xml file and generates a dictionary necessary to populate the DataJoint ScanInfo and Field tables. PrairieView works with resonance scanners with a single field. PrairieView does not support bidirectional x and y scanning. ROI information is not contained in the .xml file. All images generated using PrairieView have square dimensions(e.g. 512x512). Args: pvtiffile: An absolute path to the .ome.tif image file. Raises: FileNotFoundError: No .xml file containing information about the acquired scan was found at path in parent directory at `pvtiffile`. Returns: metainfo: A dict mapping keys to corresponding metadata values fetched from the .xml file. \"\"\" # May return multiple xml files. Only need one that contains scan metadata. xml_files = pathlib . Path ( pvtiffile ) . parent . glob ( \"*.xml\" ) for xml_file in xml_files : tree = ET . parse ( xml_file ) root = tree . getroot () if root . find ( \".//Sequence\" ): break else : raise FileNotFoundError ( f \"No PrarieView metadata XML file found at { pvtiffile . parent } \" ) bidirectional_scan = False # Does not support bidirectional n_fields = 1 # Always contains 1 field # Get all channels and find unique values channel_list = [ int ( channel . attrib . get ( \"channel\" )) for channel in root . iterfind ( \".//Sequence/Frame/File/[@channel]\" ) ] n_channels = len ( set ( channel_list )) # One \"Frame\" per depth. Gets number of frames in first sequence planes = [ int ( plane . attrib . get ( \"index\" )) for plane in root . findall ( \".//Sequence/[@cycle='1']/Frame\" ) ] n_depths = len ( set ( planes )) n_frames = len ( root . findall ( \".//Sequence/Frame\" )) roi = 1 # x and y coordinate values for the center of the field x_field = float ( root . find ( \".//PVStateValue/[@key='currentScanCenter']/IndexedValue/[@index='XAxis']\" ) . attrib . get ( \"value\" ) ) y_field = float ( root . find ( \".//PVStateValue/[@key='currentScanCenter']/IndexedValue/[@index='YAxis']\" ) . attrib . get ( \"value\" ) ) framerate = 1 / float ( root . findall ( './/PVStateValue/[@key=\"framePeriod\"]' )[ 0 ] . attrib . get ( \"value\" ) ) # rate = 1/framePeriod usec_per_line = ( float ( root . findall ( \".//PVStateValue/[@key='scanLinePeriod']\" )[ 0 ] . attrib . get ( \"value\" ) ) * 1e6 ) # Convert from seconds to microseconds scan_datetime = datetime . strptime ( root . attrib . get ( \"date\" ), \"%m/ %d /%Y %I:%M:%S %p\" ) total_duration = float ( root . findall ( \".//Sequence/Frame\" )[ - 1 ] . attrib . get ( \"relativeTime\" ) ) bidirection_z = bool ( root . find ( \".//Sequence\" ) . attrib . get ( \"bidirectionalZ\" )) px_height = int ( root . findall ( \".//PVStateValue/[@key='pixelsPerLine']\" )[ 0 ] . attrib . get ( \"value\" ) ) # All PrairieView-acquired images have square dimensions (512 x 512; 1024 x 1024) px_width = px_height um_per_pixel = float ( root . find ( \".//PVStateValue/[@key='micronsPerPixel']/IndexedValue/[@index='XAxis']\" ) . attrib . get ( \"value\" ) ) um_height = um_width = float ( px_height ) * um_per_pixel z_min = float ( root . findall ( \".//Sequence/[@cycle='1']/Frame/PVStateShard/PVStateValue/[@key='positionCurrent']/SubindexedValues/SubindexedValue/[@subindex='0']\" )[ 0 ] . attrib . get ( \"value\" ) ) z_max = float ( root . findall ( \".//Sequence/[@cycle='1']/Frame/PVStateShard/PVStateValue/[@key='positionCurrent']/SubindexedValues/SubindexedValue/[@subindex='0']\" )[ - 1 ] . attrib . get ( \"value\" ) ) z_step = float ( root . find ( \".//PVStateShard/PVStateValue/[@key='micronsPerPixel']/IndexedValue/[@index='ZAxis']\" ) . attrib . get ( \"value\" ) ) z_fields = np . arange ( z_min , z_max + 1 , z_step ) assert z_fields . size == n_depths metainfo = dict ( num_fields = n_fields , num_channels = n_channels , num_planes = n_depths , num_frames = n_frames , num_rois = roi , x_pos = None , y_pos = None , z_pos = None , frame_rate = framerate , bidirectional = bidirectional_scan , bidirectional_z = bidirection_z , scan_datetime = scan_datetime , usecs_per_line = usec_per_line , scan_duration = total_duration , height_in_pixels = px_height , width_in_pixels = px_width , height_in_um = um_height , width_in_um = um_width , fieldX = x_field , fieldY = y_field , fieldZ = z_fields , ) return metainfo", "title": "prairieviewreader.py"}, {"location": "api/element_interface/prairieviewreader/#element_interface.prairieviewreader.get_pv_metadata", "text": "Extract metadata for scans generated by PrairieView acquisition software. The PrairieView software generates one .ome.tif imaging file per frame acquired. The metadata for all frames is contained one .xml file. This function locates the .xml file and generates a dictionary necessary to populate the DataJoint ScanInfo and Field tables. PrairieView works with resonance scanners with a single field. PrairieView does not support bidirectional x and y scanning. ROI information is not contained in the .xml file. All images generated using PrairieView have square dimensions(e.g. 512x512). Parameters: Name Type Description Default pvtiffile str An absolute path to the .ome.tif image file. required Raises: Type Description FileNotFoundError No .xml file containing information about the acquired scan was found at path in parent directory at pvtiffile . Returns: Name Type Description metainfo dict A dict mapping keys to corresponding metadata values fetched from the .xml file. Source code in element_interface/prairieviewreader.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def get_pv_metadata ( pvtiffile : str ) -> dict : \"\"\"Extract metadata for scans generated by PrairieView acquisition software. The PrairieView software generates one .ome.tif imaging file per frame acquired. The metadata for all frames is contained one .xml file. This function locates the .xml file and generates a dictionary necessary to populate the DataJoint ScanInfo and Field tables. PrairieView works with resonance scanners with a single field. PrairieView does not support bidirectional x and y scanning. ROI information is not contained in the .xml file. All images generated using PrairieView have square dimensions(e.g. 512x512). Args: pvtiffile: An absolute path to the .ome.tif image file. Raises: FileNotFoundError: No .xml file containing information about the acquired scan was found at path in parent directory at `pvtiffile`. Returns: metainfo: A dict mapping keys to corresponding metadata values fetched from the .xml file. \"\"\" # May return multiple xml files. Only need one that contains scan metadata. xml_files = pathlib . Path ( pvtiffile ) . parent . glob ( \"*.xml\" ) for xml_file in xml_files : tree = ET . parse ( xml_file ) root = tree . getroot () if root . find ( \".//Sequence\" ): break else : raise FileNotFoundError ( f \"No PrarieView metadata XML file found at { pvtiffile . parent } \" ) bidirectional_scan = False # Does not support bidirectional n_fields = 1 # Always contains 1 field # Get all channels and find unique values channel_list = [ int ( channel . attrib . get ( \"channel\" )) for channel in root . iterfind ( \".//Sequence/Frame/File/[@channel]\" ) ] n_channels = len ( set ( channel_list )) # One \"Frame\" per depth. Gets number of frames in first sequence planes = [ int ( plane . attrib . get ( \"index\" )) for plane in root . findall ( \".//Sequence/[@cycle='1']/Frame\" ) ] n_depths = len ( set ( planes )) n_frames = len ( root . findall ( \".//Sequence/Frame\" )) roi = 1 # x and y coordinate values for the center of the field x_field = float ( root . find ( \".//PVStateValue/[@key='currentScanCenter']/IndexedValue/[@index='XAxis']\" ) . attrib . get ( \"value\" ) ) y_field = float ( root . find ( \".//PVStateValue/[@key='currentScanCenter']/IndexedValue/[@index='YAxis']\" ) . attrib . get ( \"value\" ) ) framerate = 1 / float ( root . findall ( './/PVStateValue/[@key=\"framePeriod\"]' )[ 0 ] . attrib . get ( \"value\" ) ) # rate = 1/framePeriod usec_per_line = ( float ( root . findall ( \".//PVStateValue/[@key='scanLinePeriod']\" )[ 0 ] . attrib . get ( \"value\" ) ) * 1e6 ) # Convert from seconds to microseconds scan_datetime = datetime . strptime ( root . attrib . get ( \"date\" ), \"%m/ %d /%Y %I:%M:%S %p\" ) total_duration = float ( root . findall ( \".//Sequence/Frame\" )[ - 1 ] . attrib . get ( \"relativeTime\" ) ) bidirection_z = bool ( root . find ( \".//Sequence\" ) . attrib . get ( \"bidirectionalZ\" )) px_height = int ( root . findall ( \".//PVStateValue/[@key='pixelsPerLine']\" )[ 0 ] . attrib . get ( \"value\" ) ) # All PrairieView-acquired images have square dimensions (512 x 512; 1024 x 1024) px_width = px_height um_per_pixel = float ( root . find ( \".//PVStateValue/[@key='micronsPerPixel']/IndexedValue/[@index='XAxis']\" ) . attrib . get ( \"value\" ) ) um_height = um_width = float ( px_height ) * um_per_pixel z_min = float ( root . findall ( \".//Sequence/[@cycle='1']/Frame/PVStateShard/PVStateValue/[@key='positionCurrent']/SubindexedValues/SubindexedValue/[@subindex='0']\" )[ 0 ] . attrib . get ( \"value\" ) ) z_max = float ( root . findall ( \".//Sequence/[@cycle='1']/Frame/PVStateShard/PVStateValue/[@key='positionCurrent']/SubindexedValues/SubindexedValue/[@subindex='0']\" )[ - 1 ] . attrib . get ( \"value\" ) ) z_step = float ( root . find ( \".//PVStateShard/PVStateValue/[@key='micronsPerPixel']/IndexedValue/[@index='ZAxis']\" ) . attrib . get ( \"value\" ) ) z_fields = np . arange ( z_min , z_max + 1 , z_step ) assert z_fields . size == n_depths metainfo = dict ( num_fields = n_fields , num_channels = n_channels , num_planes = n_depths , num_frames = n_frames , num_rois = roi , x_pos = None , y_pos = None , z_pos = None , frame_rate = framerate , bidirectional = bidirectional_scan , bidirectional_z = bidirection_z , scan_datetime = scan_datetime , usecs_per_line = usec_per_line , scan_duration = total_duration , height_in_pixels = px_height , width_in_pixels = px_width , height_in_um = um_height , width_in_um = um_width , fieldX = x_field , fieldY = y_field , fieldZ = z_fields , ) return metainfo", "title": "get_pv_metadata()"}, {"location": "api/element_interface/run_caiman/", "text": "run_caiman ( file_paths , parameters , sampling_rate , output_dir , is3D ) \u00b6 Runs the standard caiman analysis pipeline (CNMF.fit_file method). Parameters: Name Type Description Default file_paths list Image (full) paths required parameters dict Caiman parameters required sampling_rate float Image sampling rate (Hz) required output_dir str Output directory required is3D bool the data is 3D required Source code in element_interface/run_caiman.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def run_caiman ( file_paths : list , parameters : dict , sampling_rate : float , output_dir : str , is3D : bool , ): \"\"\" Runs the standard caiman analysis pipeline (CNMF.fit_file method). Args: file_paths (list): Image (full) paths parameters (dict): Caiman parameters sampling_rate (float): Image sampling rate (Hz) output_dir (str): Output directory is3D (bool): the data is 3D \"\"\" parameters [ \"is3D\" ] = is3D parameters [ \"fnames\" ] = file_paths parameters [ \"fr\" ] = sampling_rate opts = params . CNMFParams ( params_dict = parameters ) c , dview , n_processes = cm . cluster . setup_cluster ( backend = \"local\" , n_processes = None , single_thread = False ) cnm = CNMF ( n_processes , params = opts , dview = dview ) cnmf_output , mc_output = cnm . fit_file ( motion_correct = True , include_eval = True , output_dir = output_dir , return_mc = True ) cm . stop_server ( dview = dview ) cnmf_output_file = pathlib . Path ( cnmf_output . mmap_file [: - 4 ] + \"hdf5\" ) assert cnmf_output_file . exists () assert cnmf_output_file . parent == pathlib . Path ( output_dir ) _save_mc ( mc_output , cnmf_output_file . as_posix (), parameters [ \"is3D\" ])", "title": "run_caiman.py"}, {"location": "api/element_interface/run_caiman/#element_interface.run_caiman.run_caiman", "text": "Runs the standard caiman analysis pipeline (CNMF.fit_file method). Parameters: Name Type Description Default file_paths list Image (full) paths required parameters dict Caiman parameters required sampling_rate float Image sampling rate (Hz) required output_dir str Output directory required is3D bool the data is 3D required Source code in element_interface/run_caiman.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def run_caiman ( file_paths : list , parameters : dict , sampling_rate : float , output_dir : str , is3D : bool , ): \"\"\" Runs the standard caiman analysis pipeline (CNMF.fit_file method). Args: file_paths (list): Image (full) paths parameters (dict): Caiman parameters sampling_rate (float): Image sampling rate (Hz) output_dir (str): Output directory is3D (bool): the data is 3D \"\"\" parameters [ \"is3D\" ] = is3D parameters [ \"fnames\" ] = file_paths parameters [ \"fr\" ] = sampling_rate opts = params . CNMFParams ( params_dict = parameters ) c , dview , n_processes = cm . cluster . setup_cluster ( backend = \"local\" , n_processes = None , single_thread = False ) cnm = CNMF ( n_processes , params = opts , dview = dview ) cnmf_output , mc_output = cnm . fit_file ( motion_correct = True , include_eval = True , output_dir = output_dir , return_mc = True ) cm . stop_server ( dview = dview ) cnmf_output_file = pathlib . Path ( cnmf_output . mmap_file [: - 4 ] + \"hdf5\" ) assert cnmf_output_file . exists () assert cnmf_output_file . parent == pathlib . Path ( output_dir ) _save_mc ( mc_output , cnmf_output_file . as_posix (), parameters [ \"is3D\" ])", "title": "run_caiman()"}, {"location": "api/element_interface/scanimage_utils/", "text": "get_scanimage_acq_time ( scan ) \u00b6 Return ScanImage acquisition time Example loaded_scan = scanreader.read_scan(scan_filepath) header = scanimage_utils.parse_scanimage_header(loaded_scan) Parameters: Name Type Description Default scan scanimage object ScanImage object with header required Returns: Name Type Description time str acquisition time in %Y %m %d %H %M %S format Source code in element_interface/scanimage_utils.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def get_scanimage_acq_time ( scan ): \"\"\"Return ScanImage acquisition time Example: > loaded_scan = scanreader.read_scan(scan_filepath) > header = scanimage_utils.parse_scanimage_header(loaded_scan) Args: scan (scanimage object): ScanImage object with header Returns: time (str): acquisition time in %Y %m %d %H %M %S format \"\"\" header = parse_scanimage_header ( scan ) recording_time = datetime . strptime ( ( header [ \"epoch\" ][ 1 : - 1 ]) . replace ( \",\" , \" \" ), \"%Y %m %d %H %M %S. %f \" ) return recording_time parse_scanimage_header ( scan ) \u00b6 Parse ScanImage header Example loaded_scan = scanreader.read_scan(scan_filepath) recording_time = scanimage_utils.get_scanimage_acq_time(loaded_scan) Parameters: Name Type Description Default scan scanimage object ScanImage object including a header property required Returns: Name Type Description header dict ScanImage header as key-value dictionary Source code in element_interface/scanimage_utils.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def parse_scanimage_header ( scan ): \"\"\"Parse ScanImage header Example: > loaded_scan = scanreader.read_scan(scan_filepath) > recording_time = scanimage_utils.get_scanimage_acq_time(loaded_scan) Args: scan (scanimage object): ScanImage object including a header property Returns: header (dict): ScanImage header as key-value dictionary \"\"\" header = {} for item in scan . header . split ( \" \\n \" ): try : key , value = item . split ( \" = \" ) key = re . sub ( \"^scanimage_\" , \"\" , key . replace ( \".\" , \"_\" )) header [ key ] = value except : pass return header", "title": "scanimage_utils.py"}, {"location": "api/element_interface/scanimage_utils/#element_interface.scanimage_utils.get_scanimage_acq_time", "text": "Return ScanImage acquisition time Example loaded_scan = scanreader.read_scan(scan_filepath) header = scanimage_utils.parse_scanimage_header(loaded_scan) Parameters: Name Type Description Default scan scanimage object ScanImage object with header required Returns: Name Type Description time str acquisition time in %Y %m %d %H %M %S format Source code in element_interface/scanimage_utils.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def get_scanimage_acq_time ( scan ): \"\"\"Return ScanImage acquisition time Example: > loaded_scan = scanreader.read_scan(scan_filepath) > header = scanimage_utils.parse_scanimage_header(loaded_scan) Args: scan (scanimage object): ScanImage object with header Returns: time (str): acquisition time in %Y %m %d %H %M %S format \"\"\" header = parse_scanimage_header ( scan ) recording_time = datetime . strptime ( ( header [ \"epoch\" ][ 1 : - 1 ]) . replace ( \",\" , \" \" ), \"%Y %m %d %H %M %S. %f \" ) return recording_time", "title": "get_scanimage_acq_time()"}, {"location": "api/element_interface/scanimage_utils/#element_interface.scanimage_utils.parse_scanimage_header", "text": "Parse ScanImage header Example loaded_scan = scanreader.read_scan(scan_filepath) recording_time = scanimage_utils.get_scanimage_acq_time(loaded_scan) Parameters: Name Type Description Default scan scanimage object ScanImage object including a header property required Returns: Name Type Description header dict ScanImage header as key-value dictionary Source code in element_interface/scanimage_utils.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def parse_scanimage_header ( scan ): \"\"\"Parse ScanImage header Example: > loaded_scan = scanreader.read_scan(scan_filepath) > recording_time = scanimage_utils.get_scanimage_acq_time(loaded_scan) Args: scan (scanimage object): ScanImage object including a header property Returns: header (dict): ScanImage header as key-value dictionary \"\"\" header = {} for item in scan . header . split ( \" \\n \" ): try : key , value = item . split ( \" = \" ) key = re . sub ( \"^scanimage_\" , \"\" , key . replace ( \".\" , \"_\" )) header [ key ] = value except : pass return header", "title": "parse_scanimage_header()"}, {"location": "api/element_interface/suite2p_loader/", "text": "PlaneSuite2p \u00b6 Parse the suite2p output directory and load data, per plane . Suite2p output doc: https://suite2p.readthedocs.io/en/latest/outputs.html Expecting the following files ops: Options file Fneu: Neuropil traces file for functional channel Fneu_chan2: Neuropil traces file for channel 2 F: Fluorescence traces for functional channel F_chan2: Fluorescence traces for channel 2 iscell: Array of (user curated) cells and probability of being a cell spks: Spikes (raw deconvolved with OASIS package) stat: Various statistics for each cell redcell: \"Red cell\" (second channel) stats Attributes: Name Type Description alignment_channel ops[\"align_by_chan\"] as zero-indexed cell_prob correlation_map ops[\"Vcorr\"] creation_time earliest file creation time across planes curation_time latest curation time across planes F Fluorescence traces for functional channel as numpy array if exists If does not exist, returns empty list F_chan2 Fluorescence traces for channel 2 as numpy array if exists If does not exist, returns empty lists Fneu Neuropil traces file for functional channel as numpy array if exists If does not exist, returns empty list Fneu_chan2 Neuropil traces file for channel 2 as numpy array if exists If does not exist, returns empty list fpath path to plane folder iscell max_proj_image ops[\"max_proj\"] if exists. Else np.full_like(mean_image)) mean_image ops[\"meanImg\"] ops Options file as numpy array plane_idx plane index. -1 if combined, else number in path redcell \"Red cell\" (second channel) stats as numpy array if exists If does not exist, returns empty list ref_image ops[\"refImg\"] segmentation_channel ops[\"functional_chan\"] as zero-indexed spks Spikes (raw deconvolved with OASIS package) as numpy array if exists If does not exist, returns empty lists stat Various statistics for each cell as numpy array if exists If does not exist, returns empty lists Source code in element_interface/suite2p_loader.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 class PlaneSuite2p : \"\"\"Parse the suite2p output directory and load data, ***per plane***. Suite2p output doc: https://suite2p.readthedocs.io/en/latest/outputs.html Expecting the following files: - ops: Options file - Fneu: Neuropil traces file for functional channel - Fneu_chan2: Neuropil traces file for channel 2 - F: Fluorescence traces for functional channel - F_chan2: Fluorescence traces for channel 2 - iscell: Array of (user curated) cells and probability of being a cell - spks: Spikes (raw deconvolved with OASIS package) - stat: Various statistics for each cell - redcell: \"Red cell\" (second channel) stats Attributes: alignment_channel: ops[\"align_by_chan\"] as zero-indexed cell_prob: correlation_map: ops[\"Vcorr\"] creation_time: earliest file creation time across planes curation_time: latest curation time across planes F: Fluorescence traces for functional channel as numpy array if exists If does not exist, returns empty list F_chan2: Fluorescence traces for channel 2 as numpy array if exists If does not exist, returns empty lists Fneu: Neuropil traces file for functional channel as numpy array if exists If does not exist, returns empty list Fneu_chan2: Neuropil traces file for channel 2 as numpy array if exists If does not exist, returns empty list fpath: path to plane folder iscell: max_proj_image: ops[\"max_proj\"] if exists. Else np.full_like(mean_image)) mean_image: ops[\"meanImg\"] ops: Options file as numpy array plane_idx: plane index. -1 if combined, else number in path redcell: \"Red cell\" (second channel) stats as numpy array if exists If does not exist, returns empty list ref_image: ops[\"refImg\"] segmentation_channel: ops[\"functional_chan\"] as zero-indexed spks: Spikes (raw deconvolved with OASIS package) as numpy array if exists If does not exist, returns empty lists stat: Various statistics for each cell as numpy array if exists If does not exist, returns empty lists \"\"\" def __init__ ( self , suite2p_plane_dir : str ): \"\"\"Initialize PlaneSuite2p class given a plane directory Args: suite2p_plane_dir (str): Suite2p plane directory Raises: FileNotFoundError: No \"ops.npy\" found. Invalid suite2p plane folder FileNotFoundError: No \"iscell.npy\" found. Invalid suite2p plane folder \"\"\" self . fpath = pathlib . Path ( suite2p_plane_dir ) # -- Verify dataset exists -- ops_fp = self . fpath / \"ops.npy\" if not ops_fp . exists (): raise FileNotFoundError ( 'No \"ops.npy\" found. Invalid suite2p plane folder: {} ' . format ( self . fpath ) ) self . creation_time = datetime . fromtimestamp ( ops_fp . stat () . st_ctime ) iscell_fp = self . fpath / \"iscell.npy\" if not iscell_fp . exists (): raise FileNotFoundError ( 'No \"iscell.npy\" found. Invalid suite2p plane folder: {} ' . format ( self . fpath ) ) self . curation_time = datetime . fromtimestamp ( iscell_fp . stat () . st_ctime ) # -- Initialize attributes -- for s2p_type in _suite2p_ftypes : setattr ( self , \"_ {} \" . format ( s2p_type ), None ) self . _cell_prob = None self . plane_idx = ( - 1 if self . fpath . name == \"combined\" else int ( self . fpath . name . replace ( \"plane\" , \"\" )) ) # -- load core files -- @property def ops ( self ): if self . _ops is None : fp = self . fpath / \"ops.npy\" self . _ops = np . load ( fp , allow_pickle = True ) . item () return self . _ops @property def Fneu ( self ): if self . _Fneu is None : fp = self . fpath / \"Fneu.npy\" self . _Fneu = np . load ( fp ) if fp . exists () else [] return self . _Fneu @property def Fneu_chan2 ( self ): if self . _Fneu_chan2 is None : fp = self . fpath / \"Fneu_chan2.npy\" self . _Fneu_chan2 = np . load ( fp ) if fp . exists () else [] return self . _Fneu_chan2 @property def F ( self ): if self . _F is None : fp = self . fpath / \"F.npy\" self . _F = np . load ( fp ) if fp . exists () else [] return self . _F @property def F_chan2 ( self ): if self . _F_chan2 is None : fp = self . fpath / \"F_chan2.npy\" self . _F_chan2 = np . load ( fp ) if fp . exists () else [] return self . _F_chan2 @property def iscell ( self ): if self . _iscell is None : fp = self . fpath / \"iscell.npy\" d = np . load ( fp ) self . _iscell = d [:, 0 ] . astype ( bool ) self . _cell_prob = d [:, 1 ] return self . _iscell @property def cell_prob ( self ): if self . _cell_prob is None : fp = self . fpath / \"iscell.npy\" if fp . exists (): d = np . load ( fp ) self . _iscell = d [:, 0 ] . astype ( bool ) self . _cell_prob = d [:, 1 ] return self . _cell_prob @property def spks ( self ): if self . _spks is None : fp = self . fpath / \"spks.npy\" self . _spks = np . load ( fp ) if fp . exists () else [] return self . _spks @property def stat ( self ): if self . _stat is None : fp = self . fpath / \"stat.npy\" self . _stat = np . load ( fp , allow_pickle = True ) if fp . exists () else [] return self . _stat @property def redcell ( self ): if self . _redcell is None : fp = self . fpath / \"redcell.npy\" self . _redcell = np . load ( fp ) if fp . exists () else [] return self . _redcell # -- image property -- @property def ref_image ( self ): return self . ops [ \"refImg\" ] @property def mean_image ( self ): return self . ops [ \"meanImg\" ] @property def max_proj_image ( self ): return self . ops . get ( \"max_proj\" , np . full_like ( self . mean_image , np . nan )) @property def correlation_map ( self ): return self . ops [ \"Vcorr\" ] @property def alignment_channel ( self ): return self . ops [ \"align_by_chan\" ] - 1 # suite2p is 1-based, convert to 0-based @property def segmentation_channel ( self ): return self . ops [ \"functional_chan\" ] - 1 # suite2p is 1-based, convert to 0-based __init__ ( suite2p_plane_dir ) \u00b6 Initialize PlaneSuite2p class given a plane directory Parameters: Name Type Description Default suite2p_plane_dir str Suite2p plane directory required Raises: Type Description FileNotFoundError No \"ops.npy\" found. Invalid suite2p plane folder FileNotFoundError No \"iscell.npy\" found. Invalid suite2p plane folder Source code in element_interface/suite2p_loader.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def __init__ ( self , suite2p_plane_dir : str ): \"\"\"Initialize PlaneSuite2p class given a plane directory Args: suite2p_plane_dir (str): Suite2p plane directory Raises: FileNotFoundError: No \"ops.npy\" found. Invalid suite2p plane folder FileNotFoundError: No \"iscell.npy\" found. Invalid suite2p plane folder \"\"\" self . fpath = pathlib . Path ( suite2p_plane_dir ) # -- Verify dataset exists -- ops_fp = self . fpath / \"ops.npy\" if not ops_fp . exists (): raise FileNotFoundError ( 'No \"ops.npy\" found. Invalid suite2p plane folder: {} ' . format ( self . fpath ) ) self . creation_time = datetime . fromtimestamp ( ops_fp . stat () . st_ctime ) iscell_fp = self . fpath / \"iscell.npy\" if not iscell_fp . exists (): raise FileNotFoundError ( 'No \"iscell.npy\" found. Invalid suite2p plane folder: {} ' . format ( self . fpath ) ) self . curation_time = datetime . fromtimestamp ( iscell_fp . stat () . st_ctime ) # -- Initialize attributes -- for s2p_type in _suite2p_ftypes : setattr ( self , \"_ {} \" . format ( s2p_type ), None ) self . _cell_prob = None self . plane_idx = ( - 1 if self . fpath . name == \"combined\" else int ( self . fpath . name . replace ( \"plane\" , \"\" )) ) Suite2p \u00b6 Wrapper class containing all suite2p outputs from one suite2p analysis routine. This wrapper includes outputs from the individual plane, with plane indexing starting from 0 Plane index of -1 indicates a suite2p \"combined\" outputs from all planes, thus saved in the \"planes_combined\" attribute. See also PlaneSuite2p class. Directory example plane0: ops.npy, F.npy, etc. plane1: ops.npy, F.npy, etc. combined: ops.npy, F.npy, etc. Example loaded_dataset = suite2p_loader.Suite2p(output_dir) Source code in element_interface/suite2p_loader.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class Suite2p : \"\"\"Wrapper class containing all suite2p outputs from one suite2p analysis routine. This wrapper includes outputs from the individual plane, with plane indexing starting from 0 Plane index of -1 indicates a suite2p \"combined\" outputs from all planes, thus saved in the \"planes_combined\" attribute. See also PlaneSuite2p class. Directory example: - plane0: ops.npy, F.npy, etc. - plane1: ops.npy, F.npy, etc. - combined: ops.npy, F.npy, etc. Example: > loaded_dataset = suite2p_loader.Suite2p(output_dir) \"\"\" def __init__ ( self , suite2p_dir : str ): \"\"\"Initialize Suite2p class Args: suite2p_dir (str): Suite2p directory Raises: FileNotFoundError: Could not find Suite2p results \"\"\" self . suite2p_dir = pathlib . Path ( suite2p_dir ) ops_filepaths = list ( self . suite2p_dir . rglob ( \"*ops.npy\" )) if not len ( ops_filepaths ): raise FileNotFoundError ( \"Suite2p output result files not found at {} \" . format ( suite2p_dir ) ) self . planes = {} self . planes_combined = None for ops_fp in ops_filepaths : plane_s2p = PlaneSuite2p ( ops_fp . parent ) if plane_s2p . plane_idx == - 1 : self . planes_combined = plane_s2p else : self . planes [ plane_s2p . plane_idx ] = plane_s2p self . planes = OrderedDict ({ k : self . planes [ k ] for k in sorted ( self . planes )}) self . creation_time = min ( [ p . creation_time for p in self . planes . values ()] ) # ealiest file creation time self . curation_time = max ( [ p . curation_time for p in self . planes . values ()] ) # most recent curation time __init__ ( suite2p_dir ) \u00b6 Initialize Suite2p class Parameters: Name Type Description Default suite2p_dir str Suite2p directory required Raises: Type Description FileNotFoundError Could not find Suite2p results Source code in element_interface/suite2p_loader.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , suite2p_dir : str ): \"\"\"Initialize Suite2p class Args: suite2p_dir (str): Suite2p directory Raises: FileNotFoundError: Could not find Suite2p results \"\"\" self . suite2p_dir = pathlib . Path ( suite2p_dir ) ops_filepaths = list ( self . suite2p_dir . rglob ( \"*ops.npy\" )) if not len ( ops_filepaths ): raise FileNotFoundError ( \"Suite2p output result files not found at {} \" . format ( suite2p_dir ) ) self . planes = {} self . planes_combined = None for ops_fp in ops_filepaths : plane_s2p = PlaneSuite2p ( ops_fp . parent ) if plane_s2p . plane_idx == - 1 : self . planes_combined = plane_s2p else : self . planes [ plane_s2p . plane_idx ] = plane_s2p self . planes = OrderedDict ({ k : self . planes [ k ] for k in sorted ( self . planes )}) self . creation_time = min ( [ p . creation_time for p in self . planes . values ()] ) # ealiest file creation time self . curation_time = max ( [ p . curation_time for p in self . planes . values ()] ) # most recent curation time", "title": "suite2p_loader.py"}, {"location": "api/element_interface/suite2p_loader/#element_interface.suite2p_loader.PlaneSuite2p", "text": "Parse the suite2p output directory and load data, per plane . Suite2p output doc: https://suite2p.readthedocs.io/en/latest/outputs.html Expecting the following files ops: Options file Fneu: Neuropil traces file for functional channel Fneu_chan2: Neuropil traces file for channel 2 F: Fluorescence traces for functional channel F_chan2: Fluorescence traces for channel 2 iscell: Array of (user curated) cells and probability of being a cell spks: Spikes (raw deconvolved with OASIS package) stat: Various statistics for each cell redcell: \"Red cell\" (second channel) stats Attributes: Name Type Description alignment_channel ops[\"align_by_chan\"] as zero-indexed cell_prob correlation_map ops[\"Vcorr\"] creation_time earliest file creation time across planes curation_time latest curation time across planes F Fluorescence traces for functional channel as numpy array if exists If does not exist, returns empty list F_chan2 Fluorescence traces for channel 2 as numpy array if exists If does not exist, returns empty lists Fneu Neuropil traces file for functional channel as numpy array if exists If does not exist, returns empty list Fneu_chan2 Neuropil traces file for channel 2 as numpy array if exists If does not exist, returns empty list fpath path to plane folder iscell max_proj_image ops[\"max_proj\"] if exists. Else np.full_like(mean_image)) mean_image ops[\"meanImg\"] ops Options file as numpy array plane_idx plane index. -1 if combined, else number in path redcell \"Red cell\" (second channel) stats as numpy array if exists If does not exist, returns empty list ref_image ops[\"refImg\"] segmentation_channel ops[\"functional_chan\"] as zero-indexed spks Spikes (raw deconvolved with OASIS package) as numpy array if exists If does not exist, returns empty lists stat Various statistics for each cell as numpy array if exists If does not exist, returns empty lists Source code in element_interface/suite2p_loader.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 class PlaneSuite2p : \"\"\"Parse the suite2p output directory and load data, ***per plane***. Suite2p output doc: https://suite2p.readthedocs.io/en/latest/outputs.html Expecting the following files: - ops: Options file - Fneu: Neuropil traces file for functional channel - Fneu_chan2: Neuropil traces file for channel 2 - F: Fluorescence traces for functional channel - F_chan2: Fluorescence traces for channel 2 - iscell: Array of (user curated) cells and probability of being a cell - spks: Spikes (raw deconvolved with OASIS package) - stat: Various statistics for each cell - redcell: \"Red cell\" (second channel) stats Attributes: alignment_channel: ops[\"align_by_chan\"] as zero-indexed cell_prob: correlation_map: ops[\"Vcorr\"] creation_time: earliest file creation time across planes curation_time: latest curation time across planes F: Fluorescence traces for functional channel as numpy array if exists If does not exist, returns empty list F_chan2: Fluorescence traces for channel 2 as numpy array if exists If does not exist, returns empty lists Fneu: Neuropil traces file for functional channel as numpy array if exists If does not exist, returns empty list Fneu_chan2: Neuropil traces file for channel 2 as numpy array if exists If does not exist, returns empty list fpath: path to plane folder iscell: max_proj_image: ops[\"max_proj\"] if exists. Else np.full_like(mean_image)) mean_image: ops[\"meanImg\"] ops: Options file as numpy array plane_idx: plane index. -1 if combined, else number in path redcell: \"Red cell\" (second channel) stats as numpy array if exists If does not exist, returns empty list ref_image: ops[\"refImg\"] segmentation_channel: ops[\"functional_chan\"] as zero-indexed spks: Spikes (raw deconvolved with OASIS package) as numpy array if exists If does not exist, returns empty lists stat: Various statistics for each cell as numpy array if exists If does not exist, returns empty lists \"\"\" def __init__ ( self , suite2p_plane_dir : str ): \"\"\"Initialize PlaneSuite2p class given a plane directory Args: suite2p_plane_dir (str): Suite2p plane directory Raises: FileNotFoundError: No \"ops.npy\" found. Invalid suite2p plane folder FileNotFoundError: No \"iscell.npy\" found. Invalid suite2p plane folder \"\"\" self . fpath = pathlib . Path ( suite2p_plane_dir ) # -- Verify dataset exists -- ops_fp = self . fpath / \"ops.npy\" if not ops_fp . exists (): raise FileNotFoundError ( 'No \"ops.npy\" found. Invalid suite2p plane folder: {} ' . format ( self . fpath ) ) self . creation_time = datetime . fromtimestamp ( ops_fp . stat () . st_ctime ) iscell_fp = self . fpath / \"iscell.npy\" if not iscell_fp . exists (): raise FileNotFoundError ( 'No \"iscell.npy\" found. Invalid suite2p plane folder: {} ' . format ( self . fpath ) ) self . curation_time = datetime . fromtimestamp ( iscell_fp . stat () . st_ctime ) # -- Initialize attributes -- for s2p_type in _suite2p_ftypes : setattr ( self , \"_ {} \" . format ( s2p_type ), None ) self . _cell_prob = None self . plane_idx = ( - 1 if self . fpath . name == \"combined\" else int ( self . fpath . name . replace ( \"plane\" , \"\" )) ) # -- load core files -- @property def ops ( self ): if self . _ops is None : fp = self . fpath / \"ops.npy\" self . _ops = np . load ( fp , allow_pickle = True ) . item () return self . _ops @property def Fneu ( self ): if self . _Fneu is None : fp = self . fpath / \"Fneu.npy\" self . _Fneu = np . load ( fp ) if fp . exists () else [] return self . _Fneu @property def Fneu_chan2 ( self ): if self . _Fneu_chan2 is None : fp = self . fpath / \"Fneu_chan2.npy\" self . _Fneu_chan2 = np . load ( fp ) if fp . exists () else [] return self . _Fneu_chan2 @property def F ( self ): if self . _F is None : fp = self . fpath / \"F.npy\" self . _F = np . load ( fp ) if fp . exists () else [] return self . _F @property def F_chan2 ( self ): if self . _F_chan2 is None : fp = self . fpath / \"F_chan2.npy\" self . _F_chan2 = np . load ( fp ) if fp . exists () else [] return self . _F_chan2 @property def iscell ( self ): if self . _iscell is None : fp = self . fpath / \"iscell.npy\" d = np . load ( fp ) self . _iscell = d [:, 0 ] . astype ( bool ) self . _cell_prob = d [:, 1 ] return self . _iscell @property def cell_prob ( self ): if self . _cell_prob is None : fp = self . fpath / \"iscell.npy\" if fp . exists (): d = np . load ( fp ) self . _iscell = d [:, 0 ] . astype ( bool ) self . _cell_prob = d [:, 1 ] return self . _cell_prob @property def spks ( self ): if self . _spks is None : fp = self . fpath / \"spks.npy\" self . _spks = np . load ( fp ) if fp . exists () else [] return self . _spks @property def stat ( self ): if self . _stat is None : fp = self . fpath / \"stat.npy\" self . _stat = np . load ( fp , allow_pickle = True ) if fp . exists () else [] return self . _stat @property def redcell ( self ): if self . _redcell is None : fp = self . fpath / \"redcell.npy\" self . _redcell = np . load ( fp ) if fp . exists () else [] return self . _redcell # -- image property -- @property def ref_image ( self ): return self . ops [ \"refImg\" ] @property def mean_image ( self ): return self . ops [ \"meanImg\" ] @property def max_proj_image ( self ): return self . ops . get ( \"max_proj\" , np . full_like ( self . mean_image , np . nan )) @property def correlation_map ( self ): return self . ops [ \"Vcorr\" ] @property def alignment_channel ( self ): return self . ops [ \"align_by_chan\" ] - 1 # suite2p is 1-based, convert to 0-based @property def segmentation_channel ( self ): return self . ops [ \"functional_chan\" ] - 1 # suite2p is 1-based, convert to 0-based", "title": "PlaneSuite2p"}, {"location": "api/element_interface/suite2p_loader/#element_interface.suite2p_loader.PlaneSuite2p.__init__", "text": "Initialize PlaneSuite2p class given a plane directory Parameters: Name Type Description Default suite2p_plane_dir str Suite2p plane directory required Raises: Type Description FileNotFoundError No \"ops.npy\" found. Invalid suite2p plane folder FileNotFoundError No \"iscell.npy\" found. Invalid suite2p plane folder Source code in element_interface/suite2p_loader.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def __init__ ( self , suite2p_plane_dir : str ): \"\"\"Initialize PlaneSuite2p class given a plane directory Args: suite2p_plane_dir (str): Suite2p plane directory Raises: FileNotFoundError: No \"ops.npy\" found. Invalid suite2p plane folder FileNotFoundError: No \"iscell.npy\" found. Invalid suite2p plane folder \"\"\" self . fpath = pathlib . Path ( suite2p_plane_dir ) # -- Verify dataset exists -- ops_fp = self . fpath / \"ops.npy\" if not ops_fp . exists (): raise FileNotFoundError ( 'No \"ops.npy\" found. Invalid suite2p plane folder: {} ' . format ( self . fpath ) ) self . creation_time = datetime . fromtimestamp ( ops_fp . stat () . st_ctime ) iscell_fp = self . fpath / \"iscell.npy\" if not iscell_fp . exists (): raise FileNotFoundError ( 'No \"iscell.npy\" found. Invalid suite2p plane folder: {} ' . format ( self . fpath ) ) self . curation_time = datetime . fromtimestamp ( iscell_fp . stat () . st_ctime ) # -- Initialize attributes -- for s2p_type in _suite2p_ftypes : setattr ( self , \"_ {} \" . format ( s2p_type ), None ) self . _cell_prob = None self . plane_idx = ( - 1 if self . fpath . name == \"combined\" else int ( self . fpath . name . replace ( \"plane\" , \"\" )) )", "title": "__init__()"}, {"location": "api/element_interface/suite2p_loader/#element_interface.suite2p_loader.Suite2p", "text": "Wrapper class containing all suite2p outputs from one suite2p analysis routine. This wrapper includes outputs from the individual plane, with plane indexing starting from 0 Plane index of -1 indicates a suite2p \"combined\" outputs from all planes, thus saved in the \"planes_combined\" attribute. See also PlaneSuite2p class. Directory example plane0: ops.npy, F.npy, etc. plane1: ops.npy, F.npy, etc. combined: ops.npy, F.npy, etc. Example loaded_dataset = suite2p_loader.Suite2p(output_dir) Source code in element_interface/suite2p_loader.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class Suite2p : \"\"\"Wrapper class containing all suite2p outputs from one suite2p analysis routine. This wrapper includes outputs from the individual plane, with plane indexing starting from 0 Plane index of -1 indicates a suite2p \"combined\" outputs from all planes, thus saved in the \"planes_combined\" attribute. See also PlaneSuite2p class. Directory example: - plane0: ops.npy, F.npy, etc. - plane1: ops.npy, F.npy, etc. - combined: ops.npy, F.npy, etc. Example: > loaded_dataset = suite2p_loader.Suite2p(output_dir) \"\"\" def __init__ ( self , suite2p_dir : str ): \"\"\"Initialize Suite2p class Args: suite2p_dir (str): Suite2p directory Raises: FileNotFoundError: Could not find Suite2p results \"\"\" self . suite2p_dir = pathlib . Path ( suite2p_dir ) ops_filepaths = list ( self . suite2p_dir . rglob ( \"*ops.npy\" )) if not len ( ops_filepaths ): raise FileNotFoundError ( \"Suite2p output result files not found at {} \" . format ( suite2p_dir ) ) self . planes = {} self . planes_combined = None for ops_fp in ops_filepaths : plane_s2p = PlaneSuite2p ( ops_fp . parent ) if plane_s2p . plane_idx == - 1 : self . planes_combined = plane_s2p else : self . planes [ plane_s2p . plane_idx ] = plane_s2p self . planes = OrderedDict ({ k : self . planes [ k ] for k in sorted ( self . planes )}) self . creation_time = min ( [ p . creation_time for p in self . planes . values ()] ) # ealiest file creation time self . curation_time = max ( [ p . curation_time for p in self . planes . values ()] ) # most recent curation time", "title": "Suite2p"}, {"location": "api/element_interface/suite2p_loader/#element_interface.suite2p_loader.Suite2p.__init__", "text": "Initialize Suite2p class Parameters: Name Type Description Default suite2p_dir str Suite2p directory required Raises: Type Description FileNotFoundError Could not find Suite2p results Source code in element_interface/suite2p_loader.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , suite2p_dir : str ): \"\"\"Initialize Suite2p class Args: suite2p_dir (str): Suite2p directory Raises: FileNotFoundError: Could not find Suite2p results \"\"\" self . suite2p_dir = pathlib . Path ( suite2p_dir ) ops_filepaths = list ( self . suite2p_dir . rglob ( \"*ops.npy\" )) if not len ( ops_filepaths ): raise FileNotFoundError ( \"Suite2p output result files not found at {} \" . format ( suite2p_dir ) ) self . planes = {} self . planes_combined = None for ops_fp in ops_filepaths : plane_s2p = PlaneSuite2p ( ops_fp . parent ) if plane_s2p . plane_idx == - 1 : self . planes_combined = plane_s2p else : self . planes [ plane_s2p . plane_idx ] = plane_s2p self . planes = OrderedDict ({ k : self . planes [ k ] for k in sorted ( self . planes )}) self . creation_time = min ( [ p . creation_time for p in self . planes . values ()] ) # ealiest file creation time self . curation_time = max ( [ p . curation_time for p in self . planes . values ()] ) # most recent curation time", "title": "__init__()"}, {"location": "api/element_interface/suite2p_trigger/", "text": "deconvolution_suite2p ( segmentation_ops , db ) \u00b6 Performs deconvolution using the Suite2p package for single plane tiff files. The code to run deconvolution separately can be found here . Parameters: Name Type Description Default segmentation_ops dict options dictionary. Requirements: - baseline - how to compute baseline of each trace - win_baseline - window for max filter in seconds - sig_baseline - width of Gaussian filter in seconds - fs - sampling rate per plane - prctile_baseline - percentile of trace to use as baseline if using constant_prctile for baseline - batch_size - number of frames processed per batch - tau - timescale of the sensor, used for the deconvolution kernel - neucoeff - neuropil coefficient for all regions of interest - do_registration=0 - two_step_registration=False - roidetect=False - spikedetect=True required Returns: Type Description np . ndarray spks.npy: Updates the file with an array of deconvolved traces Source code in element_interface/suite2p_trigger.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def deconvolution_suite2p ( segmentation_ops : dict , db : dict ) -> np . ndarray : \"\"\"Performs deconvolution using the Suite2p package for single plane tiff files. The code to run deconvolution separately can be found here </https://suite2p.readthedocs.io/en/latest/deconvolution.html>. Args: segmentation_ops (dict): options dictionary. Requirements: - baseline - how to compute baseline of each trace - win_baseline - window for max filter in seconds - sig_baseline - width of Gaussian filter in seconds - fs - sampling rate per plane - prctile_baseline - percentile of trace to use as baseline if using `constant_prctile` for baseline - batch_size - number of frames processed per batch - tau - timescale of the sensor, used for the deconvolution kernel - neucoeff - neuropil coefficient for all regions of interest - do_registration=0 - two_step_registration=False - roidetect=False - spikedetect=True Returns: spks.npy: Updates the file with an array of deconvolved traces \"\"\" if ( segmentation_ops [ \"do_registration\" ] or segmentation_ops [ \"roidetect\" ] or ( not segmentation_ops [ \"spikedetect\" ]) ): warnings . warn ( \"Running deconvolution with Suite2p.\" \"Requirements include do_registration=0, roidetect=False,\" \"spikedetect=True. The ops dictionary has differing values,\" \"the flags will be set to the required values.\" ) segmentation_ops . update ( do_registration = 0 , roidetect = False , spikedetect = True ) F = np . load ( db [ \"fast-disk\" ] + \"/suite2p/\" + \"plane0\" + \"/F.npy\" , allow_pickle = True ) Fneu = np . load ( db [ \"fast-disk\" ] + \"/suite2p/\" + \"plane0\" + \"/Fneu.npy\" , allow_pickle = True ) Fc = F - segmentation_ops [ \"neucoeff\" ] * Fneu Fc = suite2p . extraction . dcnv . preprocess ( F = Fc , baseline = segmentation_ops [ \"baseline\" ], win_baseline = segmentation_ops [ \"win_baseline\" ], sig_baseline = segmentation_ops [ \"sig_baseline\" ], fs = segmentation_ops [ \"fs\" ], prctile_baseline = segmentation_ops [ \"prctile_baseline\" ], ) spikes = suite2p . extraction . dcnv . oasis ( F = Fc , batch_size = segmentation_ops [ \"batch_size\" ], tau = segmentation_ops [ \"tau\" ], fs = segmentation_ops [ \"fs\" ], ) np . save ( os . path . join ( segmentation_ops [ \"save_path\" ], \"spks.npy\" ), spikes ) return spikes motion_correction_suite2p ( ops , db ) \u00b6 Performs motion correction (i.e. registration) using the Suite2p package. Example ops = dict(suite2p.default_ops(), nonrigid=False, two_step_registration=False) db = {'h5py': [], # single h5 file path 'h5py_key': 'data', 'look_one_level_down': False, # search for TIFFs in all subfolders 'data_path': ['/test_data'], # list of folders with tiffs 'subfolders': [], # choose subfolders of 'data_path' 'fast-disk': '/test_data' # string path for storing binary file} ops.update(do_registration=1, roidetect=False, spikedetect=False) motion_correction_ops = element_interface.suite2p_trigger.motion_correction_suite2p(ops, db) motion_correction_ops.update(do_registration=0, roidetect=True, spikedetect=False) segmentation_ops = element_interface.suite2p_trigger.segmentation_suite2p(motion_correction_ops, db) segmentation_ops.update(do_registration=0, roidetect=False, spikedetect=True) spikes = element_interface.suite2p_trigger.deconvolution_suite2p(segmentation_ops, db) Parameters: Name Type Description Default ops dict ops dictionary can be obtained by using suite2p.default_ops() function. It contains all options and default values used to perform preprocessing. ops['do_registration'] should be set to 1. required db dict dictionary that includes paths pointing towards the input data, and path to store outputs required Returns: Name Type Description motion_correction_ops dict Dictionary that includes x and y shifts. A subset of the ops dictionary returned from suite2p.run_s2p() that is required for the segmentation step. tuple data.bin: Binary file of the data. If delete_bin is set to True (default False), the binary file is deleted after processing. tuple ops.npy: Options dictionary. This file gets updated during the segmentation and deconvolution steps. Source code in element_interface/suite2p_trigger.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def motion_correction_suite2p ( ops : dict , db : dict ) -> tuple : \"\"\"Performs motion correction (i.e. registration) using the Suite2p package. Example: > ops = dict(suite2p.default_ops(), nonrigid=False, two_step_registration=False) > db = {'h5py': [], # single h5 file path 'h5py_key': 'data', 'look_one_level_down': False, # search for TIFFs in all subfolders 'data_path': ['/test_data'], # list of folders with tiffs 'subfolders': [], # choose subfolders of 'data_path' 'fast-disk': '/test_data' # string path for storing binary file} > ops.update(do_registration=1, roidetect=False, spikedetect=False) > motion_correction_ops = element_interface.suite2p_trigger.motion_correction_suite2p(ops, db) > motion_correction_ops.update(do_registration=0, roidetect=True, spikedetect=False) > segmentation_ops = element_interface.suite2p_trigger.segmentation_suite2p(motion_correction_ops, db) > segmentation_ops.update(do_registration=0, roidetect=False, spikedetect=True) > spikes = element_interface.suite2p_trigger.deconvolution_suite2p(segmentation_ops, db) Args: ops (dict): ops dictionary can be obtained by using `suite2p.default_ops()` function. It contains all options and default values used to perform preprocessing. ops['do_registration'] should be set to 1. db (dict): dictionary that includes paths pointing towards the input data, and path to store outputs Returns: motion_correction_ops (dict): Dictionary that includes x and y shifts. A subset of the ops dictionary returned from `suite2p.run_s2p()` that is required for the segmentation step. data.bin: Binary file of the data. If delete_bin is set to True (default False), the binary file is deleted after processing. ops.npy: Options dictionary. This file gets updated during the segmentation and deconvolution steps. \"\"\" if ( not ops [ \"do_registration\" ]) or ops [ \"roidetect\" ] or ops [ \"spikedetect\" ]: warnings . warn ( \"Running motion correction with Suite2p.\" \"Requirements include do_registration=1,\" \"roidetect=False, spikedetect=False. The ops\" \"dictionary has differing values. The flags will\" \"be set to the required values.\" ) ops . update ( do_registration = 1 , roidetect = False , spikedetect = False ) if ops [ \"nonrigid\" ]: print ( \"------------Running non-rigid motion correction------------\" ) motion_correction_ops = suite2p . run_s2p ( ops , db ) subset_keys = [ \"xoff\" , \"yoff\" , \"xoff1\" , \"yoff1\" , \"do_registration\" , \"two_step_registration\" , \"roidetect\" , \"spikedetect\" , \"delete_bin\" , \"xblock\" , \"yblock\" , \"xrange\" , \"yrange\" , \"nblocks\" , \"nframes\" , ] else : print ( \"------------Running rigid motion correction------------\" ) motion_correction_ops = suite2p . run_s2p ( ops , db ) subset_keys = [ \"xoff\" , \"yoff\" , \"do_registration\" , \"two_step_registration\" , \"roidetect\" , \"spikedetect\" , \"delete_bin\" , ] motion_correction_ops = { key : motion_correction_ops [ key ] for key in subset_keys } return motion_correction_ops segmentation_suite2p ( motion_correction_ops , db ) \u00b6 Performs cell segmentation (i.e. roi detection) using Suite2p package. Parameters: Name Type Description Default motion_correction_ops dict options dictionary. Requirements: - x and y shifts - do_registration=0 - two_step_registration=False - roidetect=True - spikedetect=False required db dict dictionary that includes paths pointing towards the input data, and path to store outputs required Returns: Name Type Description segmentation_ops dict A subset of the ops dictionary returned from suite2p.run_s2p() that is required for the deconvolution step. tuple data.bin: Binary file if the one created during motion correction is deleted. If delete_bin=True, the binary file is deleted after processing. tuple ops.npy: Updated ops dictionary created by suite2p.run_s2p() tuple F.npy: Array of fluorescence traces tuple Fneu.npy: Array of neuropil fluorescence traces tuple iscell.npy: Specifies whether a region of interest is a cell and the probability tuple stat.npy: List of statistics computed for each cell tuple spks.npy: Empty file. This file is updated with deconvolved traces during the deconvolution step. Source code in element_interface/suite2p_trigger.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def segmentation_suite2p ( motion_correction_ops : dict , db : dict ) -> tuple : \"\"\"Performs cell segmentation (i.e. roi detection) using Suite2p package. Args: motion_correction_ops (dict): options dictionary. Requirements: - x and y shifts - do_registration=0 - two_step_registration=False - roidetect=True - spikedetect=False db (dict): dictionary that includes paths pointing towards the input data, and path to store outputs Returns: segmentation_ops (dict): A subset of the ops dictionary returned from `suite2p.run_s2p()` that is required for the deconvolution step. data.bin: Binary file if the one created during motion correction is deleted. If delete_bin=True, the binary file is deleted after processing. ops.npy: Updated ops dictionary created by suite2p.run_s2p() F.npy: Array of fluorescence traces Fneu.npy: Array of neuropil fluorescence traces iscell.npy: Specifies whether a region of interest is a cell and the probability stat.npy: List of statistics computed for each cell spks.npy: Empty file. This file is updated with deconvolved traces during the deconvolution step. \"\"\" if ( motion_correction_ops [ \"do_registration\" ] or not motion_correction_ops [ \"roidetect\" ] or motion_correction_ops [ \"spikedetect\" ] ): warnings . warn ( \"Running segmentation with Suite2p. Requirements\" \"include do_registration=0, roidetect=True,\" \"spikedetect=False. The ops dictionary has differing\" \"values. The flags will be set to the required values.\" ) motion_correction_ops . update ( do_registration = 0 , roidetect = True , spikedetect = False ) segmentation_ops = suite2p . run_s2p ( motion_correction_ops , db ) subset_keys = [ \"baseline\" , \"win_baseline\" , \"sig_baseline\" , \"fs\" , \"prctile_baseline\" , \"batch_size\" , \"tau\" , \"save_path\" , \"do_registration\" , \"roidetect\" , \"spikedetect\" , \"neucoeff\" , ] segmentation_ops = { key : segmentation_ops [ key ] for key in subset_keys } return segmentation_ops", "title": "suite2p_trigger.py"}, {"location": "api/element_interface/suite2p_trigger/#element_interface.suite2p_trigger.deconvolution_suite2p", "text": "Performs deconvolution using the Suite2p package for single plane tiff files. The code to run deconvolution separately can be found here . Parameters: Name Type Description Default segmentation_ops dict options dictionary. Requirements: - baseline - how to compute baseline of each trace - win_baseline - window for max filter in seconds - sig_baseline - width of Gaussian filter in seconds - fs - sampling rate per plane - prctile_baseline - percentile of trace to use as baseline if using constant_prctile for baseline - batch_size - number of frames processed per batch - tau - timescale of the sensor, used for the deconvolution kernel - neucoeff - neuropil coefficient for all regions of interest - do_registration=0 - two_step_registration=False - roidetect=False - spikedetect=True required Returns: Type Description np . ndarray spks.npy: Updates the file with an array of deconvolved traces Source code in element_interface/suite2p_trigger.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def deconvolution_suite2p ( segmentation_ops : dict , db : dict ) -> np . ndarray : \"\"\"Performs deconvolution using the Suite2p package for single plane tiff files. The code to run deconvolution separately can be found here </https://suite2p.readthedocs.io/en/latest/deconvolution.html>. Args: segmentation_ops (dict): options dictionary. Requirements: - baseline - how to compute baseline of each trace - win_baseline - window for max filter in seconds - sig_baseline - width of Gaussian filter in seconds - fs - sampling rate per plane - prctile_baseline - percentile of trace to use as baseline if using `constant_prctile` for baseline - batch_size - number of frames processed per batch - tau - timescale of the sensor, used for the deconvolution kernel - neucoeff - neuropil coefficient for all regions of interest - do_registration=0 - two_step_registration=False - roidetect=False - spikedetect=True Returns: spks.npy: Updates the file with an array of deconvolved traces \"\"\" if ( segmentation_ops [ \"do_registration\" ] or segmentation_ops [ \"roidetect\" ] or ( not segmentation_ops [ \"spikedetect\" ]) ): warnings . warn ( \"Running deconvolution with Suite2p.\" \"Requirements include do_registration=0, roidetect=False,\" \"spikedetect=True. The ops dictionary has differing values,\" \"the flags will be set to the required values.\" ) segmentation_ops . update ( do_registration = 0 , roidetect = False , spikedetect = True ) F = np . load ( db [ \"fast-disk\" ] + \"/suite2p/\" + \"plane0\" + \"/F.npy\" , allow_pickle = True ) Fneu = np . load ( db [ \"fast-disk\" ] + \"/suite2p/\" + \"plane0\" + \"/Fneu.npy\" , allow_pickle = True ) Fc = F - segmentation_ops [ \"neucoeff\" ] * Fneu Fc = suite2p . extraction . dcnv . preprocess ( F = Fc , baseline = segmentation_ops [ \"baseline\" ], win_baseline = segmentation_ops [ \"win_baseline\" ], sig_baseline = segmentation_ops [ \"sig_baseline\" ], fs = segmentation_ops [ \"fs\" ], prctile_baseline = segmentation_ops [ \"prctile_baseline\" ], ) spikes = suite2p . extraction . dcnv . oasis ( F = Fc , batch_size = segmentation_ops [ \"batch_size\" ], tau = segmentation_ops [ \"tau\" ], fs = segmentation_ops [ \"fs\" ], ) np . save ( os . path . join ( segmentation_ops [ \"save_path\" ], \"spks.npy\" ), spikes ) return spikes", "title": "deconvolution_suite2p()"}, {"location": "api/element_interface/suite2p_trigger/#element_interface.suite2p_trigger.motion_correction_suite2p", "text": "Performs motion correction (i.e. registration) using the Suite2p package. Example ops = dict(suite2p.default_ops(), nonrigid=False, two_step_registration=False) db = {'h5py': [], # single h5 file path 'h5py_key': 'data', 'look_one_level_down': False, # search for TIFFs in all subfolders 'data_path': ['/test_data'], # list of folders with tiffs 'subfolders': [], # choose subfolders of 'data_path' 'fast-disk': '/test_data' # string path for storing binary file} ops.update(do_registration=1, roidetect=False, spikedetect=False) motion_correction_ops = element_interface.suite2p_trigger.motion_correction_suite2p(ops, db) motion_correction_ops.update(do_registration=0, roidetect=True, spikedetect=False) segmentation_ops = element_interface.suite2p_trigger.segmentation_suite2p(motion_correction_ops, db) segmentation_ops.update(do_registration=0, roidetect=False, spikedetect=True) spikes = element_interface.suite2p_trigger.deconvolution_suite2p(segmentation_ops, db) Parameters: Name Type Description Default ops dict ops dictionary can be obtained by using suite2p.default_ops() function. It contains all options and default values used to perform preprocessing. ops['do_registration'] should be set to 1. required db dict dictionary that includes paths pointing towards the input data, and path to store outputs required Returns: Name Type Description motion_correction_ops dict Dictionary that includes x and y shifts. A subset of the ops dictionary returned from suite2p.run_s2p() that is required for the segmentation step. tuple data.bin: Binary file of the data. If delete_bin is set to True (default False), the binary file is deleted after processing. tuple ops.npy: Options dictionary. This file gets updated during the segmentation and deconvolution steps. Source code in element_interface/suite2p_trigger.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def motion_correction_suite2p ( ops : dict , db : dict ) -> tuple : \"\"\"Performs motion correction (i.e. registration) using the Suite2p package. Example: > ops = dict(suite2p.default_ops(), nonrigid=False, two_step_registration=False) > db = {'h5py': [], # single h5 file path 'h5py_key': 'data', 'look_one_level_down': False, # search for TIFFs in all subfolders 'data_path': ['/test_data'], # list of folders with tiffs 'subfolders': [], # choose subfolders of 'data_path' 'fast-disk': '/test_data' # string path for storing binary file} > ops.update(do_registration=1, roidetect=False, spikedetect=False) > motion_correction_ops = element_interface.suite2p_trigger.motion_correction_suite2p(ops, db) > motion_correction_ops.update(do_registration=0, roidetect=True, spikedetect=False) > segmentation_ops = element_interface.suite2p_trigger.segmentation_suite2p(motion_correction_ops, db) > segmentation_ops.update(do_registration=0, roidetect=False, spikedetect=True) > spikes = element_interface.suite2p_trigger.deconvolution_suite2p(segmentation_ops, db) Args: ops (dict): ops dictionary can be obtained by using `suite2p.default_ops()` function. It contains all options and default values used to perform preprocessing. ops['do_registration'] should be set to 1. db (dict): dictionary that includes paths pointing towards the input data, and path to store outputs Returns: motion_correction_ops (dict): Dictionary that includes x and y shifts. A subset of the ops dictionary returned from `suite2p.run_s2p()` that is required for the segmentation step. data.bin: Binary file of the data. If delete_bin is set to True (default False), the binary file is deleted after processing. ops.npy: Options dictionary. This file gets updated during the segmentation and deconvolution steps. \"\"\" if ( not ops [ \"do_registration\" ]) or ops [ \"roidetect\" ] or ops [ \"spikedetect\" ]: warnings . warn ( \"Running motion correction with Suite2p.\" \"Requirements include do_registration=1,\" \"roidetect=False, spikedetect=False. The ops\" \"dictionary has differing values. The flags will\" \"be set to the required values.\" ) ops . update ( do_registration = 1 , roidetect = False , spikedetect = False ) if ops [ \"nonrigid\" ]: print ( \"------------Running non-rigid motion correction------------\" ) motion_correction_ops = suite2p . run_s2p ( ops , db ) subset_keys = [ \"xoff\" , \"yoff\" , \"xoff1\" , \"yoff1\" , \"do_registration\" , \"two_step_registration\" , \"roidetect\" , \"spikedetect\" , \"delete_bin\" , \"xblock\" , \"yblock\" , \"xrange\" , \"yrange\" , \"nblocks\" , \"nframes\" , ] else : print ( \"------------Running rigid motion correction------------\" ) motion_correction_ops = suite2p . run_s2p ( ops , db ) subset_keys = [ \"xoff\" , \"yoff\" , \"do_registration\" , \"two_step_registration\" , \"roidetect\" , \"spikedetect\" , \"delete_bin\" , ] motion_correction_ops = { key : motion_correction_ops [ key ] for key in subset_keys } return motion_correction_ops", "title": "motion_correction_suite2p()"}, {"location": "api/element_interface/suite2p_trigger/#element_interface.suite2p_trigger.segmentation_suite2p", "text": "Performs cell segmentation (i.e. roi detection) using Suite2p package. Parameters: Name Type Description Default motion_correction_ops dict options dictionary. Requirements: - x and y shifts - do_registration=0 - two_step_registration=False - roidetect=True - spikedetect=False required db dict dictionary that includes paths pointing towards the input data, and path to store outputs required Returns: Name Type Description segmentation_ops dict A subset of the ops dictionary returned from suite2p.run_s2p() that is required for the deconvolution step. tuple data.bin: Binary file if the one created during motion correction is deleted. If delete_bin=True, the binary file is deleted after processing. tuple ops.npy: Updated ops dictionary created by suite2p.run_s2p() tuple F.npy: Array of fluorescence traces tuple Fneu.npy: Array of neuropil fluorescence traces tuple iscell.npy: Specifies whether a region of interest is a cell and the probability tuple stat.npy: List of statistics computed for each cell tuple spks.npy: Empty file. This file is updated with deconvolved traces during the deconvolution step. Source code in element_interface/suite2p_trigger.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def segmentation_suite2p ( motion_correction_ops : dict , db : dict ) -> tuple : \"\"\"Performs cell segmentation (i.e. roi detection) using Suite2p package. Args: motion_correction_ops (dict): options dictionary. Requirements: - x and y shifts - do_registration=0 - two_step_registration=False - roidetect=True - spikedetect=False db (dict): dictionary that includes paths pointing towards the input data, and path to store outputs Returns: segmentation_ops (dict): A subset of the ops dictionary returned from `suite2p.run_s2p()` that is required for the deconvolution step. data.bin: Binary file if the one created during motion correction is deleted. If delete_bin=True, the binary file is deleted after processing. ops.npy: Updated ops dictionary created by suite2p.run_s2p() F.npy: Array of fluorescence traces Fneu.npy: Array of neuropil fluorescence traces iscell.npy: Specifies whether a region of interest is a cell and the probability stat.npy: List of statistics computed for each cell spks.npy: Empty file. This file is updated with deconvolved traces during the deconvolution step. \"\"\" if ( motion_correction_ops [ \"do_registration\" ] or not motion_correction_ops [ \"roidetect\" ] or motion_correction_ops [ \"spikedetect\" ] ): warnings . warn ( \"Running segmentation with Suite2p. Requirements\" \"include do_registration=0, roidetect=True,\" \"spikedetect=False. The ops dictionary has differing\" \"values. The flags will be set to the required values.\" ) motion_correction_ops . update ( do_registration = 0 , roidetect = True , spikedetect = False ) segmentation_ops = suite2p . run_s2p ( motion_correction_ops , db ) subset_keys = [ \"baseline\" , \"win_baseline\" , \"sig_baseline\" , \"fs\" , \"prctile_baseline\" , \"batch_size\" , \"tau\" , \"save_path\" , \"do_registration\" , \"roidetect\" , \"spikedetect\" , \"neucoeff\" , ] segmentation_ops = { key : segmentation_ops [ key ] for key in subset_keys } return segmentation_ops", "title": "segmentation_suite2p()"}, {"location": "api/element_interface/utils/", "text": "dict_to_uuid ( key ) \u00b6 Given a dictionary key , returns a hash string as UUID Parameters: Name Type Description Default key dict Any python dictionary required Source code in element_interface/utils.py 91 92 93 94 95 96 97 98 99 100 def dict_to_uuid ( key : dict ): \"\"\"Given a dictionary `key`, returns a hash string as UUID Args: key (dict): Any python dictionary\"\"\" hashed = hashlib . md5 () for k , v in sorted ( key . items ()): hashed . update ( str ( k ) . encode ()) hashed . update ( str ( v ) . encode ()) return uuid . UUID ( hex = hashed . hexdigest ()) find_full_path ( root_directories , relative_path ) \u00b6 Given a list of roots and a relative path, search and return the full-path Root directories are searched in the provided order Parameters: Name Type Description Default root_directories list potential root directories required relative_path str the relative path to find the valid root directory required Returns: Type Description pathlib . PosixPath full-path (pathlib.Path object) Raises: Type Description FileNotFoundError No valid full path Source code in element_interface/utils.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def find_full_path ( root_directories : list , relative_path : str ) -> pathlib . PosixPath : \"\"\"Given a list of roots and a relative path, search and return the full-path Root directories are searched in the provided order Args: root_directories (list): potential root directories relative_path (str): the relative path to find the valid root directory Returns: full-path (pathlib.Path object) Raises: FileNotFoundError: No valid full path \"\"\" relative_path = _to_Path ( relative_path ) if relative_path . exists (): return relative_path # Turn to list if only a single root directory is provided if isinstance ( root_directories , ( str , pathlib . Path )): root_directories = [ _to_Path ( root_directories )] for root_dir in root_directories : if ( _to_Path ( root_dir ) / relative_path ) . exists (): return _to_Path ( root_dir ) / relative_path raise FileNotFoundError ( \"No valid full-path found (from {} )\" \" for {} \" . format ( root_directories , relative_path ) ) find_root_directory ( root_directories , full_path ) \u00b6 Given multiple potential root directories and a full-path, return parent root. Search and return one directory that is the parent of the given path. Parameters: Name Type Description Default root_directories list potential root directories required full_path str the full path to search the root directory required Returns: Type Description pathlib . PosixPath root_directory (pathlib.Path object) Raises: Type Description FileNotFoundError Full path does not exist FileNotFoundError No valid root directory Source code in element_interface/utils.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def find_root_directory ( root_directories : list , full_path : str ) -> pathlib . PosixPath : \"\"\"Given multiple potential root directories and a full-path, return parent root. Search and return one directory that is the parent of the given path. Args: root_directories (list): potential root directories full_path (str): the full path to search the root directory Returns: root_directory (pathlib.Path object) Raises: FileNotFoundError: Full path does not exist FileNotFoundError: No valid root directory \"\"\" full_path = _to_Path ( full_path ) if not full_path . exists (): raise FileNotFoundError ( f \" { full_path } does not exist!\" ) # Turn to list if only a single root directory is provided if isinstance ( root_directories , ( str , pathlib . Path )): root_directories = [ _to_Path ( root_directories )] try : return next ( _to_Path ( root_dir ) for root_dir in root_directories if _to_Path ( root_dir ) in set ( full_path . parents ) ) except StopIteration : raise FileNotFoundError ( \"No valid root directory found (from {} )\" \" for {} \" . format ( root_directories , full_path ) ) ingest_csv_to_table ( csvs , tables , verbose = True , skip_duplicates = True , ignore_extra_fields = True , allow_direct_insert = False ) \u00b6 Inserts data from a series of csvs into their corresponding table: Example ingest_csv_to_table(['./lab_data.csv', './proj_data.csv'], [lab.Lab(),lab.Project()] Parameters: Name Type Description Default csvs list list of paths to CSV files relative to current directory. CSV are delimited by commas. required tables list list of datajoint tables with terminal () required verbose bool print number inserted (i.e., table length change) True skip_duplicates bool skip duplicate entries. See DataJoint's insert True ignore_extra_fields bool if a csv feeds multiple tables, the subset of columns not applicable to a table will be ignored. See DataJoint's insert True allow_direct_insert bool permit insertion into Imported and Computed tables See DataJoint's insert . False Source code in element_interface/utils.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def ingest_csv_to_table ( csvs : list , tables : list , verbose : bool = True , skip_duplicates : bool = True , ignore_extra_fields : bool = True , allow_direct_insert : bool = False , ): \"\"\"Inserts data from a series of csvs into their corresponding table: Example: > ingest_csv_to_table(['./lab_data.csv', './proj_data.csv'], [lab.Lab(),lab.Project()] Args: csvs (list): list of paths to CSV files relative to current directory. CSV are delimited by commas. tables (list): list of datajoint tables with terminal `()` verbose (bool): print number inserted (i.e., table length change) skip_duplicates (bool): skip duplicate entries. See DataJoint's `insert` ignore_extra_fields (bool): if a csv feeds multiple tables, the subset of columns not applicable to a table will be ignored. See DataJoint's `insert` allow_direct_insert (bool): permit insertion into Imported and Computed tables See DataJoint's `insert`. \"\"\" for csv_filepath , table in zip ( csvs , tables ): with open ( csv_filepath , newline = \"\" ) as f : data = list ( csv . DictReader ( f , delimiter = \",\" )) if verbose : prev_len = len ( table ) table . insert ( data , skip_duplicates = skip_duplicates , # Ignore extra fields because some CSVs feed multiple tables ignore_extra_fields = ignore_extra_fields , # Allow direct bc element-event uses dj.Imported w/o `make` funcs allow_direct_insert = allow_direct_insert , ) if verbose : insert_len = len ( table ) - prev_len print ( f \" \\n ---- Inserting { insert_len } entry(s) \" + f \"into { table . table_name } ----\" ) recursive_search ( key , dictionary ) \u00b6 Return value for key in a nested dictionary Search through a nested dictionary for a key and returns its value. If there are more than one key with the same name at different depths, the algorithm returns the value of the least nested key. Parameters: Name Type Description Default key str Key used to search through a nested dictionary required dictionary dict Nested dictionary required Returns: Name Type Description value any value of the input argument key Source code in element_interface/utils.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def recursive_search ( key : str , dictionary : dict ) -> any : \"\"\"Return value for key in a nested dictionary Search through a nested dictionary for a key and returns its value. If there are more than one key with the same name at different depths, the algorithm returns the value of the least nested key. Args: key (str): Key used to search through a nested dictionary dictionary (dict): Nested dictionary Returns: value (any): value of the input argument `key` \"\"\" if key in dictionary : return dictionary [ key ] for value in dictionary . values (): if isinstance ( value , dict ): a = recursive_search ( key , value ) if a is not None : return a return None", "title": "utils.py"}, {"location": "api/element_interface/utils/#element_interface.utils.dict_to_uuid", "text": "Given a dictionary key , returns a hash string as UUID Parameters: Name Type Description Default key dict Any python dictionary required Source code in element_interface/utils.py 91 92 93 94 95 96 97 98 99 100 def dict_to_uuid ( key : dict ): \"\"\"Given a dictionary `key`, returns a hash string as UUID Args: key (dict): Any python dictionary\"\"\" hashed = hashlib . md5 () for k , v in sorted ( key . items ()): hashed . update ( str ( k ) . encode ()) hashed . update ( str ( v ) . encode ()) return uuid . UUID ( hex = hashed . hexdigest ())", "title": "dict_to_uuid()"}, {"location": "api/element_interface/utils/#element_interface.utils.find_full_path", "text": "Given a list of roots and a relative path, search and return the full-path Root directories are searched in the provided order Parameters: Name Type Description Default root_directories list potential root directories required relative_path str the relative path to find the valid root directory required Returns: Type Description pathlib . PosixPath full-path (pathlib.Path object) Raises: Type Description FileNotFoundError No valid full path Source code in element_interface/utils.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def find_full_path ( root_directories : list , relative_path : str ) -> pathlib . PosixPath : \"\"\"Given a list of roots and a relative path, search and return the full-path Root directories are searched in the provided order Args: root_directories (list): potential root directories relative_path (str): the relative path to find the valid root directory Returns: full-path (pathlib.Path object) Raises: FileNotFoundError: No valid full path \"\"\" relative_path = _to_Path ( relative_path ) if relative_path . exists (): return relative_path # Turn to list if only a single root directory is provided if isinstance ( root_directories , ( str , pathlib . Path )): root_directories = [ _to_Path ( root_directories )] for root_dir in root_directories : if ( _to_Path ( root_dir ) / relative_path ) . exists (): return _to_Path ( root_dir ) / relative_path raise FileNotFoundError ( \"No valid full-path found (from {} )\" \" for {} \" . format ( root_directories , relative_path ) )", "title": "find_full_path()"}, {"location": "api/element_interface/utils/#element_interface.utils.find_root_directory", "text": "Given multiple potential root directories and a full-path, return parent root. Search and return one directory that is the parent of the given path. Parameters: Name Type Description Default root_directories list potential root directories required full_path str the full path to search the root directory required Returns: Type Description pathlib . PosixPath root_directory (pathlib.Path object) Raises: Type Description FileNotFoundError Full path does not exist FileNotFoundError No valid root directory Source code in element_interface/utils.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def find_root_directory ( root_directories : list , full_path : str ) -> pathlib . PosixPath : \"\"\"Given multiple potential root directories and a full-path, return parent root. Search and return one directory that is the parent of the given path. Args: root_directories (list): potential root directories full_path (str): the full path to search the root directory Returns: root_directory (pathlib.Path object) Raises: FileNotFoundError: Full path does not exist FileNotFoundError: No valid root directory \"\"\" full_path = _to_Path ( full_path ) if not full_path . exists (): raise FileNotFoundError ( f \" { full_path } does not exist!\" ) # Turn to list if only a single root directory is provided if isinstance ( root_directories , ( str , pathlib . Path )): root_directories = [ _to_Path ( root_directories )] try : return next ( _to_Path ( root_dir ) for root_dir in root_directories if _to_Path ( root_dir ) in set ( full_path . parents ) ) except StopIteration : raise FileNotFoundError ( \"No valid root directory found (from {} )\" \" for {} \" . format ( root_directories , full_path ) )", "title": "find_root_directory()"}, {"location": "api/element_interface/utils/#element_interface.utils.ingest_csv_to_table", "text": "Inserts data from a series of csvs into their corresponding table: Example ingest_csv_to_table(['./lab_data.csv', './proj_data.csv'], [lab.Lab(),lab.Project()] Parameters: Name Type Description Default csvs list list of paths to CSV files relative to current directory. CSV are delimited by commas. required tables list list of datajoint tables with terminal () required verbose bool print number inserted (i.e., table length change) True skip_duplicates bool skip duplicate entries. See DataJoint's insert True ignore_extra_fields bool if a csv feeds multiple tables, the subset of columns not applicable to a table will be ignored. See DataJoint's insert True allow_direct_insert bool permit insertion into Imported and Computed tables See DataJoint's insert . False Source code in element_interface/utils.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def ingest_csv_to_table ( csvs : list , tables : list , verbose : bool = True , skip_duplicates : bool = True , ignore_extra_fields : bool = True , allow_direct_insert : bool = False , ): \"\"\"Inserts data from a series of csvs into their corresponding table: Example: > ingest_csv_to_table(['./lab_data.csv', './proj_data.csv'], [lab.Lab(),lab.Project()] Args: csvs (list): list of paths to CSV files relative to current directory. CSV are delimited by commas. tables (list): list of datajoint tables with terminal `()` verbose (bool): print number inserted (i.e., table length change) skip_duplicates (bool): skip duplicate entries. See DataJoint's `insert` ignore_extra_fields (bool): if a csv feeds multiple tables, the subset of columns not applicable to a table will be ignored. See DataJoint's `insert` allow_direct_insert (bool): permit insertion into Imported and Computed tables See DataJoint's `insert`. \"\"\" for csv_filepath , table in zip ( csvs , tables ): with open ( csv_filepath , newline = \"\" ) as f : data = list ( csv . DictReader ( f , delimiter = \",\" )) if verbose : prev_len = len ( table ) table . insert ( data , skip_duplicates = skip_duplicates , # Ignore extra fields because some CSVs feed multiple tables ignore_extra_fields = ignore_extra_fields , # Allow direct bc element-event uses dj.Imported w/o `make` funcs allow_direct_insert = allow_direct_insert , ) if verbose : insert_len = len ( table ) - prev_len print ( f \" \\n ---- Inserting { insert_len } entry(s) \" + f \"into { table . table_name } ----\" )", "title": "ingest_csv_to_table()"}, {"location": "api/element_interface/utils/#element_interface.utils.recursive_search", "text": "Return value for key in a nested dictionary Search through a nested dictionary for a key and returns its value. If there are more than one key with the same name at different depths, the algorithm returns the value of the least nested key. Parameters: Name Type Description Default key str Key used to search through a nested dictionary required dictionary dict Nested dictionary required Returns: Name Type Description value any value of the input argument key Source code in element_interface/utils.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def recursive_search ( key : str , dictionary : dict ) -> any : \"\"\"Return value for key in a nested dictionary Search through a nested dictionary for a key and returns its value. If there are more than one key with the same name at different depths, the algorithm returns the value of the least nested key. Args: key (str): Key used to search through a nested dictionary dictionary (dict): Nested dictionary Returns: value (any): value of the input argument `key` \"\"\" if key in dictionary : return dictionary [ key ] for value in dictionary . values (): if isinstance ( value , dict ): a = recursive_search ( key , value ) if a is not None : return a return None", "title": "recursive_search()"}, {"location": "api/element_interface/version/", "text": "Package metadata", "title": "version.py"}]}