{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Elements Interface for external analysis packages", "text": "<p>DataJoint Element for interoperability with other software. DataJoint Elements collectively standardize and automate data collection and analysis for neuroscience experiments. Typical Elements are modular pipelines for data storage and processing with corresponding database tables that can be combined with other Elements to assemble a fully functional pipeline. Element Interface is home to a number of utilities that make this possible. </p> <p>Element Interface provides a number of loaders and utility functions used across  a number of other Elements.</p> <ul> <li>Calcium imaging loaders: Suite2p, CaImAn, PrairieView</li> </ul> <ul> <li>File management, see <code>find_full_path</code> API</li> </ul> <ul> <li>Data ingestion, see <code>ingest_csv_to_table</code> API</li> </ul> <p>Visit the Concepts page for more information on these tools.</p>"}, {"location": "changelog/", "title": "Changelog", "text": "<p>Observes Semantic Versioning standard and Keep a Changelog convention.</p>"}, {"location": "changelog/#053-2023-05-11", "title": "0.5.3 - 2023-05-11", "text": "<ul> <li>Fix - <code>.ipynb</code> dark mode output for all notebooks.</li> <li>Fix - Remove <code>GOOGLE_ANALYTICS_KEY</code> from <code>u24_element_release_call.yml</code>.</li> </ul>"}, {"location": "changelog/#052-2023-04-28", "title": "0.5.2 - 2023-04-28", "text": "<ul> <li>Fix - <code>.ipynb</code> output in tutorials is not visible in dark mode.</li> </ul>"}, {"location": "changelog/#051-2023-03-15", "title": "0.5.1 - 2023-03-15", "text": "<ul> <li>Fix - ingestion routine for multiple Z devices in <code>prairieviewreader.py</code>.</li> </ul>"}, {"location": "changelog/#050-2023-01-09", "title": "0.5.0 - 2023-01-09", "text": "<ul> <li>Remove - <code>recursive_search</code> function</li> <li>Add - pre-commit checks to the repo to observe flake8, black, isort</li> <li>Add - <code>value_to_bool</code> and <code>QuietStdOut</code> utilities</li> </ul>"}, {"location": "changelog/#042-2022-12-16", "title": "0.4.2 - 2022-12-16", "text": "<ul> <li>Update - PrairieView loader checks for multi-plane vs single-plane scans.</li> </ul>"}, {"location": "changelog/#041-2022-12-15", "title": "0.4.1 - 2022-12-15", "text": "<ul> <li>Update - PrairieView loader now reads recording start time from metadata file</li> </ul>"}, {"location": "changelog/#040-2022-12-14", "title": "0.4.0 - 2022-12-14", "text": "<ul> <li>Add - mkdocs documentation</li> <li>Add - improved docstrings for mkdocs</li> <li>Add - EXTRACT trigger and loader tools</li> </ul>"}, {"location": "changelog/#030-2022-10-7", "title": "0.3.0 - 2022-10-7", "text": "<ul> <li>Add - Function <code>prairieviewreader</code> to parse metadata from Bruker PrarieView acquisition     system</li> <li>Update - Changelog with tag links</li> </ul>"}, {"location": "changelog/#021-2022-07-13", "title": "0.2.1 - 2022-07-13", "text": "<ul> <li>Add - Adopt <code>black</code> formatting</li> <li>Add - Code of Conduct</li> </ul>"}, {"location": "changelog/#020-2022-07-06", "title": "0.2.0 - 2022-07-06", "text": "<ul> <li>First release of <code>element-interface</code>.</li> <li>Bugfix - Fix for <code>tifffile</code> import.</li> <li>Add - Function <code>run_caiman</code> to trigger CNMF algorithm.</li> <li>Add - Function <code>ingest_csv_to_table</code> to insert data from CSV files into tables.</li> <li>Add - Function <code>recursive_search</code> to search through nested dictionary for a key.</li> <li>Add - Function <code>upload_to_dandi</code> to upload Neurodata Without Borders file to the DANDI     platform.</li> <li>Update - Remove <code>extras_require</code> feature to allow this package to be published to PyPI.</li> </ul>"}, {"location": "changelog/#010a1-2022-01-12", "title": "0.1.0a1 - 2022-01-12", "text": "<ul> <li>Change - Rename the package <code>element-data-loader</code> to <code>element-interface</code>.</li> </ul>"}, {"location": "changelog/#010a0-2021-06-21", "title": "0.1.0a0 - 2021-06-21", "text": "<ul> <li>Add - Readers for: <code>ScanImage</code>, <code>Suite2p</code>, <code>CaImAn</code>.</li> </ul>"}, {"location": "citation/", "title": "Citation", "text": "<p>If your work uses this Element, please cite the following manuscript and Research Resource Identifier (RRID).</p> <ul> <li>Yatsenko D, Nguyen T, Shen S, Gunalan K, Turner CA, Guzman R, Sasaki M, Sitonic D,   Reimer J, Walker EY, Tolias AS. DataJoint Elements: Data Workflows for   Neurophysiology. bioRxiv. 2021 Jan 1. doi: https://doi.org/10.1101/2021.03.30.437358</li> </ul> <ul> <li>DataJoint Elements (RRID:SCR_021894) -   Element Interface (version 0.5.3)</li> </ul>"}, {"location": "concepts/", "title": "Concepts", "text": ""}, {"location": "concepts/#software-interoperability", "title": "Software interoperability", "text": "<p>While software versioning helps researchers keep track of changes over time, it often makes sense to separate the interface from the pipeline itself. By collecting utilities here in Element Interface, maintainers can keep up to date with the latest developments across other packages, without causing issues in the respective Element.</p>"}, {"location": "concepts/#element-features", "title": "Element Features", "text": ""}, {"location": "concepts/#general-utilities", "title": "General utilities", "text": "<p><code>utils.find_full_path</code> and <code>utils.find_root_directory</code> are used across many Elements and Workflows to allow for the flexibility of providing one or more root directories in the user's config, and extrapolating from a relative path at runtime.</p> <p><code>utils.ingest_csv_to_table</code> is used across workflow examples to ingest from sample data from local CSV files into sets of manual tables. While researchers may wish to manually insert for day-to-day operations, it helps to have a more complete dataset when learning how to use various Elements.</p> <p><code>utils.str_to_bool</code> converts a set of strings to boolean True or False. This is implemented as the equivalent item in Python's <code>distutils</code> which will be removed in future versions.</p>"}, {"location": "concepts/#suite2p", "title": "Suite2p", "text": "<p>This Element provides functions to independently run Suite2p's motion correction, segmentation, and deconvolution steps. These functions currently work for single plane tiff files. If one is running all Suite2p pre-processing steps concurrently, these functions are not required and one can run <code>suite2p.run_s2p()</code>. The wrapper functions here were developed primarily because <code>run_s2p</code> cannot individually run deconvolution using the <code>spikedetect</code> flag (  Suite2p Issue #718).</p> <p>Requirements:</p> <ul> <li>ops dictionary</li> </ul> <ul> <li>db dictionary</li> </ul> <p>Note: The ops dictionary returned from the <code>motion_correction_suite2p</code> and <code>segmentation_suite2p</code> functions is only a subset of the keys generated with the <code>suite2p.default_ops()</code> function.</p>"}, {"location": "concepts/#prairieview-reader", "title": "PrairieView Reader", "text": "<p>This Element provides a function to read the PrairieView Scanner's metadata file. The PrairieView software generates one <code>.ome.tif</code> imaging file per frame acquired. The metadata for all frames is contained in one <code>.xml</code> file. This function locates the <code>.xml</code> file and generates a dictionary necessary to populate the DataJoint ScanInfo and Field tables. PrairieView works with resonance scanners with a single field, does not support bidirectional x and y scanning, and the <code>.xml</code> file does not contain ROI information.</p>"}, {"location": "concepts/#element-architecture", "title": "Element Architecture", "text": "<p>The functions for each acquisition and analysis package are stored within a separate module.</p> <ul> <li>Acquisition packages: ScanImage</li> <li> <p>Analysis packages:</p> <ul> <li>Suite2p loader and trigger</li> </ul> <ul> <li>CaImAn loader and trigger</li> </ul> </li> </ul> <ul> <li>Data upload: DANDI</li> </ul>"}, {"location": "concepts/#roadmap", "title": "Roadmap", "text": "<p>Further development of this Element is community driven.  Upon user requests and based on guidance from the Scientific Steering Group we will additional features to this Element.</p>"}, {"location": "api/element_interface/caiman_loader/", "title": "caiman_loader.py", "text": ""}, {"location": "api/element_interface/caiman_loader/#element_interface.caiman_loader.CaImAn", "title": "<code>CaImAn</code>", "text": "<p>Parse the CaImAn output file</p> <p>CaImAn results doc</p> Expecting the following objects <ul> <li>dims:</li> <li>dview:</li> <li>estimates:              Segmentations and traces</li> <li>mmap_file:</li> <li>params:                 Input parameters</li> <li>remove_very_bad_comps:</li> <li>skip_refinement:</li> <li>motion_correction:      Motion correction shifts and summary images</li> </ul> Example <p>output_dir = '/subject1/session0/caiman' <p>loaded_dataset = caiman_loader.CaImAn(output_dir)</p> <p>Attributes:</p> Name Type Description <code>alignment_channel</code> <p>hard-coded to 0</p> <code>caiman_fp</code> <p>file path with all required files: \"/motion_correction/reference_image\", \"/motion_correction/correlation_image\", \"/motion_correction/average_image\", \"/motion_correction/max_image\", \"/estimates/A\",</p> <code>cnmf</code> <p>loaded caiman object; cm.source_extraction.cnmf.cnmf.load_CNMF(caiman_fp)</p> <code>creation_time</code> <p>file creation time</p> <code>curation_time</code> <p>file creation time</p> <code>extract_masks</code> <code>dict</code> <p>function to extract masks</p> <code>h5f</code> <p>caiman_fp read as h5py file</p> <code>masks</code> <p>dict result of extract_masks</p> <code>motion_correction</code> <p>h5f \"motion_correction\" property</p> <code>params</code> <p>cnmf.params</p> <code>segmentation_channel</code> <p>hard-coded to 0</p> Source code in <code>element_interface/caiman_loader.py</code> <pre><code>class CaImAn:\n\"\"\"Parse the CaImAn output file\n\n    [CaImAn results doc](https://caiman.readthedocs.io/en/master/Getting_Started.html#result-variables-for-2p-batch-analysis)\n\n    Expecting the following objects:\n        - dims:\n        - dview:\n        - estimates:              Segmentations and traces\n        - mmap_file:\n        - params:                 Input parameters\n        - remove_very_bad_comps:\n        - skip_refinement:\n        - motion_correction:      Motion correction shifts and summary images\n\n    Example:\n        &gt; output_dir = '&lt;imaging_root_data_dir&gt;/subject1/session0/caiman'\n\n        &gt; loaded_dataset = caiman_loader.CaImAn(output_dir)\n\n    Attributes:\n        alignment_channel: hard-coded to 0\n        caiman_fp: file path with all required files:\n            \"/motion_correction/reference_image\",\n            \"/motion_correction/correlation_image\",\n            \"/motion_correction/average_image\",\n            \"/motion_correction/max_image\",\n            \"/estimates/A\",\n        cnmf: loaded caiman object; cm.source_extraction.cnmf.cnmf.load_CNMF(caiman_fp)\n        creation_time: file creation time\n        curation_time: file creation time\n        extract_masks: function to extract masks\n        h5f: caiman_fp read as h5py file\n        masks: dict result of extract_masks\n        motion_correction: h5f \"motion_correction\" property\n        params: cnmf.params\n        segmentation_channel: hard-coded to 0\n    \"\"\"\n\n    def __init__(self, caiman_dir: str):\n\"\"\"Initialize CaImAn loader class\n\n        Args:\n            caiman_dir (str): string, absolute file path to CaIman directory\n\n        Raises:\n            FileNotFoundError: No CaImAn analysis output file found\n            FileNotFoundError: No CaImAn analysis output found, missing required fields\n        \"\"\"\n        # ---- Search and verify CaImAn output file exists ----\n        caiman_dir = pathlib.Path(caiman_dir)\n        if not caiman_dir.exists():\n            raise FileNotFoundError(\"CaImAn directory not found: {}\".format(caiman_dir))\n\n        for fp in caiman_dir.glob(\"*.hdf5\"):\n            with h5py.File(fp, \"r\") as h5f:\n                if all(s in h5f for s in _required_hdf5_fields):\n                    self.caiman_fp = fp\n                    break\n        else:\n            raise FileNotFoundError(\n                \"No CaImAn analysis output file found at {}\"\n                \" containg all required fields ({})\".format(\n                    caiman_dir, _required_hdf5_fields\n                )\n            )\n\n        # ---- Initialize CaImAn's results ----\n        self.cnmf = cm.source_extraction.cnmf.cnmf.load_CNMF(self.caiman_fp)\n        self.params = self.cnmf.params\n\n        self.h5f = h5py.File(self.caiman_fp, \"r\")\n        self.motion_correction = self.h5f[\"motion_correction\"]\n        self._masks = None\n\n        # ---- Metainfo ----\n        self.creation_time = datetime.fromtimestamp(os.stat(self.caiman_fp).st_ctime)\n        self.curation_time = datetime.fromtimestamp(os.stat(self.caiman_fp).st_ctime)\n\n    @property\n    def masks(self):\n        if self._masks is None:\n            self._masks = self.extract_masks()\n        return self._masks\n\n    @property\n    def alignment_channel(self):\n        return 0  # hard-code to channel index 0\n\n    @property\n    def segmentation_channel(self):\n        return 0  # hard-code to channel index 0\n\n    def extract_masks(self) -&gt; dict:\n\"\"\"Extract masks from CaImAn object\n\n        Raises:\n            NotImplemented: Not yet implemented for 3D datasets\n\n        Returns:\n            dict: Mask attributes - mask_id, mask_npix, mask_weights,\n                mask_center_x, mask_center_y, mask_center_z,\n                mask_xpix, mask_ypix, mask_zpix, inferred_trace, dff, spikes\n        \"\"\"\n        if self.params.motion[\"is3D\"]:\n            raise NotImplementedError(\n                \"CaImAn mask extraction for volumetric data not yet implemented\"\n            )\n\n        comp_contours = cm.utils.visualization.get_contours(\n            self.cnmf.estimates.A, self.cnmf.dims\n        )\n\n        masks = []\n        for comp_idx, comp_contour in enumerate(comp_contours):\n            ind, _, weights = scipy.sparse.find(self.cnmf.estimates.A[:, comp_idx])\n            if self.cnmf.params.motion[\"is3D\"]:\n                xpix, ypix, zpix = np.unravel_index(ind, self.cnmf.dims, order=\"F\")\n                center_x, center_y, center_z = comp_contour[\"CoM\"].astype(int)\n            else:\n                xpix, ypix = np.unravel_index(ind, self.cnmf.dims, order=\"F\")\n                center_x, center_y = comp_contour[\"CoM\"].astype(int)\n                center_z = 0\n                zpix = np.full(len(weights), center_z)\n\n            masks.append(\n                {\n                    \"mask_id\": comp_contour[\"neuron_id\"],\n                    \"mask_npix\": len(weights),\n                    \"mask_weights\": weights,\n                    \"mask_center_x\": center_x,\n                    \"mask_center_y\": center_y,\n                    \"mask_center_z\": center_z,\n                    \"mask_xpix\": xpix,\n                    \"mask_ypix\": ypix,\n                    \"mask_zpix\": zpix,\n                    \"inferred_trace\": self.cnmf.estimates.C[comp_idx, :],\n                    \"dff\": self.cnmf.estimates.F_dff[comp_idx, :],\n                    \"spikes\": self.cnmf.estimates.S[comp_idx, :],\n                }\n            )\n        return masks\n</code></pre>"}, {"location": "api/element_interface/caiman_loader/#element_interface.caiman_loader.CaImAn.__init__", "title": "<code>__init__(caiman_dir)</code>", "text": "<p>Initialize CaImAn loader class</p> <p>Parameters:</p> Name Type Description Default <code>caiman_dir</code> <code>str</code> <p>string, absolute file path to CaIman directory</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>No CaImAn analysis output file found</p> <code>FileNotFoundError</code> <p>No CaImAn analysis output found, missing required fields</p> Source code in <code>element_interface/caiman_loader.py</code> <pre><code>def __init__(self, caiman_dir: str):\n\"\"\"Initialize CaImAn loader class\n\n    Args:\n        caiman_dir (str): string, absolute file path to CaIman directory\n\n    Raises:\n        FileNotFoundError: No CaImAn analysis output file found\n        FileNotFoundError: No CaImAn analysis output found, missing required fields\n    \"\"\"\n    # ---- Search and verify CaImAn output file exists ----\n    caiman_dir = pathlib.Path(caiman_dir)\n    if not caiman_dir.exists():\n        raise FileNotFoundError(\"CaImAn directory not found: {}\".format(caiman_dir))\n\n    for fp in caiman_dir.glob(\"*.hdf5\"):\n        with h5py.File(fp, \"r\") as h5f:\n            if all(s in h5f for s in _required_hdf5_fields):\n                self.caiman_fp = fp\n                break\n    else:\n        raise FileNotFoundError(\n            \"No CaImAn analysis output file found at {}\"\n            \" containg all required fields ({})\".format(\n                caiman_dir, _required_hdf5_fields\n            )\n        )\n\n    # ---- Initialize CaImAn's results ----\n    self.cnmf = cm.source_extraction.cnmf.cnmf.load_CNMF(self.caiman_fp)\n    self.params = self.cnmf.params\n\n    self.h5f = h5py.File(self.caiman_fp, \"r\")\n    self.motion_correction = self.h5f[\"motion_correction\"]\n    self._masks = None\n\n    # ---- Metainfo ----\n    self.creation_time = datetime.fromtimestamp(os.stat(self.caiman_fp).st_ctime)\n    self.curation_time = datetime.fromtimestamp(os.stat(self.caiman_fp).st_ctime)\n</code></pre>"}, {"location": "api/element_interface/caiman_loader/#element_interface.caiman_loader.CaImAn.extract_masks", "title": "<code>extract_masks()</code>", "text": "<p>Extract masks from CaImAn object</p> <p>Raises:</p> Type Description <code>NotImplemented</code> <p>Not yet implemented for 3D datasets</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Mask attributes - mask_id, mask_npix, mask_weights, mask_center_x, mask_center_y, mask_center_z, mask_xpix, mask_ypix, mask_zpix, inferred_trace, dff, spikes</p> Source code in <code>element_interface/caiman_loader.py</code> <pre><code>def extract_masks(self) -&gt; dict:\n\"\"\"Extract masks from CaImAn object\n\n    Raises:\n        NotImplemented: Not yet implemented for 3D datasets\n\n    Returns:\n        dict: Mask attributes - mask_id, mask_npix, mask_weights,\n            mask_center_x, mask_center_y, mask_center_z,\n            mask_xpix, mask_ypix, mask_zpix, inferred_trace, dff, spikes\n    \"\"\"\n    if self.params.motion[\"is3D\"]:\n        raise NotImplementedError(\n            \"CaImAn mask extraction for volumetric data not yet implemented\"\n        )\n\n    comp_contours = cm.utils.visualization.get_contours(\n        self.cnmf.estimates.A, self.cnmf.dims\n    )\n\n    masks = []\n    for comp_idx, comp_contour in enumerate(comp_contours):\n        ind, _, weights = scipy.sparse.find(self.cnmf.estimates.A[:, comp_idx])\n        if self.cnmf.params.motion[\"is3D\"]:\n            xpix, ypix, zpix = np.unravel_index(ind, self.cnmf.dims, order=\"F\")\n            center_x, center_y, center_z = comp_contour[\"CoM\"].astype(int)\n        else:\n            xpix, ypix = np.unravel_index(ind, self.cnmf.dims, order=\"F\")\n            center_x, center_y = comp_contour[\"CoM\"].astype(int)\n            center_z = 0\n            zpix = np.full(len(weights), center_z)\n\n        masks.append(\n            {\n                \"mask_id\": comp_contour[\"neuron_id\"],\n                \"mask_npix\": len(weights),\n                \"mask_weights\": weights,\n                \"mask_center_x\": center_x,\n                \"mask_center_y\": center_y,\n                \"mask_center_z\": center_z,\n                \"mask_xpix\": xpix,\n                \"mask_ypix\": ypix,\n                \"mask_zpix\": zpix,\n                \"inferred_trace\": self.cnmf.estimates.C[comp_idx, :],\n                \"dff\": self.cnmf.estimates.F_dff[comp_idx, :],\n                \"spikes\": self.cnmf.estimates.S[comp_idx, :],\n            }\n        )\n    return masks\n</code></pre>"}, {"location": "api/element_interface/dandi/", "title": "dandi.py", "text": ""}, {"location": "api/element_interface/dandi/#element_interface.dandi.upload_to_dandi", "title": "<code>upload_to_dandi(data_directory, dandiset_id, staging=True, working_directory=None, api_key=None, sync=False)</code>", "text": "<p>Upload NWB files to DANDI Archive</p> <p>Parameters:</p> Name Type Description Default <code>data_directory</code> <code>str</code> <p>directory that contains source data</p> required <code>dandiset_id</code> <code>str</code> <p>6-digit zero-padded string</p> required <code>staging</code> <code>bool</code> <p>If true, use staging server. If false, use production server.</p> <code>True</code> <code>working_directory</code> <code>str</code> <p>Dir in which to create symlinked dandiset. Must have write permissions to this directory. Default is current directory.</p> <code>None</code> <code>api_key</code> <code>str</code> <p>Provide the DANDI API key if not already in an environmental variable DANDI_API_KEY</p> <code>None</code> <code>sync</code> <code>str</code> <p>If True, delete all files in archive that are not present in the local directory.</p> <code>False</code> Source code in <code>element_interface/dandi.py</code> <pre><code>def upload_to_dandi(\n    data_directory: str,\n    dandiset_id: str,\n    staging: bool = True,\n    working_directory: str = None,\n    api_key: str = None,\n    sync: bool = False,\n):\n\"\"\"Upload NWB files to DANDI Archive\n\n    Args:\n        data_directory (str): directory that contains source data\n        dandiset_id (str): 6-digit zero-padded string\n        staging (bool): If true, use staging server. If false, use production server.\n        working_directory (str, optional): Dir in which to create symlinked dandiset.\n            Must have write permissions to this directory. Default is current directory.\n        api_key (str, optional): Provide the DANDI API key if not already in an\n            environmental variable DANDI_API_KEY\n        sync (str, optional): If True, delete all files in archive that are not present\n            in the local directory.\n    \"\"\"\n\n    working_directory = working_directory or os.path.curdir\n\n    if api_key is not None:\n        os.environ[\"DANDI_API_KEY\"] = api_key\n\n    dandiset_directory = os.path.join(\n        working_directory, str(dandiset_id)\n    )  # enforce str\n\n    download(\n        f\"https://gui-staging.dandiarchive.org/#/dandiset/{dandiset_id}\"\n        if staging\n        else dandiset_id,\n        output_dir=working_directory,\n    )\n\n    subprocess.run(\n        [\"dandi\", \"organize\", \"-d\", dandiset_directory, data_directory, \"-f\", \"dry\"],\n        shell=True,  # without this param, subprocess interprets first arg as file/dir\n    )\n\n    subprocess.run(\n        [\"dandi\", \"organize\", \"-d\", dandiset_directory, data_directory], shell=True\n    )\n\n    print(\n        f\"work_dir: {working_directory}\\ndata_dir: {data_directory}\\n\"\n        + f\"dand_dir: {dandiset_directory}\"\n    )\n\n    upload(\n        [dandiset_directory],\n        # dandiset_path=dandiset_directory, # dandi.upload has no such arg\n        dandi_instance=\"dandi-staging\" if staging else \"dandi\",\n        sync=sync,\n    )\n</code></pre>"}, {"location": "api/element_interface/extract_loader/", "title": "extract_loader.py", "text": ""}, {"location": "api/element_interface/extract_loader/#element_interface.extract_loader.EXTRACT_loader", "title": "<code>EXTRACT_loader</code>", "text": "Source code in <code>element_interface/extract_loader.py</code> <pre><code>class EXTRACT_loader:\n    def __init__(self, extract_dir: str):\n\"\"\"Initialize EXTRACT loader class\n\n        Args:\n            extract_dir (str): string, absolute file path to EXTRACT directory\n\n        Raises:\n            FileNotFoundError: Could not find EXTRACT results\n        \"\"\"\n        from scipy.io import loadmat\n\n        try:\n            extract_file = next(Path(extract_dir).glob(\"*_extract_output.mat\"))\n        except StopInteration:  # noqa F821\n            raise FileNotFoundError(\n                f\"EXTRACT output result file is not found at {extract_dir}.\"\n            )\n\n        results = loadmat(extract_file)\n\n        self.creation_time = datetime.fromtimestamp(os.stat(extract_file).st_ctime)\n        self.S = results[\"output\"][0][\"spatial_weights\"][0]  # (Height, Width, MaskId)\n        self.T = results[\"output\"][0][\"temporal_weights\"][0]  # (Time, MaskId)\n\n    def load_results(self):\n\"\"\"Load the EXTRACT results\n\n        Returns:\n            masks (dict): Details of the masks identified with the EXTRACT segmentation package.\n        \"\"\"\n        from scipy.sparse import find\n\n        S_transposed = self.S.transpose([2, 0, 1])  # MaskId, Height, Width\n\n        masks = []\n\n        for mask_id, s in enumerate(S_transposed):\n            ypixels, xpixels, weights = find(s)\n            masks.append(\n                dict(\n                    mask_id=mask_id,\n                    mask_npix=len(weights),\n                    mask_weights=weights,\n                    mask_center_x=int(np.average(xpixels, weights=weights) + 0.5),\n                    mask_center_y=int(np.average(ypixels, weights=weights) + 0.5),\n                    mask_center_z=None,\n                    mask_xpix=xpixels,\n                    mask_ypix=ypixels,\n                    mask_zpix=None,\n                )\n            )\n        return masks\n</code></pre>"}, {"location": "api/element_interface/extract_loader/#element_interface.extract_loader.EXTRACT_loader.__init__", "title": "<code>__init__(extract_dir)</code>", "text": "<p>Initialize EXTRACT loader class</p> <p>Parameters:</p> Name Type Description Default <code>extract_dir</code> <code>str</code> <p>string, absolute file path to EXTRACT directory</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Could not find EXTRACT results</p> Source code in <code>element_interface/extract_loader.py</code> <pre><code>def __init__(self, extract_dir: str):\n\"\"\"Initialize EXTRACT loader class\n\n    Args:\n        extract_dir (str): string, absolute file path to EXTRACT directory\n\n    Raises:\n        FileNotFoundError: Could not find EXTRACT results\n    \"\"\"\n    from scipy.io import loadmat\n\n    try:\n        extract_file = next(Path(extract_dir).glob(\"*_extract_output.mat\"))\n    except StopInteration:  # noqa F821\n        raise FileNotFoundError(\n            f\"EXTRACT output result file is not found at {extract_dir}.\"\n        )\n\n    results = loadmat(extract_file)\n\n    self.creation_time = datetime.fromtimestamp(os.stat(extract_file).st_ctime)\n    self.S = results[\"output\"][0][\"spatial_weights\"][0]  # (Height, Width, MaskId)\n    self.T = results[\"output\"][0][\"temporal_weights\"][0]  # (Time, MaskId)\n</code></pre>"}, {"location": "api/element_interface/extract_loader/#element_interface.extract_loader.EXTRACT_loader.load_results", "title": "<code>load_results()</code>", "text": "<p>Load the EXTRACT results</p> <p>Returns:</p> Name Type Description <code>masks</code> <code>dict</code> <p>Details of the masks identified with the EXTRACT segmentation package.</p> Source code in <code>element_interface/extract_loader.py</code> <pre><code>def load_results(self):\n\"\"\"Load the EXTRACT results\n\n    Returns:\n        masks (dict): Details of the masks identified with the EXTRACT segmentation package.\n    \"\"\"\n    from scipy.sparse import find\n\n    S_transposed = self.S.transpose([2, 0, 1])  # MaskId, Height, Width\n\n    masks = []\n\n    for mask_id, s in enumerate(S_transposed):\n        ypixels, xpixels, weights = find(s)\n        masks.append(\n            dict(\n                mask_id=mask_id,\n                mask_npix=len(weights),\n                mask_weights=weights,\n                mask_center_x=int(np.average(xpixels, weights=weights) + 0.5),\n                mask_center_y=int(np.average(ypixels, weights=weights) + 0.5),\n                mask_center_z=None,\n                mask_xpix=xpixels,\n                mask_ypix=ypixels,\n                mask_zpix=None,\n            )\n        )\n    return masks\n</code></pre>"}, {"location": "api/element_interface/extract_trigger/", "title": "extract_trigger.py", "text": ""}, {"location": "api/element_interface/extract_trigger/#element_interface.extract_trigger.EXTRACT_trigger", "title": "<code>EXTRACT_trigger</code>", "text": "Source code in <code>element_interface/extract_trigger.py</code> <pre><code>class EXTRACT_trigger:\n    m_template = dedent(\n\"\"\"\n        % Load Data\n        data = load('{scanfile}');\n        M = data.M;\n\n        % Input Paramaters\n        config = struct();\n        {parameters_list_string}\n\n        % Run EXTRACT\n        output = extractor(M, config);\n        save('{output_fullpath}', 'output');\n        \"\"\"\n    )\n\n    def __init__(\n        self,\n        scanfile: Union[str, Path],\n        parameters: dict,\n        output_dir: Union[str, Path],\n    ) -&gt; None:\n\"\"\"A helper class to trigger EXTRACT analysis in element-calcium-imaging.\n\n        Args:\n            scanfile (Union[str, Path]): Full path of the scan\n            parameters (dict): EXTRACT input paramaters.\n            output_dir (Union[str, Path]): Directory to store the outputs of EXTRACT analysis.\n        \"\"\"\n        assert isinstance(parameters, dict)\n\n        self.scanfile = Path(scanfile)\n        self.output_dir = Path(output_dir)\n        self.parameters = parameters\n\n    def write_matlab_run_script(self):\n\"\"\"Compose a matlab script and save it with the name run_extract.m.\n\n        The composed script is basically the formatted version of the m_template attribute.\"\"\"\n\n        self.output_fullpath = (\n            self.output_dir / f\"{self.scanfile.stem}_extract_output.mat\"\n        )\n\n        m_file_content = self.m_template.format(\n            **dict(\n                parameters_list_string=\"\\n\".join(\n                    [\n                        f\"config.{k} = '{v}';\"\n                        if isinstance(v, str)\n                        else f\"config.{k} = {str(v).lower()};\"\n                        if isinstance(v, bool)\n                        else f\"config.{k} = {v};\"\n                        for k, v in self.parameters.items()\n                    ]\n                ),\n                scanfile=self.scanfile.as_posix(),\n                output_fullpath=self.output_fullpath.as_posix(),\n            )\n        ).lstrip()\n\n        self.m_file_fp = self.output_dir / \"run_extract.m\"\n\n        with open(self.m_file_fp, \"w\") as f:\n            f.write(m_file_content)\n\n    def run(self):\n\"\"\"Run the matlab run_extract.m script.\"\"\"\n\n        self.write_matlab_run_script()\n\n        current_dir = Path.cwd()\n        os.chdir(self.output_dir)\n\n        try:\n            import matlab.engine\n\n            eng = matlab.engine.start_matlab()\n            eng.run_extract()\n        except Exception as e:\n            raise e\n        finally:\n            os.chdir(current_dir)\n</code></pre>"}, {"location": "api/element_interface/extract_trigger/#element_interface.extract_trigger.EXTRACT_trigger.__init__", "title": "<code>__init__(scanfile, parameters, output_dir)</code>", "text": "<p>A helper class to trigger EXTRACT analysis in element-calcium-imaging.</p> <p>Parameters:</p> Name Type Description Default <code>scanfile</code> <code>Union[str, Path]</code> <p>Full path of the scan</p> required <code>parameters</code> <code>dict</code> <p>EXTRACT input paramaters.</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to store the outputs of EXTRACT analysis.</p> required Source code in <code>element_interface/extract_trigger.py</code> <pre><code>def __init__(\n    self,\n    scanfile: Union[str, Path],\n    parameters: dict,\n    output_dir: Union[str, Path],\n) -&gt; None:\n\"\"\"A helper class to trigger EXTRACT analysis in element-calcium-imaging.\n\n    Args:\n        scanfile (Union[str, Path]): Full path of the scan\n        parameters (dict): EXTRACT input paramaters.\n        output_dir (Union[str, Path]): Directory to store the outputs of EXTRACT analysis.\n    \"\"\"\n    assert isinstance(parameters, dict)\n\n    self.scanfile = Path(scanfile)\n    self.output_dir = Path(output_dir)\n    self.parameters = parameters\n</code></pre>"}, {"location": "api/element_interface/extract_trigger/#element_interface.extract_trigger.EXTRACT_trigger.run", "title": "<code>run()</code>", "text": "<p>Run the matlab run_extract.m script.</p> Source code in <code>element_interface/extract_trigger.py</code> <pre><code>def run(self):\n\"\"\"Run the matlab run_extract.m script.\"\"\"\n\n    self.write_matlab_run_script()\n\n    current_dir = Path.cwd()\n    os.chdir(self.output_dir)\n\n    try:\n        import matlab.engine\n\n        eng = matlab.engine.start_matlab()\n        eng.run_extract()\n    except Exception as e:\n        raise e\n    finally:\n        os.chdir(current_dir)\n</code></pre>"}, {"location": "api/element_interface/extract_trigger/#element_interface.extract_trigger.EXTRACT_trigger.write_matlab_run_script", "title": "<code>write_matlab_run_script()</code>", "text": "<p>Compose a matlab script and save it with the name run_extract.m.</p> <p>The composed script is basically the formatted version of the m_template attribute.</p> Source code in <code>element_interface/extract_trigger.py</code> <pre><code>def write_matlab_run_script(self):\n\"\"\"Compose a matlab script and save it with the name run_extract.m.\n\n    The composed script is basically the formatted version of the m_template attribute.\"\"\"\n\n    self.output_fullpath = (\n        self.output_dir / f\"{self.scanfile.stem}_extract_output.mat\"\n    )\n\n    m_file_content = self.m_template.format(\n        **dict(\n            parameters_list_string=\"\\n\".join(\n                [\n                    f\"config.{k} = '{v}';\"\n                    if isinstance(v, str)\n                    else f\"config.{k} = {str(v).lower()};\"\n                    if isinstance(v, bool)\n                    else f\"config.{k} = {v};\"\n                    for k, v in self.parameters.items()\n                ]\n            ),\n            scanfile=self.scanfile.as_posix(),\n            output_fullpath=self.output_fullpath.as_posix(),\n        )\n    ).lstrip()\n\n    self.m_file_fp = self.output_dir / \"run_extract.m\"\n\n    with open(self.m_file_fp, \"w\") as f:\n        f.write(m_file_content)\n</code></pre>"}, {"location": "api/element_interface/prairieviewreader/", "title": "prairieviewreader.py", "text": ""}, {"location": "api/element_interface/prairieviewreader/#element_interface.prairieviewreader.get_pv_metadata", "title": "<code>get_pv_metadata(pvtiffile)</code>", "text": "<p>Extract metadata for scans generated by PrairieView acquisition software.</p> <p>The PrairieView software generates one .ome.tif imaging file per frame acquired. The metadata for all frames is contained one .xml file. This function locates the .xml file and generates a dictionary necessary to populate the DataJoint ScanInfo and Field tables. PrairieView works with resonance scanners with a single field. PrairieView does not support bidirectional x and y scanning. ROI information is not contained in the .xml file. All images generated using PrairieView have square dimensions(e.g. 512x512).</p> <p>Parameters:</p> Name Type Description Default <code>pvtiffile</code> <code>str</code> <p>An absolute path to the .ome.tif image file.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>No .xml file containing information about the acquired scan was found at path in parent directory at <code>pvtiffile</code>.</p> <p>Returns:</p> Name Type Description <code>metainfo</code> <code>dict</code> <p>A dict mapping keys to corresponding metadata values fetched from the .xml file.</p> Source code in <code>element_interface/prairieviewreader.py</code> <pre><code>def get_pv_metadata(pvtiffile: str) -&gt; dict:\n\"\"\"Extract metadata for scans generated by PrairieView acquisition software.\n\n    The PrairieView software generates one .ome.tif imaging file per frame acquired. The\n    metadata for all frames is contained one .xml file. This function locates the .xml\n    file and generates a dictionary necessary to populate the DataJoint ScanInfo and\n    Field tables. PrairieView works with resonance scanners with a single field.\n    PrairieView does not support bidirectional x and y scanning. ROI information is not\n    contained in the .xml file. All images generated using PrairieView have square\n    dimensions(e.g. 512x512).\n\n    Args:\n        pvtiffile: An absolute path to the .ome.tif image file.\n\n    Raises:\n        FileNotFoundError: No .xml file containing information about the acquired scan\n            was found at path in parent directory at `pvtiffile`.\n\n    Returns:\n        metainfo: A dict mapping keys to corresponding metadata values fetched from the\n            .xml file.\n    \"\"\"\n\n    # May return multiple xml files. Only need one that contains scan metadata.\n    xml_files = pathlib.Path(pvtiffile).parent.glob(\"*.xml\")\n\n    for xml_file in xml_files:\n        tree = ET.parse(xml_file)\n        root = tree.getroot()\n        if root.find(\".//Sequence\"):\n            break\n    else:\n        raise FileNotFoundError(\n            f\"No PrarieView metadata XML file found at {pvtiffile.parent}\"\n        )\n\n    bidirectional_scan = False  # Does not support bidirectional\n    roi = 0\n    n_fields = 1  # Always contains 1 field\n    record_start_time = root.find(\".//Sequence/[@cycle='1']\").attrib.get(\"time\")\n\n    # Get all channels and find unique values\n    channel_list = [\n        int(channel.attrib.get(\"channel\"))\n        for channel in root.iterfind(\".//Sequence/Frame/File/[@channel]\")\n    ]\n    n_channels = len(set(channel_list))\n    n_frames = len(root.findall(\".//Sequence/Frame\"))\n    framerate = 1 / float(\n        root.findall('.//PVStateValue/[@key=\"framePeriod\"]')[0].attrib.get(\"value\")\n    )  # rate = 1/framePeriod\n\n    usec_per_line = (\n        float(\n            root.findall(\".//PVStateValue/[@key='scanLinePeriod']\")[0].attrib.get(\n                \"value\"\n            )\n        )\n        * 1e6\n    )  # Convert from seconds to microseconds\n\n    scan_datetime = datetime.strptime(root.attrib.get(\"date\"), \"%m/%d/%Y %I:%M:%S %p\")\n\n    total_duration = float(\n        root.findall(\".//Sequence/Frame\")[-1].attrib.get(\"relativeTime\")\n    )\n\n    px_height = int(\n        root.findall(\".//PVStateValue/[@key='pixelsPerLine']\")[0].attrib.get(\"value\")\n    )\n    # All PrairieView-acquired images have square dimensions (512 x 512; 1024 x 1024)\n    px_width = px_height\n\n    um_per_pixel = float(\n        root.find(\n            \".//PVStateValue/[@key='micronsPerPixel']/IndexedValue/[@index='XAxis']\"\n        ).attrib.get(\"value\")\n    )\n\n    um_height = um_width = float(px_height) * um_per_pixel\n\n    # x and y coordinate values for the center of the field\n    x_field = float(\n        root.find(\n            \".//PVStateValue/[@key='currentScanCenter']/IndexedValue/[@index='XAxis']\"\n        ).attrib.get(\"value\")\n    )\n    y_field = float(\n        root.find(\n            \".//PVStateValue/[@key='currentScanCenter']/IndexedValue/[@index='YAxis']\"\n        ).attrib.get(\"value\")\n    )\n    if (\n        root.find(\n            \".//Sequence/[@cycle='1']/Frame/PVStateShard/PVStateValue/[@key='positionCurrent']/SubindexedValues/[@index='ZAxis']\"\n        )\n        is None\n    ):\n\n        z_fields = np.float64(\n            root.find(\n                \".//PVStateValue/[@key='positionCurrent']/SubindexedValues/[@index='ZAxis']/SubindexedValue\"\n            ).attrib.get(\"value\")\n        )\n        n_depths = 1\n        assert z_fields.size == n_depths\n        bidirection_z = False\n\n    else:\n\n        bidirection_z = root.find(\".//Sequence\").attrib.get(\"bidirectionalZ\") == \"True\"\n\n        # One \"Frame\" per depth. Gets number of frames in first sequence\n        planes = [\n            int(plane.attrib.get(\"index\"))\n            for plane in root.findall(\".//Sequence/[@cycle='1']/Frame\")\n        ]\n        n_depths = len(set(planes))\n\n        z_controllers = root.findall(\n            \".//Sequence/[@cycle='1']/Frame/[@index='1']/PVStateShard/PVStateValue/[@key='positionCurrent']/SubindexedValues/[@index='ZAxis']/SubindexedValue\"\n        )\n        if len(z_controllers) &gt; 1:\n\n            z_repeats = []\n            for controller in root.findall(\n                \".//Sequence/[@cycle='1']/Frame/[@index='1']/PVStateShard/PVStateValue/[@key='positionCurrent']/SubindexedValues/[@index='ZAxis']/\"):\n                z_repeats.append(\n                    [\n                        float(z.attrib.get(\"value\"))\n                        for z in root.findall(\n                            \".//Sequence/[@cycle='1']/Frame/PVStateShard/PVStateValue/[@key='positionCurrent']/SubindexedValues/[@index='ZAxis']/SubindexedValue/[@subindex='{0}']\".format(\n                                controller.attrib.get(\"subindex\")\n                            )\n                        )\n                    ]\n                )\n\n\n            controller_assert = [not all(z == z_controller[0] for z in z_controller) for z_controller in z_repeats]\n\n            assert sum(controller_assert)==1, \"Multiple controllers changing z depth is not supported\"\n\n            z_fields = z_repeats[controller_assert.index(True)]\n\n        else:\n            z_fields = [\n                z.attrib.get(\"value\")\n                for z in root.findall(\n                    \".//Sequence/[@cycle='1']/Frame/PVStateShard/PVStateValue/[@key='positionCurrent']/SubindexedValues/[@index='ZAxis']/SubindexedValue/[@subindex='0']\"\n                )\n            ]\n\n        assert (\n            len(z_fields) == n_depths\n        ), \"Number of z fields does not match number of depths.\"\n\n    metainfo = dict(\n        num_fields=n_fields,\n        num_channels=n_channels,\n        num_planes=n_depths,\n        num_frames=n_frames,\n        num_rois=roi,\n        x_pos=None,\n        y_pos=None,\n        z_pos=None,\n        frame_rate=framerate,\n        bidirectional=bidirectional_scan,\n        bidirectional_z=bidirection_z,\n        scan_datetime=scan_datetime,\n        usecs_per_line=usec_per_line,\n        scan_duration=total_duration,\n        height_in_pixels=px_height,\n        width_in_pixels=px_width,\n        height_in_um=um_height,\n        width_in_um=um_width,\n        fieldX=x_field,\n        fieldY=y_field,\n        fieldZ=z_fields,\n        recording_time=record_start_time,\n    )\n\n    return metainfo\n</code></pre>"}, {"location": "api/element_interface/run_caiman/", "title": "run_caiman.py", "text": ""}, {"location": "api/element_interface/run_caiman/#element_interface.run_caiman.run_caiman", "title": "<code>run_caiman(file_paths, parameters, sampling_rate, output_dir, is3D)</code>", "text": "<p>Runs the standard caiman analysis pipeline (CNMF.fit_file method).</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list</code> <p>Image (full) paths</p> required <code>parameters</code> <code>dict</code> <p>Caiman parameters</p> required <code>sampling_rate</code> <code>float</code> <p>Image sampling rate (Hz)</p> required <code>output_dir</code> <code>str</code> <p>Output directory</p> required <code>is3D</code> <code>bool</code> <p>the data is 3D</p> required Source code in <code>element_interface/run_caiman.py</code> <pre><code>def run_caiman(\n    file_paths: list,\n    parameters: dict,\n    sampling_rate: float,\n    output_dir: str,\n    is3D: bool,\n):\n\"\"\"\n    Runs the standard caiman analysis pipeline (CNMF.fit_file method).\n\n    Args:\n        file_paths (list): Image (full) paths\n        parameters (dict): Caiman parameters\n        sampling_rate (float): Image sampling rate (Hz)\n        output_dir (str): Output directory\n        is3D (bool):  the data is 3D\n    \"\"\"\n    parameters[\"is3D\"] = is3D\n    parameters[\"fnames\"] = file_paths\n    parameters[\"fr\"] = sampling_rate\n\n    opts = params.CNMFParams(params_dict=parameters)\n\n    c, dview, n_processes = cm.cluster.setup_cluster(\n        backend=\"local\", n_processes=None, single_thread=False\n    )\n\n    cnm = CNMF(n_processes, params=opts, dview=dview)\n    cnmf_output, mc_output = cnm.fit_file(\n        motion_correct=True, include_eval=True, output_dir=output_dir, return_mc=True\n    )\n\n    cm.stop_server(dview=dview)\n\n    cnmf_output_file = pathlib.Path(cnmf_output.mmap_file[:-4] + \"hdf5\")\n    assert cnmf_output_file.exists()\n    assert cnmf_output_file.parent == pathlib.Path(output_dir)\n\n    _save_mc(mc_output, cnmf_output_file.as_posix(), parameters[\"is3D\"])\n</code></pre>"}, {"location": "api/element_interface/scanimage_utils/", "title": "scanimage_utils.py", "text": ""}, {"location": "api/element_interface/scanimage_utils/#element_interface.scanimage_utils.get_scanimage_acq_time", "title": "<code>get_scanimage_acq_time(scan)</code>", "text": "<p>Return ScanImage acquisition time</p> Example <p>loaded_scan = scanreader.read_scan(scan_filepath)</p> <p>header = scanimage_utils.parse_scanimage_header(loaded_scan)</p> <p>Parameters:</p> Name Type Description Default <code>scan</code> <code>scanimage object</code> <p>ScanImage object with header</p> required <p>Returns:</p> Name Type Description <code>time</code> <code>str</code> <p>acquisition time in %Y %m %d %H %M %S format</p> Source code in <code>element_interface/scanimage_utils.py</code> <pre><code>def get_scanimage_acq_time(scan):\n\"\"\"Return ScanImage acquisition time\n\n    Example:\n        &gt; loaded_scan = scanreader.read_scan(scan_filepath)\n\n        &gt; header = scanimage_utils.parse_scanimage_header(loaded_scan)\n\n    Args:\n        scan (scanimage object): ScanImage object with header\n\n    Returns:\n        time (str): acquisition time in %Y %m %d %H %M %S format\n    \"\"\"\n    header = parse_scanimage_header(scan)\n    recording_time = datetime.strptime(\n        (header[\"epoch\"][1:-1]).replace(\",\", \" \"), \"%Y %m %d %H %M %S.%f\"\n    )\n    return recording_time\n</code></pre>"}, {"location": "api/element_interface/scanimage_utils/#element_interface.scanimage_utils.parse_scanimage_header", "title": "<code>parse_scanimage_header(scan)</code>", "text": "<p>Parse ScanImage header</p> Example <p>loaded_scan = scanreader.read_scan(scan_filepath)</p> <p>recording_time = scanimage_utils.get_scanimage_acq_time(loaded_scan)</p> <p>Parameters:</p> Name Type Description Default <code>scan</code> <code>scanimage object</code> <p>ScanImage object including a header property</p> required <p>Returns:</p> Name Type Description <code>header</code> <code>dict</code> <p>ScanImage header as key-value dictionary</p> Source code in <code>element_interface/scanimage_utils.py</code> <pre><code>def parse_scanimage_header(scan):\n\"\"\"Parse ScanImage header\n\n    Example:\n        &gt; loaded_scan = scanreader.read_scan(scan_filepath)\n\n        &gt; recording_time = scanimage_utils.get_scanimage_acq_time(loaded_scan)\n\n    Args:\n        scan (scanimage object): ScanImage object including a header property\n\n    Returns:\n        header (dict): ScanImage header as key-value dictionary\n    \"\"\"\n    header = {}\n    for item in scan.header.split(\"\\n\"):\n        try:\n            key, value = item.split(\" = \")\n            key = re.sub(\"^scanimage_\", \"\", key.replace(\".\", \"_\"))\n            header[key] = value\n        except:  # noqa E722\n            pass  # TODO: remove bare except\n    return header\n</code></pre>"}, {"location": "api/element_interface/suite2p_loader/", "title": "suite2p_loader.py", "text": ""}, {"location": "api/element_interface/suite2p_loader/#element_interface.suite2p_loader.PlaneSuite2p", "title": "<code>PlaneSuite2p</code>", "text": "<p>Parse the suite2p output directory and load data, per plane.</p> <p>Suite2p output doc: https://suite2p.readthedocs.io/en/latest/outputs.html</p> Expecting the following files <ul> <li>ops:        Options file</li> <li>Fneu:       Neuropil traces file for functional channel</li> <li>Fneu_chan2: Neuropil traces file for channel 2</li> <li>F:          Fluorescence traces for functional channel</li> <li>F_chan2:    Fluorescence traces for channel 2</li> <li>iscell:     Array of (user curated) cells and probability of being a cell</li> <li>spks:       Spikes (raw deconvolved with OASIS package)</li> <li>stat:       Various statistics for each cell</li> <li>redcell:    \"Red cell\" (second channel) stats</li> </ul> <p>Attributes:</p> Name Type Description <code>alignment_channel</code> <p>ops[\"align_by_chan\"] as zero-indexed</p> <code>cell_prob</code> <code>correlation_map</code> <p>ops[\"Vcorr\"]</p> <code>creation_time</code> <p>earliest file creation time across planes</p> <code>curation_time</code> <p>latest curation time across planes</p> <code>F</code> <p>Fluorescence traces for functional channel as numpy array if exists If does not exist, returns empty list</p> <code>F_chan2</code> <p>Fluorescence traces for channel 2 as numpy array if exists If does not exist, returns empty lists</p> <code>Fneu</code> <p>Neuropil traces file for functional channel as numpy array if exists If does not exist, returns empty list</p> <code>Fneu_chan2</code> <p>Neuropil traces file for channel 2 as numpy array if exists If does not exist, returns empty list</p> <code>fpath</code> <p>path to plane folder</p> <code>iscell</code> <code>max_proj_image</code> <p>ops[\"max_proj\"] if exists. Else np.full_like(mean_image))</p> <code>mean_image</code> <p>ops[\"meanImg\"]</p> <code>ops</code> <p>Options file as numpy array</p> <code>plane_idx</code> <p>plane index. -1 if combined, else number in path</p> <code>redcell</code> <p>\"Red cell\" (second channel) stats as numpy array if exists If does not exist, returns empty list</p> <code>ref_image</code> <p>ops[\"refImg\"]</p> <code>segmentation_channel</code> <p>ops[\"functional_chan\"] as zero-indexed</p> <code>spks</code> <p>Spikes (raw deconvolved with OASIS package) as numpy array if exists If does not exist, returns empty lists</p> <code>stat</code> <p>Various statistics for each cell as numpy array if exists If does not exist, returns empty lists</p> Source code in <code>element_interface/suite2p_loader.py</code> <pre><code>class PlaneSuite2p:\n\"\"\"Parse the suite2p output directory and load data, ***per plane***.\n\n    Suite2p output doc: https://suite2p.readthedocs.io/en/latest/outputs.html\n\n    Expecting the following files:\n        - ops:        Options file\n        - Fneu:       Neuropil traces file for functional channel\n        - Fneu_chan2: Neuropil traces file for channel 2\n        - F:          Fluorescence traces for functional channel\n        - F_chan2:    Fluorescence traces for channel 2\n        - iscell:     Array of (user curated) cells and probability of being a cell\n        - spks:       Spikes (raw deconvolved with OASIS package)\n        - stat:       Various statistics for each cell\n        - redcell:    \"Red cell\" (second channel) stats\n\n    Attributes:\n        alignment_channel: ops[\"align_by_chan\"] as zero-indexed\n        cell_prob:\n        correlation_map: ops[\"Vcorr\"]\n        creation_time: earliest file creation time across planes\n        curation_time: latest curation time across planes\n        F: Fluorescence traces for functional channel as numpy array if exists\n            If does not exist, returns empty list\n        F_chan2: Fluorescence traces for channel 2 as numpy array if exists\n            If does not exist, returns empty lists\n        Fneu: Neuropil traces file for functional channel as numpy array if exists\n            If does not exist, returns empty list\n        Fneu_chan2: Neuropil traces file for channel 2 as numpy array if exists\n            If does not exist, returns empty list\n        fpath: path to plane folder\n        iscell:\n        max_proj_image: ops[\"max_proj\"] if exists. Else np.full_like(mean_image))\n        mean_image: ops[\"meanImg\"]\n        ops: Options file as numpy array\n        plane_idx: plane index. -1 if combined, else number in path\n        redcell: \"Red cell\" (second channel) stats as numpy array if exists\n            If does not exist, returns empty list\n        ref_image: ops[\"refImg\"]\n        segmentation_channel: ops[\"functional_chan\"] as zero-indexed\n        spks: Spikes (raw deconvolved with OASIS package) as numpy array if exists\n            If does not exist, returns empty lists\n        stat:  Various statistics for each cell as numpy array if exists\n            If does not exist, returns empty lists\n    \"\"\"\n\n    def __init__(self, suite2p_plane_dir: str):\n\"\"\"Initialize PlaneSuite2p class given a plane directory\n\n        Args:\n            suite2p_plane_dir (str): Suite2p plane directory\n\n        Raises:\n            FileNotFoundError: No \"ops.npy\" found. Invalid suite2p plane folder\n            FileNotFoundError: No \"iscell.npy\" found. Invalid suite2p plane folder\n        \"\"\"\n        self.fpath = pathlib.Path(suite2p_plane_dir)\n\n        # -- Verify dataset exists --\n        ops_fp = self.fpath / \"ops.npy\"\n        if not ops_fp.exists():\n            raise FileNotFoundError(\n                'No \"ops.npy\" found. Invalid suite2p plane folder: {}'.format(\n                    self.fpath\n                )\n            )\n        self.creation_time = datetime.fromtimestamp(ops_fp.stat().st_ctime)\n\n        iscell_fp = self.fpath / \"iscell.npy\"\n        if not iscell_fp.exists():\n            raise FileNotFoundError(\n                'No \"iscell.npy\" found. Invalid suite2p plane folder: {}'.format(\n                    self.fpath\n                )\n            )\n        self.curation_time = datetime.fromtimestamp(iscell_fp.stat().st_ctime)\n\n        # -- Initialize attributes --\n        for s2p_type in _suite2p_ftypes:\n            setattr(self, \"_{}\".format(s2p_type), None)\n        self._cell_prob = None\n\n        self.plane_idx = (\n            -1\n            if self.fpath.name == \"combined\"\n            else int(self.fpath.name.replace(\"plane\", \"\"))\n        )\n\n    # -- load core files --\n\n    @property\n    def ops(self):\n        if self._ops is None:\n            fp = self.fpath / \"ops.npy\"\n            self._ops = np.load(fp, allow_pickle=True).item()\n        return self._ops\n\n    @property\n    def Fneu(self):\n        if self._Fneu is None:\n            fp = self.fpath / \"Fneu.npy\"\n            self._Fneu = np.load(fp) if fp.exists() else []\n        return self._Fneu\n\n    @property\n    def Fneu_chan2(self):\n        if self._Fneu_chan2 is None:\n            fp = self.fpath / \"Fneu_chan2.npy\"\n            self._Fneu_chan2 = np.load(fp) if fp.exists() else []\n        return self._Fneu_chan2\n\n    @property\n    def F(self):\n        if self._F is None:\n            fp = self.fpath / \"F.npy\"\n            self._F = np.load(fp) if fp.exists() else []\n        return self._F\n\n    @property\n    def F_chan2(self):\n        if self._F_chan2 is None:\n            fp = self.fpath / \"F_chan2.npy\"\n            self._F_chan2 = np.load(fp) if fp.exists() else []\n        return self._F_chan2\n\n    @property\n    def iscell(self):\n        if self._iscell is None:\n            fp = self.fpath / \"iscell.npy\"\n            d = np.load(fp)\n            self._iscell = d[:, 0].astype(bool)\n            self._cell_prob = d[:, 1]\n        return self._iscell\n\n    @property\n    def cell_prob(self):\n        if self._cell_prob is None:\n            fp = self.fpath / \"iscell.npy\"\n            if fp.exists():\n                d = np.load(fp)\n                self._iscell = d[:, 0].astype(bool)\n                self._cell_prob = d[:, 1]\n        return self._cell_prob\n\n    @property\n    def spks(self):\n        if self._spks is None:\n            fp = self.fpath / \"spks.npy\"\n            self._spks = np.load(fp) if fp.exists() else []\n        return self._spks\n\n    @property\n    def stat(self):\n        if self._stat is None:\n            fp = self.fpath / \"stat.npy\"\n            self._stat = np.load(fp, allow_pickle=True) if fp.exists() else []\n        return self._stat\n\n    @property\n    def redcell(self):\n        if self._redcell is None:\n            fp = self.fpath / \"redcell.npy\"\n            self._redcell = np.load(fp) if fp.exists() else []\n        return self._redcell\n\n    # -- image property --\n\n    @property\n    def ref_image(self):\n        return self.ops[\"refImg\"]\n\n    @property\n    def mean_image(self):\n        return self.ops[\"meanImg\"]\n\n    @property\n    def max_proj_image(self):\n        return self.ops.get(\"max_proj\", np.full_like(self.mean_image, np.nan))\n\n    @property\n    def correlation_map(self):\n        return self.ops[\"Vcorr\"]\n\n    @property\n    def alignment_channel(self):\n        return self.ops[\"align_by_chan\"] - 1  # suite2p is 1-based, convert to 0-based\n\n    @property\n    def segmentation_channel(self):\n        return self.ops[\"functional_chan\"] - 1  # suite2p is 1-based, convert to 0-based\n</code></pre>"}, {"location": "api/element_interface/suite2p_loader/#element_interface.suite2p_loader.PlaneSuite2p.__init__", "title": "<code>__init__(suite2p_plane_dir)</code>", "text": "<p>Initialize PlaneSuite2p class given a plane directory</p> <p>Parameters:</p> Name Type Description Default <code>suite2p_plane_dir</code> <code>str</code> <p>Suite2p plane directory</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>No \"ops.npy\" found. Invalid suite2p plane folder</p> <code>FileNotFoundError</code> <p>No \"iscell.npy\" found. Invalid suite2p plane folder</p> Source code in <code>element_interface/suite2p_loader.py</code> <pre><code>def __init__(self, suite2p_plane_dir: str):\n\"\"\"Initialize PlaneSuite2p class given a plane directory\n\n    Args:\n        suite2p_plane_dir (str): Suite2p plane directory\n\n    Raises:\n        FileNotFoundError: No \"ops.npy\" found. Invalid suite2p plane folder\n        FileNotFoundError: No \"iscell.npy\" found. Invalid suite2p plane folder\n    \"\"\"\n    self.fpath = pathlib.Path(suite2p_plane_dir)\n\n    # -- Verify dataset exists --\n    ops_fp = self.fpath / \"ops.npy\"\n    if not ops_fp.exists():\n        raise FileNotFoundError(\n            'No \"ops.npy\" found. Invalid suite2p plane folder: {}'.format(\n                self.fpath\n            )\n        )\n    self.creation_time = datetime.fromtimestamp(ops_fp.stat().st_ctime)\n\n    iscell_fp = self.fpath / \"iscell.npy\"\n    if not iscell_fp.exists():\n        raise FileNotFoundError(\n            'No \"iscell.npy\" found. Invalid suite2p plane folder: {}'.format(\n                self.fpath\n            )\n        )\n    self.curation_time = datetime.fromtimestamp(iscell_fp.stat().st_ctime)\n\n    # -- Initialize attributes --\n    for s2p_type in _suite2p_ftypes:\n        setattr(self, \"_{}\".format(s2p_type), None)\n    self._cell_prob = None\n\n    self.plane_idx = (\n        -1\n        if self.fpath.name == \"combined\"\n        else int(self.fpath.name.replace(\"plane\", \"\"))\n    )\n</code></pre>"}, {"location": "api/element_interface/suite2p_loader/#element_interface.suite2p_loader.Suite2p", "title": "<code>Suite2p</code>", "text": "<p>Wrapper class containing all suite2p outputs from one suite2p analysis routine.</p> <p>This wrapper includes outputs from the individual plane, with plane indexing starting from 0 Plane index of -1 indicates a suite2p \"combined\" outputs from all planes, thus saved in the \"planes_combined\" attribute. See also PlaneSuite2p class.</p> Directory example <ul> <li>plane0: ops.npy, F.npy, etc.</li> <li>plane1: ops.npy, F.npy, etc.</li> <li>combined: ops.npy, F.npy, etc.</li> </ul> Example <p>loaded_dataset = suite2p_loader.Suite2p(output_dir)</p> Source code in <code>element_interface/suite2p_loader.py</code> <pre><code>class Suite2p:\n\"\"\"Wrapper class containing all suite2p outputs from one suite2p analysis routine.\n\n    This wrapper includes outputs from the individual plane, with plane indexing\n    starting from 0 Plane index of -1 indicates a suite2p \"combined\" outputs from all\n    planes, thus saved in the \"planes_combined\" attribute. See also PlaneSuite2p class.\n\n    Directory example:\n        - plane0: ops.npy, F.npy, etc.\n        - plane1: ops.npy, F.npy, etc.\n        - combined: ops.npy, F.npy, etc.\n\n    Example:\n        &gt; loaded_dataset = suite2p_loader.Suite2p(output_dir)\n\n    \"\"\"\n\n    def __init__(self, suite2p_dir: str):\n\"\"\"Initialize Suite2p class\n\n        Args:\n            suite2p_dir (str): Suite2p directory\n\n        Raises:\n            FileNotFoundError: Could not find Suite2p results\n        \"\"\"\n        self.suite2p_dir = pathlib.Path(suite2p_dir)\n\n        ops_filepaths = list(self.suite2p_dir.rglob(\"*ops.npy\"))\n\n        if not len(ops_filepaths):\n            raise FileNotFoundError(\n                \"Suite2p output result files not found at {}\".format(suite2p_dir)\n            )\n\n        self.planes = {}\n        self.planes_combined = None\n        for ops_fp in ops_filepaths:\n            plane_s2p = PlaneSuite2p(ops_fp.parent)\n            if plane_s2p.plane_idx == -1:\n                self.planes_combined = plane_s2p\n            else:\n                self.planes[plane_s2p.plane_idx] = plane_s2p\n        self.planes = OrderedDict({k: self.planes[k] for k in sorted(self.planes)})\n\n        self.creation_time = min(\n            [p.creation_time for p in self.planes.values()]\n        )  # ealiest file creation time\n        self.curation_time = max(\n            [p.curation_time for p in self.planes.values()]\n        )  # most recent curation time\n</code></pre>"}, {"location": "api/element_interface/suite2p_loader/#element_interface.suite2p_loader.Suite2p.__init__", "title": "<code>__init__(suite2p_dir)</code>", "text": "<p>Initialize Suite2p class</p> <p>Parameters:</p> Name Type Description Default <code>suite2p_dir</code> <code>str</code> <p>Suite2p directory</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Could not find Suite2p results</p> Source code in <code>element_interface/suite2p_loader.py</code> <pre><code>def __init__(self, suite2p_dir: str):\n\"\"\"Initialize Suite2p class\n\n    Args:\n        suite2p_dir (str): Suite2p directory\n\n    Raises:\n        FileNotFoundError: Could not find Suite2p results\n    \"\"\"\n    self.suite2p_dir = pathlib.Path(suite2p_dir)\n\n    ops_filepaths = list(self.suite2p_dir.rglob(\"*ops.npy\"))\n\n    if not len(ops_filepaths):\n        raise FileNotFoundError(\n            \"Suite2p output result files not found at {}\".format(suite2p_dir)\n        )\n\n    self.planes = {}\n    self.planes_combined = None\n    for ops_fp in ops_filepaths:\n        plane_s2p = PlaneSuite2p(ops_fp.parent)\n        if plane_s2p.plane_idx == -1:\n            self.planes_combined = plane_s2p\n        else:\n            self.planes[plane_s2p.plane_idx] = plane_s2p\n    self.planes = OrderedDict({k: self.planes[k] for k in sorted(self.planes)})\n\n    self.creation_time = min(\n        [p.creation_time for p in self.planes.values()]\n    )  # ealiest file creation time\n    self.curation_time = max(\n        [p.curation_time for p in self.planes.values()]\n    )  # most recent curation time\n</code></pre>"}, {"location": "api/element_interface/suite2p_trigger/", "title": "suite2p_trigger.py", "text": ""}, {"location": "api/element_interface/suite2p_trigger/#element_interface.suite2p_trigger.deconvolution_suite2p", "title": "<code>deconvolution_suite2p(segmentation_ops, db)</code>", "text": "<p>Performs deconvolution using the Suite2p package for single plane tiff files.</p> <p>The code to run deconvolution separately can be found here .</p> <p>Parameters:</p> Name Type Description Default <code>segmentation_ops</code> <code>dict</code> <p>options dictionary. Requirements:     - baseline - how to compute baseline of each trace     - win_baseline - window for max filter in seconds     - sig_baseline - width of Gaussian filter in seconds     - fs - sampling rate per plane     - prctile_baseline - percentile of trace to use as baseline         if using <code>constant_prctile</code> for baseline     - batch_size - number of frames processed per batch     - tau - timescale of the sensor, used for the deconvolution kernel     - neucoeff - neuropil coefficient for all regions of interest     - do_registration=0     - two_step_registration=False     - roidetect=False     - spikedetect=True</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>spks.npy: Updates the file with an array of deconvolved traces</p> Source code in <code>element_interface/suite2p_trigger.py</code> <pre><code>def deconvolution_suite2p(segmentation_ops: dict, db: dict) -&gt; np.ndarray:\n\"\"\"Performs deconvolution using the Suite2p package for single plane tiff files.\n\n    The code to run deconvolution separately can be found here\n    &lt;/https://suite2p.readthedocs.io/en/latest/deconvolution.html&gt;.\n\n    Args:\n        segmentation_ops (dict): options dictionary.\n            Requirements:\n                - baseline - how to compute baseline of each trace\n                - win_baseline - window for max filter in seconds\n                - sig_baseline - width of Gaussian filter in seconds\n                - fs - sampling rate per plane\n                - prctile_baseline - percentile of trace to use as baseline\n                    if using `constant_prctile` for baseline\n                - batch_size - number of frames processed per batch\n                - tau - timescale of the sensor, used for the deconvolution kernel\n                - neucoeff - neuropil coefficient for all regions of interest\n                - do_registration=0\n                - two_step_registration=False\n                - roidetect=False\n                - spikedetect=True\n\n    Returns:\n        spks.npy: Updates the file with an array of deconvolved traces\n    \"\"\"\n    if (\n        segmentation_ops[\"do_registration\"]\n        or segmentation_ops[\"roidetect\"]\n        or (not segmentation_ops[\"spikedetect\"])\n    ):\n        warnings.warn(\n            \"Running deconvolution with Suite2p.\"\n            \"Requirements include do_registration=0, roidetect=False,\"\n            \"spikedetect=True. The ops dictionary has differing values,\"\n            \"the flags will be set to the required values.\"\n        )\n        segmentation_ops.update(do_registration=0, roidetect=False, spikedetect=True)\n\n    F = np.load(db[\"fast-disk\"] + \"/suite2p/\" + \"plane0\" + \"/F.npy\", allow_pickle=True)\n    Fneu = np.load(\n        db[\"fast-disk\"] + \"/suite2p/\" + \"plane0\" + \"/Fneu.npy\", allow_pickle=True\n    )\n    Fc = F - segmentation_ops[\"neucoeff\"] * Fneu\n\n    Fc = suite2p.extraction.dcnv.preprocess(\n        F=Fc,\n        baseline=segmentation_ops[\"baseline\"],\n        win_baseline=segmentation_ops[\"win_baseline\"],\n        sig_baseline=segmentation_ops[\"sig_baseline\"],\n        fs=segmentation_ops[\"fs\"],\n        prctile_baseline=segmentation_ops[\"prctile_baseline\"],\n    )\n\n    spikes = suite2p.extraction.dcnv.oasis(\n        F=Fc,\n        batch_size=segmentation_ops[\"batch_size\"],\n        tau=segmentation_ops[\"tau\"],\n        fs=segmentation_ops[\"fs\"],\n    )\n    np.save(os.path.join(segmentation_ops[\"save_path\"], \"spks.npy\"), spikes)\n\n    return spikes\n</code></pre>"}, {"location": "api/element_interface/suite2p_trigger/#element_interface.suite2p_trigger.motion_correction_suite2p", "title": "<code>motion_correction_suite2p(ops, db)</code>", "text": "<p>Performs motion correction (i.e. registration) using the Suite2p package.</p> Example <p>ops = dict(suite2p.default_ops(), nonrigid=False, two_step_registration=False)</p> <p>db = {'h5py': [], # single h5 file path         'h5py_key': 'data',         'look_one_level_down': False, # search for TIFFs in all subfolders         'data_path': ['/test_data'], # list of folders with tiffs         'subfolders': [], # choose subfolders of 'data_path'         'fast-disk': '/test_data' # string path for storing binary file}</p> <p>ops.update(do_registration=1, roidetect=False, spikedetect=False)</p> <p>motion_correction_ops = element_interface.suite2p_trigger.motion_correction_suite2p(ops, db)</p> <p>motion_correction_ops.update(do_registration=0, roidetect=True, spikedetect=False)</p> <p>segmentation_ops = element_interface.suite2p_trigger.segmentation_suite2p(motion_correction_ops, db)</p> <p>segmentation_ops.update(do_registration=0, roidetect=False, spikedetect=True)</p> <p>spikes = element_interface.suite2p_trigger.deconvolution_suite2p(segmentation_ops, db)</p> <p>Parameters:</p> Name Type Description Default <code>ops</code> <code>dict</code> <p>ops dictionary can be obtained by using <code>suite2p.default_ops()</code> function. It contains all options and default values used to perform preprocessing. ops['do_registration'] should be set to 1.</p> required <code>db</code> <code>dict</code> <p>dictionary that includes paths pointing towards the input data, and path to store outputs</p> required <p>Returns:</p> Name Type Description <code>motion_correction_ops</code> <code>dict</code> <p>Dictionary that includes x and y shifts. A subset of the ops dictionary returned from <code>suite2p.run_s2p()</code> that is required for the segmentation step.</p> <code>tuple</code> <p>data.bin: Binary file of the data. If delete_bin is set to True (default False), the binary file is deleted after processing.</p> <code>tuple</code> <p>ops.npy: Options dictionary. This file gets updated during the segmentation and deconvolution steps.</p> Source code in <code>element_interface/suite2p_trigger.py</code> <pre><code>def motion_correction_suite2p(ops: dict, db: dict) -&gt; tuple:\n\"\"\"Performs motion correction (i.e. registration) using the Suite2p package.\n\n    Example:\n        &gt; ops = dict(suite2p.default_ops(), nonrigid=False, two_step_registration=False)\n\n        &gt; db = {'h5py': [], # single h5 file path\n                'h5py_key': 'data',\n                'look_one_level_down': False, # search for TIFFs in all subfolders\n                'data_path': ['/test_data'], # list of folders with tiffs\n                'subfolders': [], # choose subfolders of 'data_path'\n                'fast-disk': '/test_data' # string path for storing binary file}\n\n        &gt; ops.update(do_registration=1, roidetect=False, spikedetect=False)\n\n        &gt; motion_correction_ops = element_interface.suite2p_trigger.motion_correction_suite2p(ops, db)\n\n        &gt; motion_correction_ops.update(do_registration=0, roidetect=True, spikedetect=False)\n\n        &gt; segmentation_ops = element_interface.suite2p_trigger.segmentation_suite2p(motion_correction_ops, db)\n\n        &gt; segmentation_ops.update(do_registration=0, roidetect=False, spikedetect=True)\n\n        &gt; spikes = element_interface.suite2p_trigger.deconvolution_suite2p(segmentation_ops, db)\n\n\n    Args:\n        ops (dict): ops dictionary can be obtained by using `suite2p.default_ops()`\n            function. It contains all options and default values used\n            to perform preprocessing. ops['do_registration'] should be\n            set to 1.\n        db (dict): dictionary that includes paths pointing towards the input\n            data, and path to store outputs\n\n    Returns:\n        motion_correction_ops (dict): Dictionary that includes x and y shifts.\n            A subset of the ops dictionary returned from `suite2p.run_s2p()` that is\n            required for the segmentation step.\n        data.bin: Binary file of the data. If delete_bin is set to True (default False),\n            the binary file is deleted after processing.\n        ops.npy: Options dictionary. This file gets updated during the\n            segmentation and deconvolution steps.\n    \"\"\"\n\n    if (not ops[\"do_registration\"]) or ops[\"roidetect\"] or ops[\"spikedetect\"]:\n        warnings.warn(\n            \"Running motion correction with Suite2p.\"\n            \"Requirements include do_registration=1,\"\n            \"roidetect=False, spikedetect=False.  The ops\"\n            \"dictionary has differing values. The flags will\"\n            \"be set to the required values.\"\n        )\n\n        ops.update(do_registration=1, roidetect=False, spikedetect=False)\n\n    if ops[\"nonrigid\"]:\n\n        print(\"------------Running non-rigid motion correction------------\")\n\n        motion_correction_ops = suite2p.run_s2p(ops, db)\n        subset_keys = [\n            \"xoff\",\n            \"yoff\",\n            \"xoff1\",\n            \"yoff1\",\n            \"do_registration\",\n            \"two_step_registration\",\n            \"roidetect\",\n            \"spikedetect\",\n            \"delete_bin\",\n            \"xblock\",\n            \"yblock\",\n            \"xrange\",\n            \"yrange\",\n            \"nblocks\",\n            \"nframes\",\n        ]\n\n    else:\n\n        print(\"------------Running rigid motion correction------------\")\n\n        motion_correction_ops = suite2p.run_s2p(ops, db)\n        subset_keys = [\n            \"xoff\",\n            \"yoff\",\n            \"do_registration\",\n            \"two_step_registration\",\n            \"roidetect\",\n            \"spikedetect\",\n            \"delete_bin\",\n        ]\n\n    motion_correction_ops = {key: motion_correction_ops[key] for key in subset_keys}\n\n    return motion_correction_ops\n</code></pre>"}, {"location": "api/element_interface/suite2p_trigger/#element_interface.suite2p_trigger.segmentation_suite2p", "title": "<code>segmentation_suite2p(motion_correction_ops, db)</code>", "text": "<p>Performs cell segmentation (i.e. roi detection) using Suite2p package.</p> <p>Parameters:</p> Name Type Description Default <code>motion_correction_ops</code> <code>dict</code> <p>options dictionary. Requirements:     - x and y shifts     - do_registration=0     - two_step_registration=False     - roidetect=True     - spikedetect=False</p> required <code>db</code> <code>dict</code> <p>dictionary that includes paths pointing towards the input data, and path to store outputs</p> required <p>Returns:</p> Name Type Description <code>segmentation_ops</code> <code>dict</code> <p>A subset of the ops dictionary returned from <code>suite2p.run_s2p()</code> that is required for the deconvolution step.</p> <code>tuple</code> <p>data.bin: Binary file if the one created during motion correction is deleted. If delete_bin=True, the binary file is deleted after processing.</p> <code>tuple</code> <p>ops.npy: Updated ops dictionary created by suite2p.run_s2p()</p> <code>tuple</code> <p>F.npy: Array of fluorescence traces</p> <code>tuple</code> <p>Fneu.npy: Array of neuropil fluorescence traces</p> <code>tuple</code> <p>iscell.npy: Specifies whether a region of interest is a cell and the probability</p> <code>tuple</code> <p>stat.npy: List of statistics computed for each cell</p> <code>tuple</code> <p>spks.npy: Empty file. This file is updated with deconvolved traces during the deconvolution step.</p> Source code in <code>element_interface/suite2p_trigger.py</code> <pre><code>def segmentation_suite2p(motion_correction_ops: dict, db: dict) -&gt; tuple:\n\"\"\"Performs cell segmentation (i.e. roi detection) using Suite2p package.\n\n    Args:\n        motion_correction_ops (dict): options dictionary.\n            Requirements:\n                - x and y shifts\n                - do_registration=0\n                - two_step_registration=False\n                - roidetect=True\n                - spikedetect=False\n        db (dict): dictionary that includes paths pointing towards the input\n            data, and path to store outputs\n\n    Returns:\n        segmentation_ops (dict): A subset of the ops dictionary returned from\n            `suite2p.run_s2p()` that is required for the deconvolution step.\n        data.bin: Binary file if the one created during motion correction is deleted.\n            If delete_bin=True, the binary file is deleted after processing.\n        ops.npy: Updated ops dictionary created by suite2p.run_s2p()\n        F.npy: Array of fluorescence traces\n        Fneu.npy: Array of neuropil fluorescence traces\n        iscell.npy: Specifies whether a region of interest is a cell and the probability\n        stat.npy: List of statistics computed for each cell\n        spks.npy: Empty file. This file is updated with deconvolved traces during the\n            deconvolution step.\n    \"\"\"\n\n    if (\n        motion_correction_ops[\"do_registration\"]\n        or not motion_correction_ops[\"roidetect\"]\n        or motion_correction_ops[\"spikedetect\"]\n    ):\n        warnings.warn(\n            \"Running segmentation with Suite2p. Requirements\"\n            \"include do_registration=0, roidetect=True,\"\n            \"spikedetect=False. The ops dictionary has differing\"\n            \"values. The flags will be set to the required values.\"\n        )\n        motion_correction_ops.update(\n            do_registration=0, roidetect=True, spikedetect=False\n        )\n\n    segmentation_ops = suite2p.run_s2p(motion_correction_ops, db)\n    subset_keys = [\n        \"baseline\",\n        \"win_baseline\",\n        \"sig_baseline\",\n        \"fs\",\n        \"prctile_baseline\",\n        \"batch_size\",\n        \"tau\",\n        \"save_path\",\n        \"do_registration\",\n        \"roidetect\",\n        \"spikedetect\",\n        \"neucoeff\",\n    ]\n\n    segmentation_ops = {key: segmentation_ops[key] for key in subset_keys}\n\n    return segmentation_ops\n</code></pre>"}, {"location": "api/element_interface/utils/", "title": "utils.py", "text": ""}, {"location": "api/element_interface/utils/#element_interface.utils.QuietStdOut", "title": "<code>QuietStdOut</code>", "text": "<p>Context for quieting standard output, and setting datajoint loglevel to warning</p> <p>Used in pytest functions to render clear output showing only pass/fail</p> Example <p>with QuietStdOut():     table.delete(safemode=False)</p> Source code in <code>element_interface/utils.py</code> <pre><code>class QuietStdOut:\n\"\"\"Context for quieting standard output, and setting datajoint loglevel to warning\n\n    Used in pytest functions to render clear output showing only pass/fail\n\n    Example:\n        with QuietStdOut():\n            table.delete(safemode=False)\n    \"\"\"\n\n    def __enter__(self):\n        self.prev_log_level = logger.level\n        logger.setLevel(30)  # set DataJoint logger to warning\n        self._original_stdout = sys.stdout\n        sys.stdout = open(os.devnull, \"w\")\n\n    def __exit__(self, *args):\n        logger.setLevel(self.prev_log_level)\n        sys.stdout.close()\n        sys.stdout = self._original_stdout\n</code></pre>"}, {"location": "api/element_interface/utils/#element_interface.utils.dict_to_uuid", "title": "<code>dict_to_uuid(key)</code>", "text": "<p>Given a dictionary <code>key</code>, returns a hash string as UUID</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>dict</code> <p>Any python dictionary</p> required Source code in <code>element_interface/utils.py</code> <pre><code>def dict_to_uuid(key: dict):\n\"\"\"Given a dictionary `key`, returns a hash string as UUID\n\n    Args:\n        key (dict): Any python dictionary\"\"\"\n    hashed = hashlib.md5()\n    for k, v in sorted(key.items()):\n        hashed.update(str(k).encode())\n        hashed.update(str(v).encode())\n    return uuid.UUID(hex=hashed.hexdigest())\n</code></pre>"}, {"location": "api/element_interface/utils/#element_interface.utils.find_full_path", "title": "<code>find_full_path(root_directories, relative_path)</code>", "text": "<p>Given a list of roots and a relative path, search and return the full-path</p> <p>Root directories are searched in the provided order</p> <p>Parameters:</p> Name Type Description Default <code>root_directories</code> <code>list</code> <p>potential root directories</p> required <code>relative_path</code> <code>str</code> <p>the relative path to find the valid root directory</p> required <p>Returns:</p> Type Description <code>pathlib.PosixPath</code> <p>full-path (pathlib.Path object)</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>No valid full path</p> Source code in <code>element_interface/utils.py</code> <pre><code>def find_full_path(root_directories: list, relative_path: str) -&gt; pathlib.PosixPath:\n\"\"\"Given a list of roots and a relative path, search and return the full-path\n\n    Root directories are searched in the provided order\n\n    Args:\n        root_directories (list): potential root directories\n        relative_path (str): the relative path to find the valid root directory\n\n    Returns:\n        full-path (pathlib.Path object)\n\n    Raises:\n        FileNotFoundError: No valid full path\n    \"\"\"\n    relative_path = _to_Path(relative_path)\n\n    if relative_path.exists():\n        return relative_path\n\n    # Turn to list if only a single root directory is provided\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [_to_Path(root_directories)]\n\n    for root_dir in root_directories:\n        if (_to_Path(root_dir) / relative_path).exists():\n            return _to_Path(root_dir) / relative_path\n\n    raise FileNotFoundError(\n        \"No valid full-path found (from {})\"\n        \" for {}\".format(root_directories, relative_path)\n    )\n</code></pre>"}, {"location": "api/element_interface/utils/#element_interface.utils.find_root_directory", "title": "<code>find_root_directory(root_directories, full_path)</code>", "text": "<p>Given multiple potential root directories and a full-path, return parent root.</p> <p>Search and return one directory that is the parent of the given path.</p> <p>Parameters:</p> Name Type Description Default <code>root_directories</code> <code>list</code> <p>potential root directories</p> required <code>full_path</code> <code>str</code> <p>the full path to search the root directory</p> required <p>Returns:</p> Type Description <code>pathlib.PosixPath</code> <p>root_directory (pathlib.Path object)</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Full path does not exist</p> <code>FileNotFoundError</code> <p>No valid root directory</p> Source code in <code>element_interface/utils.py</code> <pre><code>def find_root_directory(root_directories: list, full_path: str) -&gt; pathlib.PosixPath:\n\"\"\"Given multiple potential root directories and a full-path, return parent root.\n\n    Search and return one directory that is the parent of the given path.\n\n    Args:\n        root_directories (list): potential root directories\n        full_path (str): the full path to search the root directory\n\n    Returns:\n        root_directory (pathlib.Path object)\n\n    Raises:\n        FileNotFoundError: Full path does not exist\n        FileNotFoundError: No valid root directory\n    \"\"\"\n    full_path = _to_Path(full_path)\n\n    if not full_path.exists():\n        raise FileNotFoundError(f\"{full_path} does not exist!\")\n\n    # Turn to list if only a single root directory is provided\n    if isinstance(root_directories, (str, pathlib.Path)):\n        root_directories = [_to_Path(root_directories)]\n\n    try:\n        return next(\n            _to_Path(root_dir)\n            for root_dir in root_directories\n            if _to_Path(root_dir) in set(full_path.parents)\n        )\n\n    except StopIteration:\n        raise FileNotFoundError(\n            \"No valid root directory found (from {})\"\n            \" for {}\".format(root_directories, full_path)\n        )\n</code></pre>"}, {"location": "api/element_interface/utils/#element_interface.utils.ingest_csv_to_table", "title": "<code>ingest_csv_to_table(csvs, tables, verbose=True, skip_duplicates=True, ignore_extra_fields=True, allow_direct_insert=False)</code>", "text": "<p>Inserts data from a series of csvs into their corresponding table:</p> Example <p>ingest_csv_to_table(['./lab_data.csv', './proj_data.csv'],                          [lab.Lab(),lab.Project()]</p> <p>Parameters:</p> Name Type Description Default <code>csvs</code> <code>list</code> <p>list of paths to CSV files relative to current directory. CSV are delimited by commas.</p> required <code>tables</code> <code>list</code> <p>list of datajoint tables with terminal <code>()</code></p> required <code>verbose</code> <code>bool</code> <p>print number inserted (i.e., table length change)</p> <code>True</code> <code>skip_duplicates</code> <code>bool</code> <p>skip duplicate entries. See DataJoint's <code>insert</code></p> <code>True</code> <code>ignore_extra_fields</code> <code>bool</code> <p>if a csv feeds multiple tables, the subset of columns not applicable to a table will be ignored. See DataJoint's <code>insert</code></p> <code>True</code> <code>allow_direct_insert</code> <code>bool</code> <p>permit insertion into Imported and Computed tables See DataJoint's <code>insert</code>.</p> <code>False</code> Source code in <code>element_interface/utils.py</code> <pre><code>def ingest_csv_to_table(\n    csvs: list,\n    tables: list,\n    verbose: bool = True,\n    skip_duplicates: bool = True,\n    ignore_extra_fields: bool = True,\n    allow_direct_insert: bool = False,\n):\n\"\"\"Inserts data from a series of csvs into their corresponding table:\n\n    Example:\n        &gt; ingest_csv_to_table(['./lab_data.csv', './proj_data.csv'],\n                                 [lab.Lab(),lab.Project()]\n\n    Args:\n        csvs (list): list of paths to CSV files relative to current directory.\n            CSV are delimited by commas.\n        tables (list): list of datajoint tables with terminal `()`\n        verbose (bool): print number inserted (i.e., table length change)\n        skip_duplicates (bool): skip duplicate entries. See DataJoint's `insert`\n        ignore_extra_fields (bool): if a csv feeds multiple tables, the subset of\n            columns not applicable to a table will be ignored. See DataJoint's `insert`\n        allow_direct_insert (bool): permit insertion into Imported and Computed tables\n            See DataJoint's `insert`.\n    \"\"\"\n    for csv_filepath, table in zip(csvs, tables):\n        with open(csv_filepath, newline=\"\") as f:\n            data = list(csv.DictReader(f, delimiter=\",\"))\n        if verbose:\n            prev_len = len(table)\n        table.insert(\n            data,\n            skip_duplicates=skip_duplicates,\n            # Ignore extra fields because some CSVs feed multiple tables\n            ignore_extra_fields=ignore_extra_fields,\n            # Allow direct bc element-event uses dj.Imported w/o `make` funcs\n            allow_direct_insert=allow_direct_insert,\n        )\n        if verbose:\n            insert_len = len(table) - prev_len\n            logger.info(\n                f\"\\n---- Inserting {insert_len} entry(s) \"\n                + f\"into {to_camel_case(table.table_name)} ----\"\n            )\n</code></pre>"}, {"location": "api/element_interface/utils/#element_interface.utils.value_to_bool", "title": "<code>value_to_bool(value)</code>", "text": "<p>Return whether the provided value represents true. Otherwise false.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str, bool, int</code> <p>Any input</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if value in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\")</p> Source code in <code>element_interface/utils.py</code> <pre><code>def value_to_bool(value) -&gt; bool:\n\"\"\"Return whether the provided value represents true. Otherwise false.\n\n    Args:\n        value (str, bool, int): Any input\n\n    Returns:\n        bool (bool): True if value in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\")\n    \"\"\"\n    if not value:\n        return False\n    return str(value).lower() in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\")\n</code></pre>"}, {"location": "api/element_interface/version/", "title": "version.py", "text": "<p>Package metadata</p>"}]}